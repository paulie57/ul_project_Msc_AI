{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b5afa668-ecfa-41cd-b9e4-307eac296230",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sktime'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m#from pycaret.time_series import TSForecastingExperiment\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msktime\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtransformations\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mseries\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msummarize\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m WindowSummarizer\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpd\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'sktime'"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "#from pycaret.time_series import TSForecastingExperiment\n",
    "from sktime.transformations.series.summarize import WindowSummarizer\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7c64f55-7395-46f4-af0c-b80c14a9632e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pycaret.utils import version\n",
    "version()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4df5dd4-5f61-4ada-ba2b-2c19ba3c71a4",
   "metadata": {},
   "source": [
    "## Load training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa0ce10e-ae5b-4256-ade0-6658377ff3f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_ihfd_met= \"/home/paulharford/college/project/project_data/processed/WEATHERED_merged_v2.csv\"\n",
    "full_path_merged = os.path.abspath(merged_ihfd_met)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1803a279-d233-4ffd-a691-ae747566f06e",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_reg = pd.read_csv(full_path_merged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e674e236-a105-4730-ab33-23db5e1a6d5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of columns to drop\n",
    "columns_to_drop = ['cold_lag', 'wind_lag', 'precip_lag', 'heat_lag','red_warning_lag','orange_warning_lag','no_adverse_weather']\n",
    "\n",
    "# Drop the columns\n",
    "merged_reg = merged_reg.drop(columns=columns_to_drop)\n",
    "\n",
    "# Verify the columns were dropped\n",
    "print(f\"Shape after dropping columns: {merged_reg.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efa05f67-5028-4391-bc0e-302212961b8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#['hip_fracture_event'] = (df_merged_train['hip_fracture_count'] > 0).astype(int)\n",
    "merged_reg['has_weather_event'] = (merged_reg['weather_event'] > 0).astype(int)\n",
    "merged_reg['hip_weather_interaction'] = merged_reg['hip_fracture_count'] * merged_reg['has_weather_event']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae28ef56-cd32-437d-9825-aef334668b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_reg['date'] = pd.to_datetime(merged_reg['date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e67b428a-66fc-4f55-9952-a8833e73ceb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_reg.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa133d2f-8887-4dc0-8c92-625e1976a93b",
   "metadata": {},
   "source": [
    "## Load and prepare 2024 data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "939ec5f2-7f6f-43ef-9427-2354af313eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "census_2024 = \"/home/paulharford/college/project/project_data/processed/WEATHERED_census_estimated_2024_pop_age_grp_gender_region.csv\"\n",
    "full_path_census_2024 = os.path.abspath(census_2024)\n",
    "weather_2024 = \"/home/paulharford/college/project/project_data/processed/WEATHERED_warnings_2024_cleaned_v1.0.csv\"\n",
    "full_path_weather_2024 = os.path.abspath(weather_2024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4edc1683-12a8-4bbd-b8fc-01b6d86bd0f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_census_2024 = pd.read_csv(full_path_census_2024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74f864b5-9ec7-49eb-928f-99ae7ba4d99c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_census_2024.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a81a5dff-c1e5-42b7-be2d-89288d86f7db",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_weather_2024 = pd.read_csv(full_path_weather_2024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fcd7e6e-1813-430b-8b80-b60889849c76",
   "metadata": {},
   "outputs": [],
   "source": [
    "##create 2024 datset columsn based on original data\n",
    "# Create dataset with all regions, dates, age groups, and genders\n",
    "regions = merged_reg['region'].unique()\n",
    "age_groups = merged_reg['age_group'].unique()\n",
    "genders = merged_reg['gender'].unique()\n",
    "\n",
    "# Create dates for first 6 months of 2024\n",
    "dates_2024 = pd.date_range(start='2024-01-01', end='2024-06-30', freq='D')\n",
    "\n",
    "# Create the base combinations\n",
    "base_rows = []\n",
    "for region in regions:\n",
    "    for date in dates_2024:\n",
    "        for age_group in age_groups:\n",
    "            for gender in genders:\n",
    "                base_rows.append({\n",
    "                    'region': region,\n",
    "                    'date': date,\n",
    "                    'age_group': age_group,\n",
    "                    'gender': gender\n",
    "                })\n",
    "\n",
    "df_2024_base = pd.DataFrame(base_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf00aada-89ae-48a7-a8e7-f1e92a912e17",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2024_base['date'] = pd.to_datetime(df_2024_base['date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8a27ab0-d866-4062-bcae-d20ca96a8ae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add month\n",
    "df_2024_base['month'] = df_2024_base['date'].dt.month\n",
    "\n",
    "def assign_season(month):\n",
    "    if month in [12, 1, 2]:\n",
    "        return 'Winter'\n",
    "    elif month in [3, 4, 5]:\n",
    "        return 'Spring'\n",
    "    elif month in [6, 7, 8]:\n",
    "        return 'Summer'\n",
    "    else:\n",
    "        return 'Autumn'\n",
    "\n",
    "df_2024_base['season'] = df_2024_base['month'].apply(assign_season)\n",
    "\n",
    "# Add season indicator variables\n",
    "df_2024_base['is_winter'] = (df_2024_base['season'] == 'Winter').astype(int)\n",
    "df_2024_base['is_spring'] = (df_2024_base['season'] == 'Spring').astype(int)\n",
    "df_2024_base['is_summer'] = (df_2024_base['season'] == 'Summer').astype(int)\n",
    "df_2024_base['is_autumn'] = (df_2024_base['season'] == 'Autumn').astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bff48c49-7e8b-42c1-9322-37cf8b2bef22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create expanded population data with gender column\n",
    "expanded_population = []\n",
    "\n",
    "# For each row in your current population data\n",
    "for _, row in df_census_2024.iterrows():\n",
    "    # Create a row for females\n",
    "    female_row = row.copy()\n",
    "    female_row['gender'] = 'female'\n",
    "    female_row['population'] = row['female']  # Set population to the female count\n",
    "    \n",
    "    # Create a row for males\n",
    "    male_row = row.copy()\n",
    "    male_row['gender'] = 'male'\n",
    "    male_row['population'] = row['male']  # Set population to the male count\n",
    "    \n",
    "    # Add both rows to the expanded dataset\n",
    "    expanded_population.append(female_row)\n",
    "    expanded_population.append(male_row)\n",
    "\n",
    "# Convert to DataFrame\n",
    "df_census_2024_exp = pd.DataFrame(expanded_population)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5ef0fce-8cd9-4528-a9da-c59d48242864",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_census_2024_exp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7ec9869-8f37-473d-b364-fb4c72979aa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize_population_to_match_training(pop_df):\n",
    "    import pandas as pd\n",
    "    \n",
    "    # Create a copy to avoid modifying the original\n",
    "    pop_copy = pop_df.copy()\n",
    "    \n",
    "    # 1. Create a precise mapping for age groups to match training data format\n",
    "    age_group_mapping = {\n",
    "        '60 - 64': '60-64',\n",
    "        '65 - 69': '65-69',\n",
    "        '70 - 74': '70-74',\n",
    "        '75 - 79': '75-79',\n",
    "        '80 - 84': '80-84',\n",
    "        '85 and over': '85 years and over'\n",
    "    }\n",
    "    \n",
    "    # Apply mapping to population dataset\n",
    "    pop_copy['age_group'] = pop_copy['age_group'].replace(age_group_mapping)\n",
    "    \n",
    "    # 2. Standardize gender (capitalize to match training data)\n",
    "    pop_copy['gender'] = pop_copy['gender'].str.capitalize()\n",
    "    \n",
    "    # 3. Ensure dates are datetime objects for proper operations\n",
    "    pop_copy['date'] = pd.to_datetime(pop_copy['date'])\n",
    "    \n",
    "    # 4. Strip any extra whitespace from region names\n",
    "    if 'region' in pop_copy.columns:\n",
    "        pop_copy['region'] = pop_copy['region'].str.strip()\n",
    "    \n",
    "    # 5. Calculate log_population if needed\n",
    "    if 'population' in pop_copy.columns and 'log_population' not in pop_copy.columns:\n",
    "        import numpy as np\n",
    "        pop_copy['log_population'] = np.log(pop_copy['population'])\n",
    "    \n",
    "    return pop_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bf8df9c-c95a-486f-916f-358cc8c3dd92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply standardization to your expanded population dataset\n",
    "population_data_2024_standardized = standardize_population_to_match_training(\n",
    "    df_census_2024_exp\n",
    ")\n",
    "\n",
    "# Now merge with your base dataset\n",
    "#df_merged = df_2024_base.merge(\n",
    "#    population_data_2024_standardized,\n",
    "#    on=['region', 'date', 'age_group', 'gender'],\n",
    "#    how='left'\n",
    "#)\n",
    "\n",
    "# Check the merge results\n",
    "print(\"NaN counts after merging:\")\n",
    "print(population_data_2024_standardized.isna().sum())\n",
    "print(df_2024_base.isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63f0cb3a-6f73-4d2c-8728-514920665d24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the standardized datasets\n",
    "df_merged_2024 = df_2024_base.merge(\n",
    "    population_data_2024_standardized,\n",
    "    on=['region', 'date', 'age_group', 'gender'],\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Verify the merge results\n",
    "print(\"Shape of merged dataset:\", df_merged_2024.shape)\n",
    "print(\"NaN counts after merging:\")\n",
    "print(df_merged_2024.isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bdf205e-20e8-46a3-b06a-090f93b0ed87",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_weather_2024.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff2d919b-b85c-4490-a4da-c3db79668afc",
   "metadata": {},
   "outputs": [],
   "source": [
    "regions = df_weather_2024['region'].unique()\n",
    "min_date = df_weather_2024['date'].min()\n",
    "max_date = df_weather_2024['date'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28452dbc-6215-4031-8fa7-a770ba02dbfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "##create all dates for all years to merge with datasets so we can have days with and without events \n",
    "all_dates = pd.date_range(start=min_date, end=max_date, freq='D')\n",
    "multi_index = pd.MultiIndex.from_product([regions, all_dates], names=['region', 'date'])\n",
    "df_region_date = pd.DataFrame(index=multi_index).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6721b98d-9ea4-4d6f-bb72-adab7e2c4c58",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_region_date_weather = df_region_date.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7ed081d-2ec6-4b4e-833e-6c1cb2f5f84a",
   "metadata": {},
   "outputs": [],
   "source": [
    "##create a numerical mapping of the weather severity\n",
    "severity_mapping = {\n",
    "    'Yellow': 1,\n",
    "    'Orange': 2,\n",
    "    'Red': 3\n",
    "}\n",
    "\n",
    "df_weather_2024['warning_severity_numeric'] = df_weather_2024['warning_severity'].map(severity_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc7db906-0078-4c4b-bea6-cbd26b4f09c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_weather_data(df_weather, keep_columns=None):\n",
    "    \"\"\"\n",
    "    Aggregate weather data by region, date, weather_type, and warning_severity while\n",
    "    preserving specified additional columns.\n",
    "    \n",
    "    Args:\n",
    "        df_weather: DataFrame containing weather data\n",
    "        keep_columns: List of additional columns to preserve (beyond the groupby columns)\n",
    "        \n",
    "    Returns:\n",
    "        Aggregated DataFrame with one row per unique region-date-weather_type-warning_severity combination\n",
    "    \"\"\"\n",
    "    # Define default columns to keep if none provided\n",
    "    if keep_columns is None:\n",
    "        keep_columns = [\n",
    "            'counties_in_region', \t\n",
    "            'county_weight'\t\n",
    "         ]\n",
    "    \n",
    "    # Define all columns to group by\n",
    "    group_cols = ['region', 'date', 'warning_phenomenon', 'warning_severity']\n",
    "    \n",
    "    # Define aggregation functions for numeric and other columns\n",
    "    agg_dict = {\n",
    "        'warning_severity_numeric': 'max'\n",
    "    }\n",
    "    \n",
    "    # Add aggregation functions for additional columns to preserve\n",
    "    for col in keep_columns:\n",
    "        if col in df_weather.columns:\n",
    "            # For string columns, take the first value (assuming they're the same within groups)\n",
    "            if df_weather[col].dtype == 'object':\n",
    "                agg_dict[col] = 'first'\n",
    "            # For boolean or integer flags (like has_multiple_events, warning_upgraded)\n",
    "            elif df_weather[col].dtype in ['bool', 'int64', 'int32']:\n",
    "                agg_dict[col] = 'max'\n",
    "            # For numeric columns, take the max\n",
    "            elif pd.api.types.is_numeric_dtype(df_weather[col]):\n",
    "                agg_dict[col] = 'max'\n",
    "            # For datetime columns\n",
    "            elif pd.api.types.is_datetime64_dtype(df_weather[col]):\n",
    "                agg_dict[col] = 'max'\n",
    "            # Default to first value for any other types\n",
    "            else:\n",
    "                agg_dict[col] = 'first'\n",
    "    \n",
    "    # Group by the specified columns and aggregate\n",
    "    weather_agg = df_weather.groupby(group_cols, as_index=False).agg(agg_dict)\n",
    "    \n",
    "    # Add the weather flag column (set to 1 if there is any event)\n",
    "    weather_agg['weather_event'] = 1\n",
    "    \n",
    "    return weather_agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f6fe1fa-94b8-479a-9d6f-27745126ded7",
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_agg = aggregate_weather_data(df_weather_2024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "152de7dc-a126-4f06-93af-00d22beed9f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_agg.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4284e71a-afd8-4fcf-b232-2fc265cbb501",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_region_date_weather['date'] = pd.to_datetime(df_region_date_weather['date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a93d8bc-577f-43ff-8f1f-b59deaae0f49",
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_agg['date'] = pd.to_datetime(weather_agg['date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b90995a8-0744-47ef-9e49-1c8f0f2e4397",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_weather_final = pd.merge(\n",
    "    df_region_date_weather,    # all region-date combos\n",
    "    weather_agg,               # your aggregated counts\n",
    "    on=['region', 'date'],    # merge keys\n",
    "    how='left'                # left-join so we keep all rows from df_region_date\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b60b6aa5-37a9-4dec-8651-b1069b40d2e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, check the format of your weather data\n",
    "print(\"Weather data format check:\")\n",
    "print(\"Columns:\", df_weather_2024.columns)\n",
    "print(\"Date sample:\", df_weather_2024['date'].iloc[0], \"Type:\", type(df_weather_2024['date'].iloc[0]))\n",
    "print(\"Unique regions:\", df_weather_2024['region'].unique())\n",
    "\n",
    "# Standardize the weather data to match your base dataset format\n",
    "def standardize_weather_data(weather_df):\n",
    "    \"\"\"\n",
    "    Standardize the weather dataset to match the format of the base dataset\n",
    "    \"\"\"\n",
    "    weather_copy = weather_df.copy()\n",
    "    \n",
    "    # Ensure date is in datetime format\n",
    "    weather_copy['date'] = pd.to_datetime(weather_copy['date'])\n",
    "    \n",
    "    # Strip any whitespace from region names\n",
    "    if 'region' in weather_copy.columns:\n",
    "        weather_copy['region'] = weather_copy['region'].str.strip()\n",
    "    \n",
    "    return weather_copy\n",
    "\n",
    "# Apply standardization to weather data\n",
    "weather_data_2024_std = standardize_weather_data(df_weather_final)\n",
    "\n",
    "# Merge weather data with your already merged base+population dataset\n",
    "df_merged_with_weather = df_merged_2024.merge(\n",
    "    weather_data_2024_std,\n",
    "    on=['region', 'date'],\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Check the merge results\n",
    "print(\"\\nMerged dataset with weather:\")\n",
    "print(\"Shape:\", df_merged_with_weather.shape)\n",
    "print(\"NaN counts:\")\n",
    "print(df_merged_with_weather.isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3a049db-7030-43da-b4fd-28fb622f1f8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged_with_weather['warning_severity_numeric'] = df_merged_with_weather['warning_severity_numeric'].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3343379d-75db-444f-be83-1248f3ba9328",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged_with_weather['weather_event'] = df_merged_with_weather['weather_event'].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a21f6a0e-f18e-4f89-8337-d195e3523a36",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged_with_weather['warning_phenomenon'] = df_merged_with_weather['warning_phenomenon'].fillna('no_weather_type')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd47af6f-aa2a-4a7c-b2c4-3e90a3df21ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged_with_weather['warning_severity'] = df_merged_with_weather['warning_severity'].fillna('no_weather_severity')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea53f504-6b92-4de2-b228-30953aa4633d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create lagged variables based on warning severity\n",
    "# First, create indicator for orange warnings\n",
    "df_merged_with_weather['orange_warning'] = ((df_merged_with_weather['warning_severity'] == 'Orange') | \n",
    "                             (df_merged_with_weather['warning_severity_numeric'] == 2)).astype(int)\n",
    "\n",
    "# Create indicator for red warnings\n",
    "df_merged_with_weather['red_warning'] = ((df_merged_with_weather['warning_severity'] == 'Red') | \n",
    "                           (df_merged_with_weather['warning_severity_numeric'] == 3)).astype(int)\n",
    "\n",
    "# Create indicator for red warnings\n",
    "df_merged_with_weather['yellow_warning'] = ((df_merged_with_weather['warning_severity'] == 'Yellow') | \n",
    "                           (df_merged_with_weather['warning_severity_numeric'] == 1)).astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30114552-c161-4eb7-a258-fc4b9d53d045",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged_with_weather['cold_weather'] = df_merged_with_weather['warning_phenomenon'].isin(\n",
    "    ['Snow_Ice', 'Ice', 'Low-Temperature', 'Low-Temperature_Ice', 'Fog']).astype(int)\n",
    "\n",
    "df_merged_with_weather['wind_weather'] = df_merged_with_weather['warning_phenomenon'].isin(['Wind']).astype(int)\n",
    "\n",
    "df_merged_with_weather['precipitation'] = df_merged_with_weather['warning_phenomenon'].isin(\n",
    "    ['Rainfall', 'Thunder', 'Hail']).astype(int)\n",
    "\n",
    "df_merged_with_weather['heat_weather'] = df_merged_with_weather['warning_phenomenon'].isin(['High-Temperature']).astype(int)\n",
    "\n",
    "df_merged_with_weather['no_adverse_weather'] = df_merged_with_weather['warning_phenomenon'].isin(\n",
    "    ['no_weather_type']).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d756c08e-aa30-4963-a37a-14a01ae46585",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_merged_with_weather.isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2637b4e8-ddcc-475a-951e-3b152b73e576",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add has_weather_event indicator\n",
    "df_merged_with_weather['has_weather_event'] = (~df_merged_with_weather['warning_phenomenon'].isin(['none', None])).astype(int)\n",
    "\n",
    "# Add empty hip_fracture_count column (this will be what we're forecasting)\n",
    "df_merged_with_weather['hip_fracture_count'] = np.nan\n",
    "\n",
    "# Add hip_weather_interaction with zeros (since we don't have hip fracture data yet)\n",
    "df_merged_with_weather['hip_weather_interaction'] = 0\n",
    "\n",
    "# Verify all needed columns are now present\n",
    "print(\"Checking if all required columns are now present...\")\n",
    "missing_columns = [col for col in merged_reg.columns if col not in df_merged_with_weather.columns]\n",
    "print(\"Still missing:\", missing_columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8a72444-6ed1-4145-8ca7-27ccc70c64d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0ea8c06-fd06-4ae9-80bc-ddeed964f7dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbdb44e3-0f3a-423b-b013-ef3d5a174cb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get columns from the full historical dataset (2014-2023)\n",
    "historical_columns = set(merged_reg.columns)  \n",
    "\n",
    "# Get columns from your new 2024 dataset\n",
    "new_dataset_columns = set(df_merged_with_weather.columns)\n",
    "\n",
    "# Find columns in historical but not in new dataset\n",
    "missing_in_new = historical_columns - new_dataset_columns\n",
    "print(\"Columns in historical dataset but missing in new dataset:\")\n",
    "for col in sorted(missing_in_new):\n",
    "    print(f\"- {col}\")\n",
    "\n",
    "# Find columns in new dataset but not in historical\n",
    "extra_in_new = new_dataset_columns - historical_columns\n",
    "print(\"\\nColumns in new dataset but not in historical:\")\n",
    "for col in sorted(extra_in_new):\n",
    "    print(f\"- {col}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "266bfa49-a64e-42d9-a3dc-06743aa837b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop 'no_adverse_weather' and 'year' from the 2024 dataset (if they exist)\n",
    "columns_to_drop = ['no_adverse_weather', 'year']\n",
    "for col in columns_to_drop:\n",
    "    if col in df_merged_with_weather.columns:\n",
    "        df_merged_with_weather = df_merged_with_weather.drop(columns=col)\n",
    "        print(f\"Dropped column: {col}\")\n",
    "\n",
    "# Verify the current columns\n",
    "print(\"Current columns in 2024 dataset:\", df_merged_with_weather.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "960e8725-e7ed-48b7-a4d4-ba441a130e2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, let's verify which values exist for each region\n",
    "region_county_info = df_merged_with_weather[\n",
    "    df_merged_with_weather['counties_in_region'].notna()\n",
    "].groupby('region')[['counties_in_region', 'county_weight']].first()\n",
    "\n",
    "print(\"County information by region:\")\n",
    "print(region_county_info)\n",
    "\n",
    "# Now fill the missing values based on region\n",
    "# First, create a mapping dictionary for each region\n",
    "region_counties_mapping = df_merged_with_weather[\n",
    "    df_merged_with_weather['counties_in_region'].notna()\n",
    "].groupby('region')['counties_in_region'].first().to_dict()\n",
    "\n",
    "region_weight_mapping = df_merged_with_weather[\n",
    "    df_merged_with_weather['county_weight'].notna()\n",
    "].groupby('region')['county_weight'].first().to_dict()\n",
    "\n",
    "# Fill in the missing values using the mappings\n",
    "df_merged_with_weather['counties_in_region'] = df_merged_with_weather.apply(\n",
    "    lambda row: region_counties_mapping.get(row['region'], 0) \n",
    "                if pd.isna(row['counties_in_region']) else row['counties_in_region'],\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "df_merged_with_weather['county_weight'] = df_merged_with_weather.apply(\n",
    "    lambda row: region_weight_mapping.get(row['region'], 0) \n",
    "                if pd.isna(row['county_weight']) else row['county_weight'],\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Verify that there are no more NaN values in these columns\n",
    "print(\"\\nMissing values after filling:\")\n",
    "print(\"counties_in_region:\", df_merged_with_weather['counties_in_region'].isna().sum())\n",
    "print(\"county_weight:\", df_merged_with_weather['county_weight'].isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0cc2e00-86e2-41c8-a98a-f361b738bc7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged_with_weather.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85e45311-7130-4c91-9c41-730f7f70219b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by year and demographic segments to see the trend\n",
    "merged_reg['year'] = merged_reg['date'].dt.year\n",
    "population_trends = merged_reg.groupby(['year', 'region', 'age_group', 'gender'])[['female', 'male']].mean()\n",
    "\n",
    "# Calculate growth rates between consecutive years\n",
    "growth_rates = {}\n",
    "for region in merged_reg['region'].unique():\n",
    "    for age_group in merged_reg['age_group'].unique():\n",
    "        for gender in merged_reg['gender'].unique():\n",
    "            # Get historical data for this segment\n",
    "            segment_data = population_trends.xs((region, age_group, gender), level=(1, 2, 3))\n",
    "            \n",
    "            if len(segment_data) >= 2:  # Need at least 2 years to calculate growth\n",
    "                # Calculate year-over-year growth rates\n",
    "                if gender == 'Female':\n",
    "                    pop_col = 'female'\n",
    "                else:\n",
    "                    pop_col = 'male'\n",
    "                \n",
    "                yearly_values = segment_data[pop_col]\n",
    "                yearly_growth = yearly_values.pct_change().dropna()\n",
    "                \n",
    "                # Use average of last 3 years' growth, or whatever is available\n",
    "                recent_growth = yearly_growth.tail(3).mean()\n",
    "                growth_rates[(region, age_group, gender)] = recent_growth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3ac2268-cc10-48fe-8623-09187c93f10e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by year and demographic segments to see the trend\n",
    "df_merged_with_weather['year'] = df_merged_with_weather['date'].dt.year\n",
    "population_trends = df_merged_with_weather.groupby(['year', 'region', 'age_group', 'gender'])[['female', 'male']].mean()\n",
    "\n",
    "# Calculate growth rates between consecutive years\n",
    "growth_rates = {}\n",
    "for region in df_merged_with_weather['region'].unique():\n",
    "    for age_group in df_merged_with_weather['age_group'].unique():\n",
    "        for gender in df_merged_with_weather['gender'].unique():\n",
    "            # Get historical data for this segment\n",
    "            segment_data = population_trends.xs((region, age_group, gender), level=(1, 2, 3))\n",
    "            \n",
    "            if len(segment_data) >= 2:  # Need at least 2 years to calculate growth\n",
    "                # Calculate year-over-year growth rates\n",
    "                if gender == 'Female':\n",
    "                    pop_col = 'female'\n",
    "                else:\n",
    "                    pop_col = 'male'\n",
    "                \n",
    "                yearly_values = segment_data[pop_col]\n",
    "                yearly_growth = yearly_values.pct_change().dropna()\n",
    "                \n",
    "                # Use average of last 3 years' growth, or whatever is available\n",
    "                recent_growth = yearly_growth.tail(3).mean()\n",
    "                growth_rates[(region, age_group, gender)] = recent_growth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48e3295b-26f5-4c58-b1c8-6d0256e121bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the most recent year in the data\n",
    "latest_year = merged_reg['year'].max()\n",
    "\n",
    "# Create a dataframe for 2024 projections\n",
    "latest_data = merged_reg[merged_reg['year'] == latest_year].copy()\n",
    "unique_segments = latest_data.drop_duplicates(['region', 'age_group', 'gender'])\n",
    "\n",
    "# Create a template for 2024 data\n",
    "forecast_segments = []\n",
    "for _, row in unique_segments.iterrows():\n",
    "    # Create a new row for 2024\n",
    "    new_row = row.copy()\n",
    "    \n",
    "    # Project the population using growth rates\n",
    "    growth_key = (row['region'], row['age_group'], row['gender'])\n",
    "    if growth_key in growth_rates:\n",
    "        growth_rate = growth_rates[growth_key]\n",
    "        years_to_project = 2024 - latest_year\n",
    "        \n",
    "        if row['gender'] == 'Female':\n",
    "            new_row['female'] = row['female'] * (1 + growth_rate) ** years_to_project\n",
    "            new_row['male'] = 0\n",
    "        else:\n",
    "            new_row['female'] = 0\n",
    "            new_row['male'] = row['male'] * (1 + growth_rate) ** years_to_project\n",
    "        \n",
    "        new_row['total'] = new_row['female'] + new_row['male']\n",
    "        new_row['population'] = new_row['female'] + new_row['male']\n",
    "    \n",
    "    forecast_segments.append(new_row)\n",
    "\n",
    "forecast_base = pd.DataFrame(forecast_segments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be23ea77-371c-4f6b-bbb3-34d385b50526",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d74fc6c-3f88-4102-bf99-3f67ca2e351f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_national_monthly_data(df):\n",
    "    \"\"\"\n",
    "    Transform daily data into monthly national aggregates with weather patterns.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : DataFrame\n",
    "        The raw daily data with hip fracture counts and weather indicators\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    DataFrame\n",
    "        Monthly aggregated data with hip fracture counts and weather metrics\n",
    "    \"\"\"\n",
    "    # Step 1: Basic monthly aggregation of hip fractures and primary indicators\n",
    "    national_monthly = df.groupby(pd.Grouper(key='date', freq='M')).agg({\n",
    "        'hip_fracture_count': 'sum',         # Total fractures per month\n",
    "        \n",
    "        # Weather indicators (take maximum per day, then aggregate)\n",
    "        'has_weather_event': 'max',          # Any weather event in the period\n",
    "        'cold_weather': 'max',               # Any cold weather in the period\n",
    "        'wind_weather': 'max',               # Any wind warning in the period\n",
    "        'precipitation': 'max',              # Any precipitation warning in the period\n",
    "        'heat_weather': 'max',               # Any heat warning in the period\n",
    "        \n",
    "        # Warning severities (maximum level reached in the period)\n",
    "        'yellow_warning': 'max',             # Any yellow warnings\n",
    "        'orange_warning': 'max',             # Any orange warnings\n",
    "        'red_warning': 'max',                # Any red warnings\n",
    "        'warning_severity_numeric': 'max',   # Maximum warning severity\n",
    "    }).reset_index()\n",
    "    \n",
    "    # Step 2: Count days with different weather conditions\n",
    "    # First, aggregate daily indicators with groupby to get daily maximums\n",
    "    daily_weather = df.groupby('date').agg({\n",
    "        'has_weather_event': 'max',\n",
    "        'cold_weather': 'max',\n",
    "        'wind_weather': 'max',\n",
    "        'precipitation': 'max',\n",
    "        'heat_weather': 'max',\n",
    "        'yellow_warning': 'max',\n",
    "        'orange_warning': 'max',\n",
    "        'red_warning': 'max',\n",
    "    }).reset_index()\n",
    "    \n",
    "    # Then count days per month where each condition was present\n",
    "    monthly_weather_days = daily_weather.groupby(pd.Grouper(key='date', freq='M')).agg({\n",
    "        'has_weather_event': 'sum',      # Count days with any weather event\n",
    "        'cold_weather': 'sum',           # Count days with cold weather\n",
    "        'wind_weather': 'sum',           # Count days with wind warnings\n",
    "        'precipitation': 'sum',          # Count days with precipitation\n",
    "        'heat_weather': 'sum',           # Count days with heat warnings\n",
    "        'yellow_warning': 'sum',         # Count days with yellow warnings\n",
    "        'orange_warning': 'sum',         # Count days with orange warnings\n",
    "        'red_warning': 'sum',            # Count days with red warnings\n",
    "    }).reset_index()\n",
    "    \n",
    "    # Rename columns to clarify these are day counts\n",
    "    monthly_weather_days = monthly_weather_days.rename(columns={\n",
    "        'has_weather_event': 'days_with_weather',\n",
    "        'cold_weather': 'days_with_cold',\n",
    "        'wind_weather': 'days_with_wind',\n",
    "        'precipitation': 'days_with_precipitation',\n",
    "        'heat_weather': 'days_with_heat',\n",
    "        'yellow_warning': 'days_with_yellow',\n",
    "        'orange_warning': 'days_with_orange',\n",
    "        'red_warning': 'days_with_red',\n",
    "    })\n",
    "    \n",
    "    # Step 3: Calculate consecutive days metrics\n",
    "    def calc_consecutive_days(series):\n",
    "        \"\"\"Calculate maximum consecutive days with a condition\"\"\"\n",
    "        if len(series) == 0:\n",
    "            return 0\n",
    "            \n",
    "        # Convert to numpy for performance\n",
    "        arr = series.values\n",
    "        \n",
    "        # Find runs of consecutive True values\n",
    "        # First, identify where values change\n",
    "        changes = np.diff(np.concatenate(([0], arr, [0])))\n",
    "        \n",
    "        # Start indices of consecutive sequences (where value changes from 0 to 1)\n",
    "        starts = np.where(changes == 1)[0]\n",
    "        \n",
    "        # End indices of consecutive sequences (where value changes from 1 to 0)\n",
    "        ends = np.where(changes == -1)[0]\n",
    "        \n",
    "        # Calculate lengths of all sequences\n",
    "        lengths = ends - starts\n",
    "        \n",
    "        # Return maximum length (or 0 if no sequences found)\n",
    "        return np.max(lengths) if lengths.size > 0 else 0\n",
    "    \n",
    "    # For each month, calculate max consecutive days for each condition\n",
    "    consecutive_days = []\n",
    "    \n",
    "    # Group the daily data by month\n",
    "    monthly_groups = daily_weather.groupby(pd.Grouper(key='date', freq='M'))\n",
    "    \n",
    "    # For each month, calculate the consecutive days metrics\n",
    "    for month_date, month_data in monthly_groups:\n",
    "        max_consecutive = {\n",
    "            'date': month_date,\n",
    "            'max_consecutive_weather': calc_consecutive_days(month_data['has_weather_event']),\n",
    "            'max_consecutive_cold': calc_consecutive_days(month_data['cold_weather']),\n",
    "            'max_consecutive_wind': calc_consecutive_days(month_data['wind_weather']),\n",
    "            'max_consecutive_precipitation': calc_consecutive_days(month_data['precipitation']),\n",
    "            'max_consecutive_heat': calc_consecutive_days(month_data['heat_weather']),\n",
    "            'max_consecutive_yellow': calc_consecutive_days(month_data['yellow_warning']),\n",
    "            'max_consecutive_orange': calc_consecutive_days(month_data['orange_warning']),\n",
    "            'max_consecutive_red': calc_consecutive_days(month_data['red_warning']),\n",
    "        }\n",
    "        consecutive_days.append(max_consecutive)\n",
    "    \n",
    "    # Convert to dataframe\n",
    "    consecutive_days_df = pd.DataFrame(consecutive_days)\n",
    "    \n",
    "    # Step 4: Merge all features together\n",
    "    # First merge monthly counts with basic aggregation\n",
    "    result = pd.merge(national_monthly, monthly_weather_days, on='date')\n",
    "    \n",
    "    # Then merge consecutive days metrics\n",
    "    result = pd.merge(result, consecutive_days_df, on='date')\n",
    "    \n",
    "    # Step 5: Add month and season indicators\n",
    "    result['month'] = result['date'].dt.month\n",
    "    result['year'] = result['date'].dt.year\n",
    "    \n",
    "    # Add season based on month\n",
    "    season_map = {\n",
    "        12: 'Winter', 1: 'Winter', 2: 'Winter',\n",
    "        3: 'Spring', 4: 'Spring', 5: 'Spring',\n",
    "        6: 'Summer', 7: 'Summer', 8: 'Summer',\n",
    "        9: 'Autumn', 10: 'Autumn', 11: 'Autumn'\n",
    "    }\n",
    "    result['season'] = result['month'].map(season_map)\n",
    "    \n",
    "    # Add cyclical encoding of month (captures seasonality)\n",
    "    result['month_sin'] = np.sin(2 * np.pi * result['month'] / 12)\n",
    "    result['month_cos'] = np.cos(2 * np.pi * result['month'] / 12)\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aae6da23-7cd2-41dd-b819-b88555445e2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_regional_monthly_data(df):\n",
    "    \"\"\"\n",
    "    Transform daily data into monthly regional aggregates with weather patterns.\n",
    "    \n",
    "    This function groups data by region and month, calculating fracture counts\n",
    "    and various weather metrics at the regional level. It captures both the\n",
    "    occurrence and intensity of weather events in each region.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : DataFrame\n",
    "        The raw daily data with hip fracture counts and weather indicators\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    DataFrame\n",
    "        Monthly aggregated data by region with hip fracture counts and weather metrics\n",
    "    \"\"\"\n",
    "    # Step 1: Basic monthly aggregation by region\n",
    "    regional_monthly = df.groupby(['region', pd.Grouper(key='date', freq='M')]).agg({\n",
    "        'hip_fracture_count': 'sum',         # Total fractures per region per month\n",
    "        'counties_in_region': 'mean',        # Number of counties in this region\n",
    "        \n",
    "        # Weather indicators (maximum per region per month)\n",
    "        'has_weather_event': 'max',          # Any weather event in the region\n",
    "        'cold_weather': 'max',               # Any cold weather in the region\n",
    "        'wind_weather': 'max',               # Any wind warning in the region\n",
    "        'precipitation': 'max',              # Any precipitation warning in the region\n",
    "        'heat_weather': 'max',               # Any heat warning in the region\n",
    "        \n",
    "        # Warning severities (maximum level reached in the region)\n",
    "        'yellow_warning': 'max',             # Any yellow warnings in the region\n",
    "        'orange_warning': 'max',             # Any orange warnings in the region\n",
    "        'red_warning': 'max',                # Any red warnings in the region\n",
    "        'warning_severity_numeric': 'max',   # Maximum warning severity in the region\n",
    "    }).reset_index()\n",
    "    \n",
    "    # Step 2: Count days with different weather conditions by region\n",
    "    # First, we need daily maximums for each region\n",
    "    daily_regional_weather = df.groupby(['region', 'date']).agg({\n",
    "        'has_weather_event': 'max',\n",
    "        'cold_weather': 'max',\n",
    "        'wind_weather': 'max',\n",
    "        'precipitation': 'max',\n",
    "        'heat_weather': 'max',\n",
    "        'yellow_warning': 'max',\n",
    "        'orange_warning': 'max',\n",
    "        'red_warning': 'max',\n",
    "    }).reset_index()\n",
    "    \n",
    "    # For each region and month, count days with each condition\n",
    "    regional_weather_days = daily_regional_weather.groupby(['region', pd.Grouper(key='date', freq='M')]).agg({\n",
    "        'has_weather_event': 'sum',      # Count days with any weather event\n",
    "        'cold_weather': 'sum',           # Count days with cold weather\n",
    "        'wind_weather': 'sum',           # Count days with wind warnings\n",
    "        'precipitation': 'sum',          # Count days with precipitation\n",
    "        'heat_weather': 'sum',           # Count days with heat warnings\n",
    "        'yellow_warning': 'sum',         # Count days with yellow warnings\n",
    "        'orange_warning': 'sum',         # Count days with orange warnings\n",
    "        'red_warning': 'sum',            # Count days with red warnings\n",
    "    }).reset_index()\n",
    "    \n",
    "    # Rename columns to clarify these are day counts\n",
    "    regional_weather_days = regional_weather_days.rename(columns={\n",
    "        'has_weather_event': 'days_with_weather',\n",
    "        'cold_weather': 'days_with_cold',\n",
    "        'wind_weather': 'days_with_wind',\n",
    "        'precipitation': 'days_with_precipitation',\n",
    "        'heat_weather': 'days_with_heat',\n",
    "        'yellow_warning': 'days_with_yellow',\n",
    "        'orange_warning': 'days_with_orange',\n",
    "        'red_warning': 'days_with_red',\n",
    "    })\n",
    "    \n",
    "    # Step 3: Calculate consecutive days metrics for each region\n",
    "    def calc_consecutive_days(series):\n",
    "        \"\"\"Calculate maximum consecutive days with a condition\"\"\"\n",
    "        if len(series) == 0:\n",
    "            return 0\n",
    "            \n",
    "        # Convert to numpy for performance\n",
    "        arr = series.values\n",
    "        \n",
    "        # Find runs of consecutive True values\n",
    "        changes = np.diff(np.concatenate(([0], arr, [0])))\n",
    "        starts = np.where(changes == 1)[0]\n",
    "        ends = np.where(changes == -1)[0]\n",
    "        lengths = ends - starts\n",
    "        \n",
    "        return np.max(lengths) if lengths.size > 0 else 0\n",
    "    \n",
    "    # Initialize list to store consecutive day metrics\n",
    "    consecutive_days = []\n",
    "    \n",
    "    # For each region and month, calculate consecutive days\n",
    "    for region in df['region'].unique():\n",
    "        # Filter data for this region\n",
    "        region_data = daily_regional_weather[daily_regional_weather['region'] == region]\n",
    "        \n",
    "        # Group by month\n",
    "        monthly_groups = region_data.groupby(pd.Grouper(key='date', freq='M'))\n",
    "        \n",
    "        # For each month, calculate consecutive day metrics\n",
    "        for month_date, month_data in monthly_groups:\n",
    "            if len(month_data) > 0:  # Ensure we have data for this month\n",
    "                max_consecutive = {\n",
    "                    'region': region,\n",
    "                    'date': month_date,\n",
    "                    'max_consecutive_weather': calc_consecutive_days(month_data['has_weather_event']),\n",
    "                    'max_consecutive_cold': calc_consecutive_days(month_data['cold_weather']),\n",
    "                    'max_consecutive_wind': calc_consecutive_days(month_data['wind_weather']),\n",
    "                    'max_consecutive_precipitation': calc_consecutive_days(month_data['precipitation']),\n",
    "                    'max_consecutive_heat': calc_consecutive_days(month_data['heat_weather']),\n",
    "                    'max_consecutive_yellow': calc_consecutive_days(month_data['yellow_warning']),\n",
    "                    'max_consecutive_orange': calc_consecutive_days(month_data['orange_warning']),\n",
    "                    'max_consecutive_red': calc_consecutive_days(month_data['red_warning']),\n",
    "                }\n",
    "                consecutive_days.append(max_consecutive)\n",
    "    \n",
    "    # Convert to dataframe\n",
    "    consecutive_days_df = pd.DataFrame(consecutive_days)\n",
    "    \n",
    "    # Step 4: Merge all features together\n",
    "    # First merge basic aggregation with day counts\n",
    "    result = pd.merge(regional_monthly, regional_weather_days, on=['region', 'date'])\n",
    "    \n",
    "    # Then merge consecutive days metrics\n",
    "    if len(consecutive_days_df) > 0:\n",
    "        result = pd.merge(result, consecutive_days_df, on=['region', 'date'])\n",
    "    \n",
    "    # Step 5: Add month and season indicators\n",
    "    result['month'] = result['date'].dt.month\n",
    "    result['year'] = result['date'].dt.year\n",
    "    \n",
    "    # Add season based on month\n",
    "    season_map = {\n",
    "        12: 'Winter', 1: 'Winter', 2: 'Winter',\n",
    "        3: 'Spring', 4: 'Spring', 5: 'Spring',\n",
    "        6: 'Summer', 7: 'Summer', 8: 'Summer',\n",
    "        9: 'Autumn', 10: 'Autumn', 11: 'Autumn'\n",
    "    }\n",
    "    result['season'] = result['month'].map(season_map)\n",
    "    \n",
    "    # Add cyclical encoding of month (captures seasonality)\n",
    "    result['month_sin'] = np.sin(2 * np.pi * result['month'] / 12)\n",
    "    result['month_cos'] = np.cos(2 * np.pi * result['month'] / 12)\n",
    "    \n",
    "    # Step 6: Add region-specific metrics\n",
    "    # Calculate fracture rate per region (per 100,000 population)\n",
    "    if 'population' in df.columns:\n",
    "        # First get the average population per region per month\n",
    "        region_population = df.groupby(['region', pd.Grouper(key='date', freq='M')])['population'].mean().reset_index()\n",
    "        region_population = region_population.rename(columns={'population': 'avg_population'})\n",
    "        \n",
    "        # Merge with the result\n",
    "        result = pd.merge(result, region_population, on=['region', 'date'])\n",
    "        \n",
    "        # Calculate fracture rate\n",
    "        result['fracture_rate'] = (result['hip_fracture_count'] / result['avg_population']) * 100000\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f74260b0-5278-4bd4-92e2-86ab3e31f1cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf3ec5ec-feb4-491a-9f1c-b9a164f3daf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_demographic_monthly_data(df):\n",
    "    \"\"\"\n",
    "    Transform daily data into monthly demographic aggregates.\n",
    "    \n",
    "    This function groups data by age group, gender, and month, calculating\n",
    "    fracture counts and relevant metrics for each demographic group.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : DataFrame\n",
    "        The raw daily data with hip fracture counts and demographic indicators\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    DataFrame\n",
    "        Monthly aggregated data by demographic group\n",
    "    \"\"\"\n",
    "    # Step 1: Group by age group, gender, and month\n",
    "    demographic_monthly = df.groupby(['age_group', 'gender', pd.Grouper(key='date', freq='M')]).agg({\n",
    "        'hip_fracture_count': 'sum',       # Sum fractures by demographic group\n",
    "        \n",
    "        # Weather indicators affect all demographic groups the same way within a time period\n",
    "        'has_weather_event': 'max',\n",
    "        'cold_weather': 'max',\n",
    "        'wind_weather': 'max',\n",
    "        'precipitation': 'max',\n",
    "        'heat_weather': 'max',\n",
    "    }).reset_index()\n",
    "    \n",
    "    # Step 2: Add broader age categories for more stable analysis\n",
    "    # Define age risk categories\n",
    "    def map_age_to_risk(age):\n",
    "        if age in ['60-64', '65-69']:\n",
    "            return 'Senior (60-69)'\n",
    "        elif age in ['70-74', '75-79']:\n",
    "            return 'Elderly (70-79)'\n",
    "        elif age in ['80-84', '85 years and over']:\n",
    "            return 'Very Elderly (80+)'\n",
    "        else:\n",
    "            return age  # Keep original if it doesn't match our categories\n",
    "    \n",
    "    demographic_monthly['age_category'] = demographic_monthly['age_group'].apply(map_age_to_risk)\n",
    "    \n",
    "    # Step 3: Calculate demographic-specific rates if population data is available\n",
    "    if 'population' in df.columns:\n",
    "        # Get population by demographic group\n",
    "        demo_population = df.groupby(['age_group', 'gender', pd.Grouper(key='date', freq='M')])['population'].mean().reset_index()\n",
    "        demo_population = demo_population.rename(columns={'population': 'avg_population'})\n",
    "        \n",
    "        # Merge with demographic data\n",
    "        demographic_monthly = pd.merge(demographic_monthly, demo_population, \n",
    "                                      on=['age_group', 'gender', 'date'])\n",
    "        \n",
    "        # Calculate fracture rate per 100,000 in demographic group\n",
    "        demographic_monthly['fracture_rate'] = (demographic_monthly['hip_fracture_count'] / \n",
    "                                               demographic_monthly['avg_population']) * 100000\n",
    "    \n",
    "    # Step 4: Add month and season indicators\n",
    "    demographic_monthly['month'] = demographic_monthly['date'].dt.month\n",
    "    demographic_monthly['year'] = demographic_monthly['date'].dt.year\n",
    "    \n",
    "    # Add season based on month\n",
    "    season_map = {\n",
    "        12: 'Winter', 1: 'Winter', 2: 'Winter',\n",
    "        3: 'Spring', 4: 'Spring', 5: 'Spring',\n",
    "        6: 'Summer', 7: 'Summer', 8: 'Summer',\n",
    "        9: 'Autumn', 10: 'Autumn', 11: 'Autumn'\n",
    "    }\n",
    "    demographic_monthly['season'] = demographic_monthly['month'].map(season_map)\n",
    "    \n",
    "    # Add cyclical encoding of month\n",
    "    demographic_monthly['month_sin'] = np.sin(2 * np.pi * demographic_monthly['month'] / 12)\n",
    "    demographic_monthly['month_cos'] = np.cos(2 * np.pi * demographic_monthly['month'] / 12)\n",
    "    \n",
    "    return demographic_monthly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "588d75d3-83e3-4bce-96dd-cb1edec177d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_consecutive_weather_days(df):\n",
    "    \"\"\"\n",
    "    Calculate the maximum consecutive days for various weather conditions.\n",
    "    \n",
    "    This function analyzes daily weather data to identify patterns of\n",
    "    consecutive days with various weather conditions. These metrics can\n",
    "    reveal sustained weather events that may have cumulative effects\n",
    "    on health outcomes.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : DataFrame\n",
    "        Daily data with weather indicators\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    DataFrame\n",
    "        Monthly aggregated consecutive day metrics\n",
    "    \"\"\"\n",
    "    # Helper function to calculate consecutive days\n",
    "    def calc_consecutive_days(series):\n",
    "        \"\"\"Calculate maximum consecutive days with a condition\"\"\"\n",
    "        if len(series) == 0:\n",
    "            return 0\n",
    "            \n",
    "        # Convert to numpy for performance\n",
    "        arr = series.values\n",
    "        \n",
    "        # Find runs of consecutive True values\n",
    "        changes = np.diff(np.concatenate(([0], arr, [0])))\n",
    "        starts = np.where(changes == 1)[0]\n",
    "        ends = np.where(changes == -1)[0]\n",
    "        lengths = ends - starts\n",
    "        \n",
    "        return np.max(lengths) if lengths.size > 0 else 0\n",
    "    \n",
    "    # Aggregate weather indicators at the daily level\n",
    "    daily_weather = df.groupby('date').agg({\n",
    "        'has_weather_event': 'max',\n",
    "        'cold_weather': 'max',\n",
    "        'wind_weather': 'max',\n",
    "        'precipitation': 'max',\n",
    "        'yellow_warning': 'max',\n",
    "        'orange_warning': 'max',\n",
    "        'red_warning': 'max',\n",
    "    }).reset_index()\n",
    "    \n",
    "    # Calculate consecutive days metrics by month\n",
    "    monthly_consecutive = daily_weather.groupby(pd.Grouper(key='date', freq='M')).apply(\n",
    "        lambda x: pd.Series({\n",
    "            'max_consecutive_weather_days': calc_consecutive_days(x['has_weather_event']),\n",
    "            'max_consecutive_cold_days': calc_consecutive_days(x['cold_weather']),\n",
    "            'max_consecutive_wind_days': calc_consecutive_days(x['wind_weather']),\n",
    "            'max_consecutive_precip_days': calc_consecutive_days(x['precipitation']),\n",
    "            'max_consecutive_yellow_days': calc_consecutive_days(x['yellow_warning']),\n",
    "            'max_consecutive_orange_days': calc_consecutive_days(x['orange_warning']),\n",
    "            'max_consecutive_red_days': calc_consecutive_days(x['red_warning']),\n",
    "        })\n",
    "    ).reset_index()\n",
    "    \n",
    "    return monthly_consecutive\n",
    "\n",
    "\n",
    "def add_risk_level_aggregation(demographic_monthly):\n",
    "    \"\"\"\n",
    "    Create an aggregation by risk level from demographic data.\n",
    "    \n",
    "    This function takes demographic monthly data and creates a new\n",
    "    dataset aggregated by risk level, which categorizes age groups\n",
    "    into clinically relevant risk tiers for hip fractures.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    demographic_monthly : DataFrame\n",
    "        Monthly data aggregated by demographic groups\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    DataFrame\n",
    "        Monthly data aggregated by risk level\n",
    "    \"\"\"\n",
    "    # Define risk levels for different age groups\n",
    "    age_risk_mapping = {\n",
    "        '60-64': 'Moderate Risk',\n",
    "        '65-69': 'Moderate Risk',\n",
    "        '70-74': 'High Risk',\n",
    "        '75-79': 'High Risk',\n",
    "        '80-84': 'Very High Risk',\n",
    "        '85 years and over': 'Very High Risk'\n",
    "    }\n",
    "    \n",
    "    # Make a copy to avoid modifying the original\n",
    "    demographic_copy = demographic_monthly.copy()\n",
    "    \n",
    "    # Add risk level to demographic data\n",
    "    demographic_copy['risk_level'] = demographic_copy['age_group'].map(age_risk_mapping)\n",
    "    \n",
    "    # Create age-risk level aggregations\n",
    "    risk_monthly = demographic_copy.groupby(['risk_level', pd.Grouper(key='date', freq='M')]).agg({\n",
    "        'hip_fracture_count': 'sum',\n",
    "        'has_weather_event': 'max',\n",
    "        'cold_weather': 'max',\n",
    "        'wind_weather': 'max',\n",
    "        'precipitation': 'max',\n",
    "    }).reset_index()\n",
    "    \n",
    "    # Add month and seasonal indicators\n",
    "    risk_monthly['month'] = risk_monthly['date'].dt.month\n",
    "    risk_monthly['year'] = risk_monthly['date'].dt.year\n",
    "    \n",
    "    # Add cyclical encoding of month\n",
    "    risk_monthly['month_sin'] = np.sin(2 * np.pi * risk_monthly['month'] / 12)\n",
    "    risk_monthly['month_cos'] = np.cos(2 * np.pi * risk_monthly['month'] / 12)\n",
    "    \n",
    "    # Add season based on month\n",
    "    season_map = {\n",
    "        12: 'Winter', 1: 'Winter', 2: 'Winter',\n",
    "        3: 'Spring', 4: 'Spring', 5: 'Spring',\n",
    "        6: 'Summer', 7: 'Summer', 8: 'Summer',\n",
    "        9: 'Autumn', 10: 'Autumn', 11: 'Autumn'\n",
    "    }\n",
    "    risk_monthly['season'] = risk_monthly['month'].map(season_map)\n",
    "    \n",
    "    # Add fracture rate if population data is available\n",
    "    if 'avg_population' in demographic_copy.columns:\n",
    "        # Aggregate population by risk level\n",
    "        risk_population = demographic_copy.groupby(['risk_level', pd.Grouper(key='date', freq='M')])['avg_population'].sum().reset_index()\n",
    "        \n",
    "        # Merge with risk data\n",
    "        risk_monthly = pd.merge(risk_monthly, risk_population, on=['risk_level', 'date'])\n",
    "        \n",
    "        # Calculate fracture rate per 100,000\n",
    "        risk_monthly['fracture_rate'] = (risk_monthly['hip_fracture_count'] / risk_monthly['avg_population']) * 100000\n",
    "    \n",
    "    # Add hierarchy level identifier\n",
    "    risk_monthly['level'] = 'Risk'\n",
    "    risk_monthly['id'] = risk_monthly['risk_level']\n",
    "    \n",
    "    return risk_monthly\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73f20b34-d6c4-4284-a093-19da50fc6113",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_hierarchical_data(df):\n",
    "    \"\"\"\n",
    "    Create a comprehensive hierarchical dataset for hip fracture forecasting.\n",
    "    \n",
    "    This function uses a modular approach to create data at four levels:\n",
    "    1. National level - monthly data for the entire country\n",
    "    2. Regional level - monthly data broken down by region\n",
    "    3. Demographic level - monthly data by age group and gender\n",
    "    4. Risk level - monthly data aggregated by clinical risk categories\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : DataFrame\n",
    "        Raw daily data with hip fracture counts and indicators\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    tuple\n",
    "        (national_monthly, regional_monthly, demographic_monthly, risk_monthly)\n",
    "    \"\"\"\n",
    "    # Import necessary libraries\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    \n",
    "    # Step 1: Create national monthly data\n",
    "    national_monthly = create_national_monthly_data(df)\n",
    "    \n",
    "    # Step 2: Add consecutive weather patterns to national data\n",
    "    consecutive_weather = calculate_consecutive_weather_days(df)\n",
    "    national_monthly = pd.merge(national_monthly, consecutive_weather, on='date')\n",
    "    \n",
    "    # Step 3: Create regional monthly data\n",
    "    regional_monthly = create_regional_monthly_data(df)\n",
    "    \n",
    "    # Step 4: Create demographic monthly data\n",
    "    demographic_monthly = create_demographic_monthly_data(df)\n",
    "    \n",
    "    # Step 5: Create risk level data\n",
    "    risk_monthly = add_risk_level_aggregation(demographic_monthly)\n",
    "    \n",
    "    # Step 6: Add hierarchical identifiers to national and regional data\n",
    "    # (demographic and risk already have these from their creation functions)\n",
    "    national_monthly['level'] = 'National'\n",
    "    national_monthly['id'] = 'Total'\n",
    "    \n",
    "    regional_monthly['level'] = 'Regional'\n",
    "    regional_monthly['id'] = regional_monthly['region']\n",
    "    \n",
    "    demographic_monthly['level'] = 'Demographic'\n",
    "    demographic_monthly['id'] = demographic_monthly['age_group'] + '_' + demographic_monthly['gender']\n",
    "    \n",
    "    return national_monthly, regional_monthly, demographic_monthly, risk_monthly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bb46c17-ac90-4f08-9610-10e5f02523f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process training data\n",
    "national_train, regional_train, demographic_train, risk_train = create_hierarchical_data(merged_reg)\n",
    "\n",
    "# Process unseen data (2024)\n",
    "national_unseen, regional_unseen, demographic_unseen, risk_unseen = create_hierarchical_data(df_merged_with_weather)\n",
    "\n",
    "# Verify the data is consistent\n",
    "print(f\"National data: {national_train.shape} train, {national_unseen.shape} unseen\")\n",
    "print(f\"Regional data: {regional_train.shape} train, {regional_unseen.shape} unseen\")\n",
    "print(f\"Demographic data: {demographic_train.shape} train, {demographic_unseen.shape} unseen\")\n",
    "print(f\"Risk-level data: {risk_train.shape} train, {risk_unseen.shape} unseen\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ac982f3-fe2e-4038-af1c-4d28d39fdc2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For ARIMA, we need time series objects\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "\n",
    "# National level ARIMA\n",
    "national_ts = national_train.set_index('date')['hip_fracture_count']\n",
    "\n",
    "# Function to prepare Prophet data\n",
    "def prepare_prophet_data(df, target='hip_fracture_count'):\n",
    "    # Prophet requires columns named 'ds' and 'y'\n",
    "    prophet_df = df.rename(columns={'date': 'ds', target: 'y'})\n",
    "    \n",
    "    # Add additional regressors\n",
    "    exog_columns = ['days_with_cold', 'days_with_wind', 'days_with_precipitation',\n",
    "                   'max_consecutive_weather_days', 'days_with_yellow', \n",
    "                   'days_with_orange', 'days_with_red']\n",
    "    \n",
    "    # Only keep columns that exist in the dataframe\n",
    "    exog_columns = [col for col in exog_columns if col in prophet_df.columns]\n",
    "    \n",
    "    # Final dataframe with target and regressors\n",
    "    return prophet_df[['ds', 'y'] + exog_columns]\n",
    "\n",
    "# Prepare national data for Prophet\n",
    "national_prophet = prepare_prophet_data(national_train)\n",
    "\n",
    "# Prepare regional data for Prophet\n",
    "regional_prophet = {}\n",
    "for region in regional_train['region'].unique():\n",
    "    region_data = regional_train[regional_train['region'] == region]\n",
    "    regional_prophet[region] = prepare_prophet_data(region_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45fdcb69-a474-4067-829b-5a8772e48fd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "import pmdarima as pm\n",
    "from prophet import Prophet\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acac0ad4-03f2-4f92-a43d-a6ccb8c94b43",
   "metadata": {},
   "source": [
    "## New features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fb7fe7d-8edd-418d-aaef-dc03b201b2b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create more targeted interaction features\n",
    "def create_enhanced_features(df):\n",
    "    # Copy the dataframe to avoid modifying the original\n",
    "    enhanced_df = df.copy()\n",
    "    \n",
    "    # Hip fracture and weather interaction (you already have this)\n",
    "    enhanced_df['hip_weather_interaction'] = enhanced_df['hip_fracture_count'] * enhanced_df['has_weather_event']\n",
    "    \n",
    "    # Create age group categories for easier analysis\n",
    "    age_groups = {\n",
    "        'elderly': ['75-79', '80-84', '85 years and over'],\n",
    "        'senior': ['60-64', '65-69', '70-74']\n",
    "    }\n",
    "    \n",
    "    # Create gender-specific features\n",
    "    enhanced_df['female_ratio'] = enhanced_df['female'] / (enhanced_df['female'] + enhanced_df['male'])\n",
    "    \n",
    "    # Create interaction between weather and demographic groups\n",
    "    if 'age_group' in enhanced_df.columns:\n",
    "        enhanced_df['elderly_weather'] = ((enhanced_df['age_group'].isin(age_groups['elderly'])) & \n",
    "                                         (enhanced_df['has_weather_event'] == 1)).astype(int)\n",
    "        \n",
    "        enhanced_df['senior_weather'] = ((enhanced_df['age_group'].isin(age_groups['senior'])) & \n",
    "                                        (enhanced_df['has_weather_event'] == 1)).astype(int)\n",
    "    \n",
    "    # Create seasonal interaction features\n",
    "    enhanced_df['winter_weather'] = enhanced_df['is_winter'] * enhanced_df['has_weather_event']\n",
    "    enhanced_df['cold_weather_intensity'] = enhanced_df['cold_weather'] * enhanced_df['warning_severity_numeric']\n",
    "    \n",
    "    # Create thresholds for extreme weather events (if warning_severity_numeric exists)\n",
    "    if 'warning_severity_numeric' in enhanced_df.columns:\n",
    "        enhanced_df['severe_weather_event'] = (enhanced_df['warning_severity_numeric'] >= 2).astype(int)\n",
    "        enhanced_df['extreme_weather_event'] = (enhanced_df['warning_severity_numeric'] == 3).astype(int)\n",
    "    \n",
    "    return enhanced_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9d24928-140e-468a-b0a1-80f0f5d31424",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_enhanced_monthly_data(df):\n",
    "    # First create the enhanced features\n",
    "    enhanced_df = create_enhanced_features(df)\n",
    "    \n",
    "    # Define aggregation functions for different feature types\n",
    "    agg_functions = {\n",
    "        'hip_fracture_count': 'sum',\n",
    "        'hip_weather_interaction': 'sum',\n",
    "        'has_weather_event': 'mean',  # Percentage of days with weather events\n",
    "        'cold_weather': 'mean',       # Percentage of days with cold weather\n",
    "        'yellow_warning': 'mean',     # Percentage of days with yellow warnings\n",
    "        'orange_warning': 'mean',     # Percentage of days with orange warnings\n",
    "        'red_warning': 'mean',        # Percentage of days with red warnings\n",
    "        'female_ratio': 'mean',       # Average female ratio\n",
    "        'winter_weather': 'mean',     # Percentage of winter days with weather events\n",
    "        'severe_weather_event': 'mean', # Percentage of days with severe weather\n",
    "        'extreme_weather_event': 'mean', # Percentage of days with extreme weather\n",
    "    }\n",
    "    \n",
    "    # Only include columns that exist in the dataframe\n",
    "    agg_columns = {col: func for col, func in agg_functions.items() \n",
    "                  if col in enhanced_df.columns}\n",
    "    \n",
    "    # Aggregate to monthly level\n",
    "    monthly_df = enhanced_df.groupby(pd.Grouper(key='date', freq='M')).agg(agg_columns).reset_index()\n",
    "    \n",
    "    # Add month and season indicators\n",
    "    monthly_df['month'] = monthly_df['date'].dt.month\n",
    "    monthly_df['year'] = monthly_df['date'].dt.year\n",
    "    \n",
    "    # Add cyclical encoding of month\n",
    "    monthly_df['month_sin'] = np.sin(2 * np.pi * monthly_df['month'] / 12)\n",
    "    monthly_df['month_cos'] = np.cos(2 * np.pi * monthly_df['month'] / 12)\n",
    "    \n",
    "    return monthly_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48f68143-f864-4bfa-bd62-a576fe72334c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_lag_features(df, target_col='hip_fracture_count', lags=[1, 7, 30, 365]):\n",
    "    \"\"\"\n",
    "    Create lag features for time series data\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : DataFrame\n",
    "        Time series data with a date column\n",
    "    target_col : str\n",
    "        Name of the target column to lag\n",
    "    lags : list\n",
    "        List of lag periods to create\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    DataFrame\n",
    "        DataFrame with added lag features\n",
    "    \"\"\"\n",
    "    # Make a copy to avoid modifying the original\n",
    "    df_with_lags = df.copy()\n",
    "    \n",
    "    # Ensure the dataframe is sorted by date\n",
    "    if 'date' in df_with_lags.columns:\n",
    "        df_with_lags = df_with_lags.sort_values('date')\n",
    "    \n",
    "    # Store NaN positions to restore later\n",
    "    nan_mask = df_with_lags[target_col].isna()\n",
    "    \n",
    "    # Temporarily fill NaNs with 0 for calculating lags\n",
    "    temp_values = df_with_lags[target_col].fillna(0)\n",
    "    \n",
    "    # Create each lag feature without modifying the original target column\n",
    "    for lag in lags:\n",
    "        lag_name = f'{target_col}_lag{lag}'\n",
    "        df_with_lags[lag_name] = temp_values.shift(lag)\n",
    "    \n",
    "    # No need to restore NaNs to original column since we never modified it\n",
    "    \n",
    "    return df_with_lags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a14b8fd3-8efb-4a12-8fae-752f8c0bca7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_prophet_components(model, forecast):\n",
    "    \"\"\"Analyze how different components contribute to the Prophet forecast\"\"\"\n",
    "    \n",
    "    # Extract components\n",
    "    components = ['trend', 'yearly']\n",
    "    \n",
    "    # Plot the component contributions\n",
    "    plt.figure(figsize=(14, 10))\n",
    "    \n",
    "    # Plot each component\n",
    "    for i, component in enumerate(components, 1):\n",
    "        if component in forecast.columns:\n",
    "            plt.subplot(len(components) + 1, 1, i)\n",
    "            plt.plot(forecast['ds'], forecast[component])\n",
    "            plt.title(f'{component.capitalize()} Component')\n",
    "            plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot the final prediction\n",
    "    plt.subplot(len(components) + 1, 1, len(components) + 1)\n",
    "    plt.plot(forecast['ds'], forecast['yhat'], label='Forecast')\n",
    "    \n",
    "    # If we have actual values, plot them too\n",
    "    if 'y' in forecast.columns:\n",
    "        plt.plot(forecast['ds'], forecast['y'], 'k.', label='Actual')\n",
    "    \n",
    "    plt.title('Overall Forecast')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Calculate contribution percentages\n",
    "    if len(components) > 0:\n",
    "        component_ranges = {}\n",
    "        for component in components:\n",
    "            if component in forecast.columns:\n",
    "                component_range = forecast[component].max() - forecast[component].min()\n",
    "                component_ranges[component] = component_range\n",
    "        \n",
    "        total_range = sum(component_ranges.values())\n",
    "        \n",
    "        print(\"\\nComponent Contribution Analysis:\")\n",
    "        for component, range_val in component_ranges.items():\n",
    "            contribution = (range_val / total_range) * 100 if total_range > 0 else 0\n",
    "            print(f\"{component.capitalize()}: {contribution:.1f}% of forecast variation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ab1c68f-6fce-4a70-bac4-d4122bdd0eda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to aggregate to monthly with both lags and enhanced features\n",
    "def create_monthly_data_with_features(df, target_col='hip_fracture_count'):\n",
    "    \"\"\"Create monthly data with lag features and enhanced interactions\"\"\"\n",
    "    # First add lag features\n",
    "    lags = [1, 7, 30, 90, 365]  # Days to lag\n",
    "    df_with_lags = create_lag_features(df, target_col, lags)\n",
    "    \n",
    "    # Then add enhanced features\n",
    "    enhanced_df = create_enhanced_features(df_with_lags)\n",
    "    \n",
    "    # Define aggregation functions for different feature types\n",
    "    agg_functions = {\n",
    "        'hip_fracture_count': 'sum',\n",
    "        'has_weather_event': 'mean',  # Percentage of days with weather events\n",
    "        'cold_weather': 'mean',       # Percentage of days with cold weather\n",
    "        'yellow_warning': 'mean',     # Percentage of days with yellow warnings\n",
    "        'orange_warning': 'mean',     # Percentage of days with orange warnings\n",
    "        'red_warning': 'mean',        # Percentage of days with red warnings\n",
    "    }\n",
    "    \n",
    "    # Add lag features aggregation\n",
    "    for lag in lags:\n",
    "        lag_col = f'{target_col}_lag{lag}'\n",
    "        if lag_col in enhanced_df.columns:\n",
    "            agg_functions[lag_col] = 'mean'  # Use mean for the lagged values\n",
    "    \n",
    "    # Add enhanced features aggregation\n",
    "    enhanced_aggs = {\n",
    "        'hip_weather_interaction': 'sum',\n",
    "        'female_ratio': 'mean',\n",
    "        'winter_weather': 'mean',\n",
    "        'cold_weather_intensity': 'mean',\n",
    "        'severe_weather_event': 'mean',\n",
    "        'extreme_weather_event': 'mean'\n",
    "    }\n",
    "    agg_functions.update(enhanced_aggs)\n",
    "    \n",
    "    # Only include columns that exist in the dataframe\n",
    "    agg_columns = {col: func for col, func in agg_functions.items() \n",
    "                  if col in enhanced_df.columns}\n",
    "    \n",
    "    # Aggregate to monthly level\n",
    "    monthly_df = enhanced_df.groupby(pd.Grouper(key='date', freq='M')).agg(agg_columns).reset_index()\n",
    "    \n",
    "    # Add month and season indicators\n",
    "    monthly_df['month'] = monthly_df['date'].dt.month\n",
    "    monthly_df['year'] = monthly_df['date'].dt.year\n",
    "    \n",
    "    # Add cyclical encoding of month\n",
    "    monthly_df['month_sin'] = np.sin(2 * np.pi * monthly_df['month'] / 12)\n",
    "    monthly_df['month_cos'] = np.cos(2 * np.pi * monthly_df['month'] / 12)\n",
    "    \n",
    "    return monthly_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b22b0fa8-2ceb-403a-8190-0f6fccb0d85a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the training data\n",
    "def prepare_prophet_training_data(training_df):\n",
    "    \"\"\"Prepare the training data for Prophet\"\"\"\n",
    "    \n",
    "    # Create monthly data with lag features and enhanced interactions\n",
    "    monthly_train = create_monthly_data_with_features(training_df)\n",
    "    \n",
    "    # Drop rows with NaN values from the lags at the beginning of the series\n",
    "    monthly_train_clean = monthly_train.dropna()\n",
    "    \n",
    "    print(f\"Training data shape after creating features: {monthly_train.shape}\")\n",
    "    print(f\"Training data shape after removing NaN values: {monthly_train_clean.shape}\")\n",
    "    \n",
    "    return monthly_train_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "470e873a-f9ec-4ab9-9b53-d039bbb60dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_prophet_with_features(monthly_train):\n",
    "    \"\"\"Train a Prophet model with lag features and enhanced interactions\"\"\"\n",
    "    \n",
    "    # Prepare data for Prophet\n",
    "    prophet_df = monthly_train.rename(columns={'date': 'ds', 'hip_fracture_count': 'y'})\n",
    "    \n",
    "    # Define which features to use as regressors\n",
    "    potential_regressors = [\n",
    "        # Lag features\n",
    "        'hip_fracture_count_lag30',   # Previous month\n",
    "        'hip_fracture_count_lag90',   # Previous quarter\n",
    "        'hip_fracture_count_lag365',  # Same month last year\n",
    "        \n",
    "        # Weather interactions\n",
    "        'hip_weather_interaction',\n",
    "        'winter_weather',\n",
    "        'cold_weather_intensity',\n",
    "        \n",
    "        # Basic weather\n",
    "        'has_weather_event',\n",
    "        'cold_weather',\n",
    "        'severe_weather_event',\n",
    "        \n",
    "        # Demographics\n",
    "        'female_ratio',\n",
    "        \n",
    "        # Cyclical features\n",
    "        'month_sin',\n",
    "        'month_cos'\n",
    "    ]\n",
    "    \n",
    "    # Only use regressors that exist in the dataframe\n",
    "    regressors = [col for col in potential_regressors if col in prophet_df.columns]\n",
    "    \n",
    "    print(f\"Using these regressors in Prophet model: {regressors}\")\n",
    "    \n",
    "    # Initialize Prophet model\n",
    "    model = Prophet(\n",
    "        yearly_seasonality=True,         # Annual pattern\n",
    "        weekly_seasonality=False,        # No weekly pattern for monthly data\n",
    "        daily_seasonality=False,         # No daily pattern for monthly data\n",
    "        seasonality_mode='multiplicative', # Multiplicative seasonality\n",
    "        interval_width=0.95,            # 95% prediction intervals\n",
    "        changepoint_prior_scale=0.05    # Flexibility for trend changes\n",
    "    )\n",
    "    \n",
    "    # Add the regressors\n",
    "    for regressor in regressors:\n",
    "        model.add_regressor(regressor)\n",
    "    \n",
    "    # Fit the model\n",
    "    model.fit(prophet_df)\n",
    "    \n",
    "    return model, regressors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c2ed4d8-9dff-49fd-aad9-e42185176677",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_prophet_model_with_monthly_splits(df_merged_train, n_splits=3, test_size_months=6):\n",
    "    \"\"\"\n",
    "    Validate the Prophet model using monthly time-based splits\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df_merged_train : DataFrame\n",
    "        The dataset with date and target variable (hip_fracture_count)\n",
    "    n_splits : int\n",
    "        Number of splits for time series cross-validation\n",
    "    test_size_months : int\n",
    "        Size of each test set in months\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    dict with evaluation metrics, trained models, and forecasts\n",
    "    \"\"\"\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "    from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "    \n",
    "    # Sort the data by date\n",
    "    df_sorted = df_merged_train.sort_values('date')\n",
    "    \n",
    "    # Extract month information\n",
    "    df_sorted['month'] = df_sorted['date'].dt.to_period('M')\n",
    "    \n",
    "    # Get unique months in order\n",
    "    unique_months = sorted(df_sorted['month'].unique())\n",
    "    \n",
    "    # Determine the total number of months available\n",
    "    n_months = len(unique_months)\n",
    "    print(f\"Dataset contains {n_months} months of data\")\n",
    "    \n",
    "    # Check if we have enough data for the requested splits\n",
    "    total_required_months = n_splits * test_size_months\n",
    "    if total_required_months > n_months:\n",
    "        print(f\"Warning: Not enough data for {n_splits} splits with {test_size_months} months each.\")\n",
    "        print(f\"Reducing to {n_months // test_size_months} splits.\")\n",
    "        n_splits = max(1, n_months // test_size_months)\n",
    "    \n",
    "    # Calculate where to start the test sets\n",
    "    # We want the test sets to end at the end of the data\n",
    "    # So we calculate backwards from the end\n",
    "    test_start_indices = []\n",
    "    for i in range(n_splits):\n",
    "        # Start from the end and move backwards\n",
    "        test_start_idx = n_months - (i + 1) * test_size_months\n",
    "        if test_start_idx < 0:\n",
    "            # Not enough data for this split\n",
    "            break\n",
    "        test_start_indices.append(test_start_idx)\n",
    "    \n",
    "    # Reverse so we go chronologically\n",
    "    test_start_indices.reverse()\n",
    "    \n",
    "    # Initialize storage for results\n",
    "    results = {\n",
    "        'models': [],\n",
    "        'regressors': [],\n",
    "        'forecasts': [],\n",
    "        'metrics': [],\n",
    "        'train_periods': [],\n",
    "        'test_periods': []\n",
    "    }\n",
    "    \n",
    "    # Create figure for all validation plots\n",
    "    n_actual_splits = len(test_start_indices)\n",
    "    fig, axes = plt.subplots(n_actual_splits, 1, figsize=(12, 6*n_actual_splits))\n",
    "    if n_actual_splits == 1:\n",
    "        axes = [axes]\n",
    "    \n",
    "    # Perform time series validation with the calculated splits\n",
    "    for i, test_start_idx in enumerate(test_start_indices):\n",
    "        # Define the months for this split\n",
    "        train_months = unique_months[:test_start_idx]\n",
    "        test_months = unique_months[test_start_idx:test_start_idx + test_size_months]\n",
    "        \n",
    "        if len(train_months) == 0 or len(test_months) == 0:\n",
    "            print(f\"Warning: Split {i+1} has empty train or test set. Skipping.\")\n",
    "            continue\n",
    "        \n",
    "        # Convert periods to timestamps for display\n",
    "        train_start = train_months[0].to_timestamp()\n",
    "        train_end = train_months[-1].to_timestamp()\n",
    "        test_start = test_months[0].to_timestamp()\n",
    "        test_end = test_months[-1].to_timestamp()\n",
    "        \n",
    "        print(f\"\\n--- Split {i+1}/{n_actual_splits} ---\")\n",
    "        print(f\"Training period: {train_start.strftime('%Y-%m')} to {train_end.strftime('%Y-%m')}\")\n",
    "        print(f\"Testing period: {test_start.strftime('%Y-%m')} to {test_end.strftime('%Y-%m')}\")\n",
    "        \n",
    "        # Get training and test data based on months\n",
    "        train_data = df_sorted[df_sorted['month'].isin(train_months)]\n",
    "        test_data = df_sorted[df_sorted['month'].isin(test_months)]\n",
    "        \n",
    "        print(f\"Training data shape: {train_data.shape}\")\n",
    "        print(f\"Test data shape: {test_data.shape}\")\n",
    "        \n",
    "        # Prepare training data with features\n",
    "        monthly_train = prepare_prophet_training_data(train_data)\n",
    "        \n",
    "        # Train the model\n",
    "        model, regressors = train_prophet_with_features(monthly_train)\n",
    "        \n",
    "        # Now prepare test data WITH the same lag features\n",
    "        # We need to include the training data to calculate lags correctly\n",
    "        combined_data = pd.concat([train_data, test_data]).sort_values('date')\n",
    "        monthly_combined = create_monthly_data_with_features(combined_data)\n",
    "        \n",
    "        # Extract just the test period - using proper date filtering\n",
    "        test_start_date = test_months[0].to_timestamp()\n",
    "        test_end_date = (test_months[-1] + 1).to_timestamp() - pd.Timedelta(days=1)  # Last day of the month\n",
    "        monthly_test = monthly_combined[(monthly_combined['date'] >= test_start_date) & \n",
    "                                      (monthly_combined['date'] <= test_end_date)]\n",
    "        \n",
    "        # Check if monthly_test is empty\n",
    "        if len(monthly_test) == 0:\n",
    "            print(f\"Warning: No data found for test period. Skipping split {i+1}.\")\n",
    "            continue\n",
    "        \n",
    "        # Prepare for Prophet\n",
    "        prophet_test = monthly_test.rename(columns={'date': 'ds', 'hip_fracture_count': 'y'})\n",
    "        \n",
    "        # Ensure all regressors exist in the test data\n",
    "        for regressor in regressors:\n",
    "            if regressor not in prophet_test.columns:\n",
    "                print(f\"Warning: Regressor {regressor} missing from test data. Setting to 0.\")\n",
    "                prophet_test[regressor] = 0\n",
    "        \n",
    "        # Make predictions\n",
    "        test_forecast = model.predict(prophet_test)\n",
    "        \n",
    "        # Add actual values for comparison\n",
    "        test_forecast['y'] = prophet_test['y'].values\n",
    "        \n",
    "        # Calculate metrics\n",
    "        mae = mean_absolute_error(test_forecast['y'], test_forecast['yhat'])\n",
    "        rmse = np.sqrt(mean_squared_error(test_forecast['y'], test_forecast['yhat']))\n",
    "        r2 = r2_score(test_forecast['y'], test_forecast['yhat'])\n",
    "        \n",
    "        metrics = {\n",
    "            'split': i+1,\n",
    "            'mae': mae,\n",
    "            'rmse': rmse,\n",
    "            'r2': r2,\n",
    "            'train_period': (train_start, train_end),\n",
    "            'test_period': (test_start, test_end)\n",
    "        }\n",
    "        \n",
    "        print(f\"Prophet Model Performance on Test Data (Split {i+1}):\")\n",
    "        print(f\"Mean Absolute Error: {mae:.2f}\")\n",
    "        print(f\"Root Mean Squared Error: {rmse:.2f}\")\n",
    "        print(f\"R² Score: {r2:.4f}\")\n",
    "        \n",
    "        # Save results\n",
    "        results['models'].append(model)\n",
    "        results['regressors'].append(regressors)\n",
    "        results['forecasts'].append(test_forecast)\n",
    "        results['metrics'].append(metrics)\n",
    "        results['train_periods'].append((train_start, train_end))\n",
    "        results['test_periods'].append((test_start, test_end))\n",
    "        \n",
    "        # Plot forecast vs actuals for this split\n",
    "        ax = axes[i]\n",
    "        ax.plot(test_forecast['ds'], test_forecast['y'], 'ko', label='Actual')\n",
    "        ax.plot(test_forecast['ds'], test_forecast['yhat'], 'b-', label='Forecast')\n",
    "        \n",
    "        # Add prediction interval\n",
    "        ax.fill_between(test_forecast['ds'], \n",
    "                        test_forecast['yhat_lower'], \n",
    "                        test_forecast['yhat_upper'], \n",
    "                        color='blue', alpha=0.2)\n",
    "        \n",
    "        ax.set_title(f'Split {i+1}: Prophet Forecast (Test Period: {test_start.strftime(\"%Y-%m\")} to {test_end.strftime(\"%Y-%m\")})')\n",
    "        ax.legend()\n",
    "        ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Calculate and display average metrics across all splits\n",
    "    if results['metrics']:\n",
    "        avg_mae = np.mean([m['mae'] for m in results['metrics']])\n",
    "        avg_rmse = np.mean([m['rmse'] for m in results['metrics']])\n",
    "        # Filter out NaN R² values before calculating mean\n",
    "        r2_values = [m['r2'] for m in results['metrics'] if not np.isnan(m['r2'])]\n",
    "        if r2_values:\n",
    "            avg_r2 = np.mean(r2_values)\n",
    "            print(\"\\n--- Average Performance Across All Splits ---\")\n",
    "            print(f\"Mean Absolute Error: {avg_mae:.2f}\")\n",
    "            print(f\"Root Mean Squared Error: {avg_rmse:.2f}\")\n",
    "            print(f\"R² Score: {avg_r2:.4f}\")\n",
    "        else:\n",
    "            print(\"\\n--- Average Performance Across All Splits ---\")\n",
    "            print(f\"Mean Absolute Error: {avg_mae:.2f}\")\n",
    "            print(f\"Root Mean Squared Error: {avg_rmse:.2f}\")\n",
    "            print(\"R² Score: Could not be calculated (possibly constant values in test sets)\")\n",
    "    \n",
    "    # Optionally plot component plots for the last model\n",
    "    if results['models']:\n",
    "        final_model = results['models'][-1]\n",
    "        final_forecast = results['forecasts'][-1]\n",
    "        fig = final_model.plot_components(final_forecast)\n",
    "        plt.show()\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "def run_prophet_monthly_validation(df_merged_train, n_splits=3, test_size_months=6):\n",
    "    \"\"\"\n",
    "    Convenience function to run prophet validation with monthly splits\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df_merged_train : DataFrame\n",
    "        The dataset with date and target variable\n",
    "    n_splits : int\n",
    "        Number of splits for time series validation\n",
    "    test_size_months : int\n",
    "        Size of each test set in months\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    Validation results\n",
    "    \"\"\"\n",
    "    # Run the validation\n",
    "    results = validate_prophet_model_with_monthly_splits(\n",
    "        df_merged_train,\n",
    "        n_splits=n_splits,\n",
    "        test_size_months=test_size_months\n",
    "    )\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f62841e2-f52e-443b-8ce6-56e3a49bbb8a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44283265-9567-4094-b570-0749c40040b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d7168d6-76d7-4aad-8b27-51deaf51c987",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_final_model_and_forecast(df_merged_train, df_merged_unseen):\n",
    "    \"\"\"\n",
    "    Train a model on all training data and forecast unseen data\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df_merged_train : DataFrame\n",
    "        The complete training dataset\n",
    "    df_merged_unseen : DataFrame\n",
    "        The unseen data to forecast\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    final_model, full_forecast, feature_importance\n",
    "    \"\"\"\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "    from prophet import Prophet\n",
    "    from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "    \n",
    "    # Prepare monthly data with features for training\n",
    "    monthly_train = create_monthly_data_with_features(df_merged_train)\n",
    "    \n",
    "    # Prepare for Prophet\n",
    "    prophet_train = monthly_train.rename(columns={'date': 'ds', 'hip_fracture_count': 'y'})\n",
    "    \n",
    "    # Get list of all potential regressors \n",
    "    all_regressors = [col for col in prophet_train.columns \n",
    "                     if col not in ['ds', 'y', 'month', 'year']]\n",
    "    \n",
    "    # Train Prophet model with all regressors\n",
    "    final_model = Prophet(\n",
    "        yearly_seasonality=False,\n",
    "        weekly_seasonality=False,\n",
    "        daily_seasonality=False,\n",
    "        seasonality_mode='multiplicative',\n",
    "        interval_width=0.95\n",
    "    )\n",
    "    \n",
    "    # Add custom monthly seasonality\n",
    "    final_model.add_seasonality(\n",
    "        name='monthly', \n",
    "        period=30.5, \n",
    "        fourier_order=5\n",
    "    )\n",
    "    \n",
    "    # Add regressors\n",
    "    for regressor in all_regressors:\n",
    "        final_model.add_regressor(regressor)\n",
    "    \n",
    "    # Fit model\n",
    "    final_model.fit(prophet_train)\n",
    "    \n",
    "    # Prepare unseen data with correct lag features\n",
    "    combined_data = pd.concat([df_merged_train, df_merged_unseen]).sort_values('date')\n",
    "    monthly_combined = create_monthly_data_with_features(combined_data)\n",
    "    \n",
    "    # Extract just the unseen period\n",
    "    monthly_unseen = monthly_combined[monthly_combined['date'] >= df_merged_unseen['date'].min()]\n",
    "    \n",
    "    # Prepare for Prophet\n",
    "    prophet_unseen = monthly_unseen.rename(columns={'date': 'ds', 'hip_fracture_count': 'y'})\n",
    "    \n",
    "    # Make predictions\n",
    "    full_forecast = final_model.predict(prophet_unseen)\n",
    "    \n",
    "    # Add actual values for comparison\n",
    "    full_forecast['y'] = prophet_unseen['y'].values\n",
    "    \n",
    "    # Calculate metrics\n",
    "    mae = mean_absolute_error(full_forecast['y'], full_forecast['yhat'])\n",
    "    rmse = np.sqrt(mean_squared_error(full_forecast['y'], full_forecast['yhat']))\n",
    "    r2 = r2_score(full_forecast['y'], full_forecast['yhat'])\n",
    "    \n",
    "    print(f\"\\nFull Model Performance on Unseen 2024 Data:\")\n",
    "    print(f\"Mean Absolute Error: {mae:.2f}\")\n",
    "    print(f\"Root Mean Squared Error: {rmse:.2f}\")\n",
    "    print(f\"R² Score: {r2:.4f}\")\n",
    "    \n",
    "    # Print monthly actual vs forecasted values\n",
    "    print(\"\\nMonthly Actual and Forecasted Hip Fracture Rates (Full Model):\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"{'Month':<12} {'Actual':<10} {'Forecast':<10} {'Lower 95%':<12} {'Upper 95%':<12} {'Diff':<10} {'Diff %':<10}\")\n",
    "    print(\"-\"*80)\n",
    "    \n",
    "    for i in range(len(full_forecast)):\n",
    "        # Format date as YYYY-MM\n",
    "        month = full_forecast['ds'].iloc[i].strftime('%Y-%m')\n",
    "        actual = full_forecast['y'].iloc[i]\n",
    "        forecast = full_forecast['yhat'].iloc[i]\n",
    "        lower = full_forecast['yhat_lower'].iloc[i]\n",
    "        upper = full_forecast['yhat_upper'].iloc[i]\n",
    "        \n",
    "        # Calculate difference and percentage difference\n",
    "        diff = forecast - actual\n",
    "        diff_pct = (diff / actual * 100) if actual != 0 else float('inf')\n",
    "        \n",
    "        print(f\"{month:<12} {actual:<10.2f} {forecast:<10.2f} {lower:<12.2f} {upper:<12.2f} {diff:<10.2f} {diff_pct:<10.2f}%\")\n",
    "    \n",
    "    # Calculate and print summary statistics\n",
    "    actuals = full_forecast['y']\n",
    "    forecasts = full_forecast['yhat']\n",
    "    \n",
    "    print(\"\\nSummary Statistics (Full Model):\")\n",
    "    print(f\"Actual:    Min: {actuals.min():.2f}, Max: {actuals.max():.2f}, Mean: {actuals.mean():.2f}, Median: {actuals.median():.2f}\")\n",
    "    print(f\"Forecast:  Min: {forecasts.min():.2f}, Max: {forecasts.max():.2f}, Mean: {forecasts.mean():.2f}, Median: {forecasts.median():.2f}\")\n",
    "    \n",
    "    # Visualize forecast vs actuals\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(full_forecast['ds'], full_forecast['y'], 'ko', label='Actual')\n",
    "    plt.plot(full_forecast['ds'], full_forecast['yhat'], 'r-', label='Forecast')\n",
    "    plt.fill_between(full_forecast['ds'], \n",
    "                   full_forecast['yhat_lower'], \n",
    "                   full_forecast['yhat_upper'], \n",
    "                   color='red', alpha=0.2)\n",
    "    plt.title('Full Data Model: Forecast for Unseen 2024 Data')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()\n",
    "    \n",
    "    # Calculate feature importance through perturbation analysis\n",
    "    feature_importance = calculate_perturbation_importance(\n",
    "        final_model, prophet_unseen, all_regressors\n",
    "    )\n",
    "    \n",
    "    return final_model, full_forecast, feature_importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "132f8791-cc8b-4e05-b167-5772c924a3e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_perturbation_importance(model, test_data, features):\n",
    "    \"\"\"Calculate feature importance by perturbing each feature\"\"\"\n",
    "    \n",
    "    # Make baseline prediction\n",
    "    baseline_forecast = model.predict(test_data)\n",
    "    baseline_predictions = baseline_forecast['yhat'].values\n",
    "    \n",
    "    # Calculate impact of each feature\n",
    "    feature_importance = {}\n",
    "    \n",
    "    for feature in features:\n",
    "        if feature in test_data.columns:\n",
    "            # Create a copy with this feature set to its mean\n",
    "            modified_df = test_data.copy()\n",
    "            feature_mean = modified_df[feature].mean()\n",
    "            modified_df[feature] = feature_mean\n",
    "            \n",
    "            # Make predictions\n",
    "            modified_forecast = model.predict(modified_df)\n",
    "            modified_predictions = modified_forecast['yhat'].values\n",
    "            \n",
    "            # Calculate the impact\n",
    "            impact = np.mean(np.abs(baseline_predictions - modified_predictions))\n",
    "            feature_importance[feature] = impact\n",
    "    \n",
    "    # Create DataFrame and sort\n",
    "    importance_df = pd.DataFrame({\n",
    "        'Feature': list(feature_importance.keys()),\n",
    "        'Importance': list(feature_importance.values())\n",
    "    })\n",
    "    \n",
    "    # Add relative importance\n",
    "    if len(importance_df) > 0 and importance_df['Importance'].sum() > 0:\n",
    "        importance_df['Relative_Importance'] = (\n",
    "            importance_df['Importance'] / importance_df['Importance'].sum() * 100\n",
    "        )\n",
    "        importance_df = importance_df.sort_values('Importance', ascending=False)\n",
    "        \n",
    "        # Visualize\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        plt.barh(importance_df['Feature'], importance_df['Relative_Importance'])\n",
    "        plt.xlabel('Relative Importance (%)')\n",
    "        plt.ylabel('Feature')\n",
    "        plt.title('Feature Importance by Perturbation')\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(\"No meaningful feature importance could be calculated\")\n",
    "    \n",
    "    return importance_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be985492-5dec-4ce1-99af-e417ca0f0275",
   "metadata": {},
   "source": [
    "def forecast_with_best_model(best_model, best_regressors, df_merged_train, df_merged_unseen):\n",
    "    \"\"\"\n",
    "    Use the best model from validation to forecast unseen data\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    best_model : Prophet model\n",
    "        The best performing model from cross-validation\n",
    "    best_regressors : list\n",
    "        The list of regressors used in the best model\n",
    "    df_merged_train : DataFrame\n",
    "        The complete training dataset\n",
    "    df_merged_unseen : DataFrame\n",
    "        The unseen data to forecast\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    unseen_forecast, feature_importance\n",
    "    \"\"\"\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "    from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "    \n",
    "    # Check if we're dealing with 2024 data\n",
    "    is_forecast_data = df_merged_unseen['date'].min().year >= 2024\n",
    "    \n",
    "    # Prepare unseen data with correct lag features\n",
    "    # We need to include training data to calculate lags correctly\n",
    "    combined_data = pd.concat([df_merged_train, df_merged_unseen]).sort_values('date')\n",
    "    \n",
    "    # For 2024 data, temporarily set hip_fracture_count to 0 for lag calculation\n",
    "    if is_forecast_data:\n",
    "        combined_data.loc[combined_data['date'] >= '2024-01-01', 'hip_fracture_count'] = 0\n",
    "    \n",
    "    monthly_combined = create_monthly_data_with_features(combined_data)\n",
    "    \n",
    "    # Extract just the unseen period\n",
    "    monthly_unseen = monthly_combined[monthly_combined['date'] >= df_merged_unseen['date'].min()]\n",
    "    \n",
    "    # Prepare for Prophet\n",
    "    prophet_unseen = monthly_unseen.rename(columns={'date': 'ds', 'hip_fracture_count': 'y'})\n",
    "    \n",
    "    # Ensure all regressors exist in the unseen data\n",
    "    for regressor in best_regressors:\n",
    "        if regressor not in prophet_unseen.columns:\n",
    "            print(f\"Warning: Regressor {regressor} missing from unseen data. Setting to 0.\")\n",
    "            prophet_unseen[regressor] = 0\n",
    "    \n",
    "    # Make predictions\n",
    "    unseen_forecast = best_model.predict(prophet_unseen)\n",
    "    \n",
    "    # Add actual values for comparison\n",
    "    unseen_forecast['y'] = prophet_unseen['y'].values\n",
    "    \n",
    "    # For 2024 forecasts, we don't need to calculate error metrics\n",
    "    if is_forecast_data:\n",
    "        print(f\"\\nBest Model Forecasts for 2024 Data:\")\n",
    "        # Skip error metrics for forecast data\n",
    "    else:\n",
    "        # Calculate metrics for validation data\n",
    "        mae = mean_absolute_error(unseen_forecast['y'], unseen_forecast['yhat'])\n",
    "        rmse = np.sqrt(mean_squared_error(unseen_forecast['y'], unseen_forecast['yhat']))\n",
    "        r2 = r2_score(unseen_forecast['y'], unseen_forecast['yhat'])\n",
    "        \n",
    "        print(f\"\\nBest Model Performance on Unseen Data:\")\n",
    "        print(f\"Mean Absolute Error: {mae:.2f}\")\n",
    "        print(f\"Root Mean Squared Error: {rmse:.2f}\")\n",
    "        print(f\"R² Score: {r2:.4f}\")\n",
    "    \n",
    "    # Print monthly actual vs forecasted values\n",
    "    print(\"\\nMonthly Actual and Forecasted Hip Fracture Rates:\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    if is_forecast_data:\n",
    "        # For 2024 forecast data, don't show actual values\n",
    "        print(f\"{'Month':<12} {'Forecast':<10} {'Lower 95%':<12} {'Upper 95%':<12}\")\n",
    "        print(\"-\"*80)\n",
    "        \n",
    "        for i in range(len(unseen_forecast)):\n",
    "            # Format date as YYYY-MM\n",
    "            month = unseen_forecast['ds'].iloc[i].strftime('%Y-%m')\n",
    "            forecast = unseen_forecast['yhat'].iloc[i]\n",
    "            lower = unseen_forecast['yhat_lower'].iloc[i]\n",
    "            upper = unseen_forecast['yhat_upper'].iloc[i]\n",
    "            \n",
    "            print(f\"{month:<12} {forecast:<10.2f} {lower:<12.2f} {upper:<12.2f}\")\n",
    "    else:\n",
    "        # For validation data, show actuals and differences\n",
    "        print(f\"{'Month':<12} {'Actual':<10} {'Forecast':<10} {'Lower 95%':<12} {'Upper 95%':<12} {'Diff':<10} {'Diff %':<10}\")\n",
    "        print(\"-\"*80)\n",
    "        \n",
    "        for i in range(len(unseen_forecast)):\n",
    "            # Format date as YYYY-MM\n",
    "            month = unseen_forecast['ds'].iloc[i].strftime('%Y-%m')\n",
    "            actual = unseen_forecast['y'].iloc[i]\n",
    "            forecast = unseen_forecast['yhat'].iloc[i]\n",
    "            lower = unseen_forecast['yhat_lower'].iloc[i]\n",
    "            upper = unseen_forecast['yhat_upper'].iloc[i]\n",
    "            \n",
    "            # Calculate difference and percentage difference\n",
    "            diff = forecast - actual\n",
    "            diff_pct = (diff / actual * 100) if actual != 0 else float('inf')\n",
    "            \n",
    "            print(f\"{month:<12} {actual:<10.2f} {forecast:<10.2f} {lower:<12.2f} {upper:<12.2f} {diff:<10.2f} {diff_pct:<10.2f}%\")\n",
    "    \n",
    "    # Calculate and print summary statistics\n",
    "    forecasts = unseen_forecast['yhat']\n",
    "    \n",
    "    if is_forecast_data:\n",
    "        # For 2024 data, only show forecast statistics\n",
    "        print(\"\\nSummary Statistics:\")\n",
    "        print(f\"Forecast:  Min: {forecasts.min():.2f}, Max: {forecasts.max():.2f}, Mean: {forecasts.mean():.2f}, Median: {forecasts.median():.2f}\")\n",
    "    else:\n",
    "        # For validation data, show both actual and forecast statistics\n",
    "        actuals = unseen_forecast['y']\n",
    "        print(\"\\nSummary Statistics:\")\n",
    "        print(f\"Actual:    Min: {actuals.min():.2f}, Max: {actuals.max():.2f}, Mean: {actuals.mean():.2f}, Median: {actuals.median():.2f}\")\n",
    "        print(f\"Forecast:  Min: {forecasts.min():.2f}, Max: {forecasts.max():.2f}, Mean: {forecasts.mean():.2f}, Median: {forecasts.median():.2f}\")\n",
    "    \n",
    "    # Visualize forecast\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    if not is_forecast_data:\n",
    "        # For validation data, show actuals\n",
    "        plt.plot(unseen_forecast['ds'], unseen_forecast['y'], 'ko', label='Actual')\n",
    "    \n",
    "    plt.plot(unseen_forecast['ds'], unseen_forecast['yhat'], 'b-', label='Forecast')\n",
    "    plt.fill_between(unseen_forecast['ds'], \n",
    "                    unseen_forecast['yhat_lower'], \n",
    "                    unseen_forecast['yhat_upper'], \n",
    "                    color='blue', alpha=0.2)\n",
    "    \n",
    "    title = 'Best Validation Model: Forecast for 2024' if is_forecast_data else 'Best Validation Model: Forecast for Unseen Data'\n",
    "    plt.title(title)\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()\n",
    "    \n",
    "    # Analyze forecast components\n",
    "    fig = best_model.plot_components(unseen_forecast)\n",
    "    plt.show()\n",
    "    \n",
    "    # Calculate feature importance through perturbation analysis\n",
    "    feature_importance = calculate_perturbation_importance(\n",
    "        best_model, prophet_unseen, best_regressors\n",
    "    )\n",
    "    \n",
    "    return unseen_forecast, feature_importance\n",
    "\n",
    "\n",
    "def compare_models_on_unseen_data(df_merged_train, df_merged_unseen, n_splits=6, test_size_months=6):\n",
    "    \"\"\"\n",
    "    Compare the best validation model with a model trained on all data\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df_merged_train : DataFrame\n",
    "        The complete training dataset\n",
    "    df_merged_unseen : DataFrame\n",
    "        The unseen data to forecast\n",
    "    n_splits : int\n",
    "        Number of validation splits\n",
    "    test_size_months : int\n",
    "        Size of each test window in months\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    dict with all results\n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import matplotlib.pyplot as plt\n",
    "    from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "    \n",
    "    # Run cross-validation to find the best model\n",
    "    results = run_prophet_monthly_validation(\n",
    "        df_merged_train, \n",
    "        n_splits=n_splits,\n",
    "        test_size_months=test_size_months\n",
    "    )\n",
    "    \n",
    "    # Find the best model based on R² score\n",
    "    valid_r2_scores = [m['r2'] for m in results['metrics'] if not np.isnan(m['r2'])]\n",
    "    if not valid_r2_scores:\n",
    "        print(\"Warning: No valid R² scores found. Using the last model.\")\n",
    "        best_model_index = -1\n",
    "    else:\n",
    "        best_model_index = np.argmax(valid_r2_scores)\n",
    "    \n",
    "    best_model = results['models'][best_model_index]\n",
    "    best_regressors = results['regressors'][best_model_index]\n",
    "    best_period = results['train_periods'][best_model_index]\n",
    "    \n",
    "    print(f\"\\nBest model from validation:\")\n",
    "    print(f\"Split {best_model_index + 1}, Training period: {best_period[0].strftime('%Y-%m')} to {best_period[1].strftime('%Y-%m')}\")\n",
    "    print(f\"R² Score during validation: {results['metrics'][best_model_index]['r2']:.4f}\")\n",
    "    \n",
    "    # Use the best model to forecast unseen data\n",
    "    best_forecast, best_importance = forecast_with_best_model(\n",
    "        best_model, best_regressors, df_merged_train, df_merged_unseen\n",
    "    )\n",
    "    \n",
    "    # Also train a model on all data for comparison\n",
    "    print(\"\\nTraining model on all data for comparison...\")\n",
    "    final_model, full_forecast, full_importance = train_final_model_and_forecast(\n",
    "        df_merged_train, df_merged_unseen\n",
    "    )\n",
    "    \n",
    "    # Compare the two models\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(best_forecast['ds'], best_forecast['y'], 'ko', label='Actual')\n",
    "    plt.plot(best_forecast['ds'], best_forecast['yhat'], 'b-', label='Best Validation Model')\n",
    "    plt.plot(full_forecast['ds'], full_forecast['yhat'], 'r--', label='Full Data Model')\n",
    "    plt.title('Comparison: Best Validation Model vs Full Data Model')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()\n",
    "    \n",
    "    # Combine feature importance from both models for comparison\n",
    "    combined_importance = pd.DataFrame({\n",
    "        'Feature': best_importance['Feature'],\n",
    "        'Best Model Importance': best_importance['Importance'],\n",
    "        'Full Model Importance': full_importance['Importance']\n",
    "    })\n",
    "    \n",
    "    # Sort by the average importance\n",
    "    combined_importance['Average'] = (combined_importance['Best Model Importance'] + \n",
    "                                      combined_importance['Full Model Importance']) / 2\n",
    "    combined_importance = combined_importance.sort_values('Average', ascending=False)\n",
    "    \n",
    "    # Convert feature values to strings for plotting\n",
    "    # This is the fix for the TypeError\n",
    "    features_as_str = combined_importance['Feature'].astype(str).values\n",
    "    \n",
    "    # Plot comparative feature importance\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.barh(features_as_str, combined_importance['Best Model Importance'], \n",
    "             color='blue', alpha=0.6, label='Best Validation Model')\n",
    "    plt.barh(features_as_str, combined_importance['Full Model Importance'], \n",
    "             color='red', alpha=0.6, label='Full Data Model')\n",
    "    plt.xlabel('Importance')\n",
    "    plt.title('Feature Importance Comparison')\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return {\n",
    "        'validation_results': results,\n",
    "        'best_model': best_model,\n",
    "        'best_regressors': best_regressors,\n",
    "        'best_forecast': best_forecast,\n",
    "        'best_importance': best_importance,\n",
    "        'full_model': final_model,\n",
    "        'full_forecast': full_forecast,\n",
    "        'full_importance': full_importance\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a8d201c-1054-4137-ab7a-4578338bfe9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forecast_with_best_model(best_model, best_regressors, df_merged_train, df_merged_unseen):\n",
    "    \"\"\"\n",
    "    Use the best model from validation to forecast unseen data with weekly breakdown\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    best_model : Prophet model\n",
    "        The best performing model from cross-validation\n",
    "    best_regressors : list\n",
    "        The list of regressors used in the best model\n",
    "    df_merged_train : DataFrame\n",
    "        The complete training dataset\n",
    "    df_merged_unseen : DataFrame\n",
    "        The unseen data to forecast\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    unseen_forecast, feature_importance\n",
    "    \"\"\"\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "    from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "    \n",
    "    # Check if we're dealing with 2024 data\n",
    "    is_forecast_data = df_merged_unseen['date'].min().year >= 2024\n",
    "    \n",
    "    # Prepare unseen data with correct lag features\n",
    "    # We need to include training data to calculate lags correctly\n",
    "    combined_data = pd.concat([df_merged_train, df_merged_unseen]).sort_values('date')\n",
    "    \n",
    "    # For 2024 data, temporarily set hip_fracture_count to 0 for lag calculation\n",
    "    if is_forecast_data:\n",
    "        combined_data.loc[combined_data['date'] >= '2024-01-01', 'hip_fracture_count'] = 0\n",
    "    \n",
    "    monthly_combined = create_monthly_data_with_features(combined_data)\n",
    "    \n",
    "    # Extract just the unseen period\n",
    "    monthly_unseen = monthly_combined[monthly_combined['date'] >= df_merged_unseen['date'].min()]\n",
    "    \n",
    "    # Prepare for Prophet\n",
    "    prophet_unseen = monthly_unseen.rename(columns={'date': 'ds', 'hip_fracture_count': 'y'})\n",
    "    \n",
    "    # Ensure all regressors exist in the unseen data\n",
    "    for regressor in best_regressors:\n",
    "        if regressor not in prophet_unseen.columns:\n",
    "            print(f\"Warning: Regressor {regressor} missing from unseen data. Setting to 0.\")\n",
    "            prophet_unseen[regressor] = 0\n",
    "    \n",
    "    # Make predictions\n",
    "    unseen_forecast = best_model.predict(prophet_unseen)\n",
    "    \n",
    "    # Add actual values for comparison\n",
    "    unseen_forecast['y'] = prophet_unseen['y'].values\n",
    "    \n",
    "    # For 2024 forecasts, we don't need to calculate error metrics\n",
    "    if is_forecast_data:\n",
    "        print(f\"\\nBest Model Forecasts for 2024 Data:\")\n",
    "        # Skip error metrics for forecast data\n",
    "    else:\n",
    "        # Calculate metrics for validation data\n",
    "        mae = mean_absolute_error(unseen_forecast['y'], unseen_forecast['yhat'])\n",
    "        rmse = np.sqrt(mean_squared_error(unseen_forecast['y'], unseen_forecast['yhat']))\n",
    "        r2 = r2_score(unseen_forecast['y'], unseen_forecast['yhat'])\n",
    "        \n",
    "        print(f\"\\nBest Model Performance on Unseen Data:\")\n",
    "        print(f\"Mean Absolute Error: {mae:.2f}\")\n",
    "        print(f\"Root Mean Squared Error: {rmse:.2f}\")\n",
    "        print(f\"R² Score: {r2:.4f}\")\n",
    "    \n",
    "    # Create weekly breakdown\n",
    "    weekly_breakdown = create_weekly_breakdown(unseen_forecast, is_forecast_data)\n",
    "    \n",
    "    # Print weekly breakdown tables\n",
    "    print_weekly_tables(weekly_breakdown, is_forecast_data)\n",
    "    \n",
    "    # Print original monthly summary\n",
    "    print(\"\\nMonthly Summary:\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    if is_forecast_data:\n",
    "        # For 2024 forecast data, don't show actual values\n",
    "        print(f\"{'Month':<12} {'Forecast':<10} {'Lower 95%':<12} {'Upper 95%':<12}\")\n",
    "        print(\"-\"*80)\n",
    "        \n",
    "        for i in range(len(unseen_forecast)):\n",
    "            # Format date as YYYY-MM\n",
    "            month = unseen_forecast['ds'].iloc[i].strftime('%Y-%m')\n",
    "            forecast = unseen_forecast['yhat'].iloc[i]\n",
    "            lower = unseen_forecast['yhat_lower'].iloc[i]\n",
    "            upper = unseen_forecast['yhat_upper'].iloc[i]\n",
    "            \n",
    "            print(f\"{month:<12} {forecast:<10.2f} {lower:<12.2f} {upper:<12.2f}\")\n",
    "    else:\n",
    "        # For validation data, show actuals and differences\n",
    "        print(f\"{'Month':<12} {'Actual':<10} {'Forecast':<10} {'Lower 95%':<12} {'Upper 95%':<12} {'Diff':<10} {'Diff %':<10}\")\n",
    "        print(\"-\"*80)\n",
    "        \n",
    "        for i in range(len(unseen_forecast)):\n",
    "            # Format date as YYYY-MM\n",
    "            month = unseen_forecast['ds'].iloc[i].strftime('%Y-%m')\n",
    "            actual = unseen_forecast['y'].iloc[i]\n",
    "            forecast = unseen_forecast['yhat'].iloc[i]\n",
    "            lower = unseen_forecast['yhat_lower'].iloc[i]\n",
    "            upper = unseen_forecast['yhat_upper'].iloc[i]\n",
    "            \n",
    "            # Calculate difference and percentage difference\n",
    "            diff = forecast - actual\n",
    "            diff_pct = (diff / actual * 100) if actual != 0 else float('inf')\n",
    "            \n",
    "            print(f\"{month:<12} {actual:<10.2f} {forecast:<10.2f} {lower:<12.2f} {upper:<12.2f} {diff:<10.2f} {diff_pct:<10.2f}%\")\n",
    "    \n",
    "    # Calculate and print summary statistics\n",
    "    forecasts = unseen_forecast['yhat']\n",
    "    \n",
    "    if is_forecast_data:\n",
    "        # For 2024 data, only show forecast statistics\n",
    "        print(\"\\nSummary Statistics:\")\n",
    "        print(f\"Forecast:  Min: {forecasts.min():.2f}, Max: {forecasts.max():.2f}, Mean: {forecasts.mean():.2f}, Median: {forecasts.median():.2f}\")\n",
    "    else:\n",
    "        # For validation data, show both actual and forecast statistics\n",
    "        actuals = unseen_forecast['y']\n",
    "        print(\"\\nSummary Statistics:\")\n",
    "        print(f\"Actual:    Min: {actuals.min():.2f}, Max: {actuals.max():.2f}, Mean: {actuals.mean():.2f}, Median: {actuals.median():.2f}\")\n",
    "        print(f\"Forecast:  Min: {forecasts.min():.2f}, Max: {forecasts.max():.2f}, Mean: {forecasts.mean():.2f}, Median: {forecasts.median():.2f}\")\n",
    "    \n",
    "    # Visualize forecast\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    if not is_forecast_data:\n",
    "        # For validation data, show actuals\n",
    "        plt.plot(unseen_forecast['ds'], unseen_forecast['y'], 'ko', label='Actual')\n",
    "    \n",
    "    plt.plot(unseen_forecast['ds'], unseen_forecast['yhat'], 'b-', label='Forecast')\n",
    "    plt.fill_between(unseen_forecast['ds'], \n",
    "                    unseen_forecast['yhat_lower'], \n",
    "                    unseen_forecast['yhat_upper'], \n",
    "                    color='blue', alpha=0.2)\n",
    "    \n",
    "    title = 'Best Validation Model: Forecast for 2024' if is_forecast_data else 'Best Validation Model: Forecast for Unseen Data'\n",
    "    plt.title(title)\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()\n",
    "    \n",
    "    # Analyze forecast components\n",
    "    fig = best_model.plot_components(unseen_forecast)\n",
    "    plt.show()\n",
    "    \n",
    "    # Calculate feature importance through perturbation analysis\n",
    "    feature_importance = calculate_perturbation_importance(\n",
    "        best_model, prophet_unseen, best_regressors\n",
    "    )\n",
    "    \n",
    "    return unseen_forecast, feature_importance\n",
    "\n",
    "\n",
    "def create_weekly_breakdown(monthly_forecast, is_forecast_data):\n",
    "    \"\"\"\n",
    "    Create weekly breakdown from monthly forecast data\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    monthly_forecast : DataFrame\n",
    "        Monthly forecast data from Prophet\n",
    "    is_forecast_data : bool\n",
    "        Whether this is future forecast data (no actuals)\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    DataFrame with weekly breakdown\n",
    "    \"\"\"\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    \n",
    "    weekly_data = []\n",
    "    \n",
    "    for i in range(len(monthly_forecast)):\n",
    "        month_date = monthly_forecast['ds'].iloc[i]\n",
    "        month_str = month_date.strftime('%Y-%m')\n",
    "        \n",
    "        # Get monthly values\n",
    "        monthly_forecast_val = monthly_forecast['yhat'].iloc[i]\n",
    "        monthly_lower = monthly_forecast['yhat_lower'].iloc[i]\n",
    "        monthly_upper = monthly_forecast['yhat_upper'].iloc[i]\n",
    "        \n",
    "        if not is_forecast_data:\n",
    "            monthly_actual = monthly_forecast['y'].iloc[i]\n",
    "        \n",
    "        # Divide monthly values by 4 for weekly estimates\n",
    "        weekly_forecast = monthly_forecast_val / 4\n",
    "        weekly_lower = monthly_lower / 4\n",
    "        weekly_upper = monthly_upper / 4\n",
    "        \n",
    "        if not is_forecast_data:\n",
    "            weekly_actual = monthly_actual / 4\n",
    "        \n",
    "        # Create data for each week\n",
    "        week_data = {\n",
    "            'Month': month_str,\n",
    "            'Week 1': weekly_forecast,\n",
    "            'Week 2': weekly_forecast,\n",
    "            'Week 3': weekly_forecast,\n",
    "            'Week 4': weekly_forecast,\n",
    "            'Week 1 Lower': weekly_lower,\n",
    "            'Week 2 Lower': weekly_lower,\n",
    "            'Week 3 Lower': weekly_lower,\n",
    "            'Week 4 Lower': weekly_lower,\n",
    "            'Week 1 Upper': weekly_upper,\n",
    "            'Week 2 Upper': weekly_upper,\n",
    "            'Week 3 Upper': weekly_upper,\n",
    "            'Week 4 Upper': weekly_upper,\n",
    "            'Monthly Total': monthly_forecast_val,\n",
    "            'Monthly Lower': monthly_lower,\n",
    "            'Monthly Upper': monthly_upper\n",
    "        }\n",
    "        \n",
    "        if not is_forecast_data:\n",
    "            week_data.update({\n",
    "                'Week 1 Actual': weekly_actual,\n",
    "                'Week 2 Actual': weekly_actual,\n",
    "                'Week 3 Actual': weekly_actual,\n",
    "                'Week 4 Actual': weekly_actual,\n",
    "                'Monthly Actual': monthly_actual\n",
    "            })\n",
    "        \n",
    "        weekly_data.append(week_data)\n",
    "    \n",
    "    return pd.DataFrame(weekly_data)\n",
    "\n",
    "\n",
    "def print_weekly_tables(weekly_breakdown, is_forecast_data):\n",
    "    \"\"\"\n",
    "    Print formatted weekly breakdown tables\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    weekly_breakdown : DataFrame\n",
    "        Weekly breakdown data\n",
    "    is_forecast_data : bool\n",
    "        Whether this is future forecast data (no actuals)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Combined table with forecasts and confidence intervals\n",
    "    print(\"\\nWeekly Breakdown - Combined View:\")\n",
    "    print(\"=\"*100)\n",
    "    print(f\"{'Month':<10} {'Week 1':<20} {'Week 2':<20} {'Week 3':<20} {'Week 4':<20} {'Monthly Total':<15}\")\n",
    "    print(\"-\"*100)\n",
    "    \n",
    "    for _, row in weekly_breakdown.iterrows():\n",
    "        month = row['Month']\n",
    "        w1 = f\"{row['Week 1']:.1f} ({row['Week 1 Lower']:.1f}-{row['Week 1 Upper']:.1f})\"\n",
    "        w2 = f\"{row['Week 2']:.1f} ({row['Week 2 Lower']:.1f}-{row['Week 2 Upper']:.1f})\"\n",
    "        w3 = f\"{row['Week 3']:.1f} ({row['Week 3 Lower']:.1f}-{row['Week 3 Upper']:.1f})\"\n",
    "        w4 = f\"{row['Week 4']:.1f} ({row['Week 4 Lower']:.1f}-{row['Week 4 Upper']:.1f})\"\n",
    "        total = f\"{row['Monthly Total']:.1f}\"\n",
    "        \n",
    "        print(f\"{month:<10} {w1:<20} {w2:<20} {w3:<20} {w4:<20} {total:<15}\")\n",
    "    \n",
    "    # Separate tables for forecast values\n",
    "    print(\"\\nWeekly Breakdown - Forecast Values:\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"{'Month':<10} {'Week 1':<12} {'Week 2':<12} {'Week 3':<12} {'Week 4':<12} {'Monthly Total':<15}\")\n",
    "    print(\"-\"*80)\n",
    "    \n",
    "    for _, row in weekly_breakdown.iterrows():\n",
    "        month = row['Month']\n",
    "        print(f\"{month:<10} {row['Week 1']:<12.2f} {row['Week 2']:<12.2f} {row['Week 3']:<12.2f} {row['Week 4']:<12.2f} {row['Monthly Total']:<15.2f}\")\n",
    "    \n",
    "    # Lower bounds table\n",
    "    print(\"\\nWeekly Breakdown - Lower 95% Confidence:\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"{'Month':<10} {'Week 1':<12} {'Week 2':<12} {'Week 3':<12} {'Week 4':<12} {'Monthly Total':<15}\")\n",
    "    print(\"-\"*80)\n",
    "    \n",
    "    for _, row in weekly_breakdown.iterrows():\n",
    "        month = row['Month']\n",
    "        print(f\"{month:<10} {row['Week 1 Lower']:<12.2f} {row['Week 2 Lower']:<12.2f} {row['Week 3 Lower']:<12.2f} {row['Week 4 Lower']:<12.2f} {row['Monthly Lower']:<15.2f}\")\n",
    "    \n",
    "    # Upper bounds table\n",
    "    print(\"\\nWeekly Breakdown - Upper 95% Confidence:\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"{'Month':<10} {'Week 1':<12} {'Week 2':<12} {'Week 3':<12} {'Week 4':<12} {'Monthly Total':<15}\")\n",
    "    print(\"-\"*80)\n",
    "    \n",
    "    for _, row in weekly_breakdown.iterrows():\n",
    "        month = row['Month']\n",
    "        print(f\"{month:<10} {row['Week 1 Upper']:<12.2f} {row['Week 2 Upper']:<12.2f} {row['Week 3 Upper']:<12.2f} {row['Week 4 Upper']:<12.2f} {row['Monthly Upper']:<15.2f}\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6b49e40-c33c-432d-8f40-1f5682a672c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results = compare_models_on_unseen_data(\n",
    "    merged_reg, \n",
    "    df_merged_with_weather,\n",
    "    n_splits=12, \n",
    "    test_size_months=3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9152d9df-b357-459a-8d4c-a547560458f9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
