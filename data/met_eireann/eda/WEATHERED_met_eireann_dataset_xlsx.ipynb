{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "10c2277c-d9ab-45ac-8b9c-9a68e3d9724c",
   "metadata": {},
   "source": [
    "#### Set styling for plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "008257ad-fd71-4beb-a4c5-8589307d01de",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-12T15:26:28.307615Z",
     "iopub.status.busy": "2025-01-12T15:26:28.306967Z",
     "iopub.status.idle": "2025-01-12T15:26:29.161706Z",
     "shell.execute_reply": "2025-01-12T15:26:29.159569Z",
     "shell.execute_reply.started": "2025-01-12T15:26:28.307574Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "import matplotlib.ticker as ticker\n",
    "import seaborn as sns\n",
    "sns.set_palette('colorblind')\n",
    "from matplotlib.pyplot import tight_layout\n",
    "# ##SETTING PARAMS FOR MATPLOTLIB FIGURES\n",
    "plt.rcParams.update({\"figure.figsize\": (6, 6),\n",
    "                 \"axes.facecolor\": \"white\",\n",
    "                 \"axes.edgecolor\": \"black\"})\n",
    "plt.rcParams['axes.prop_cycle'] = plt.cycler(color=sns.color_palette('colorblind'))\n",
    "##set font size\n",
    "font = {'family': 'sans-serif',\n",
    "       'weight': 'normal',\n",
    "       'size': 14}\n",
    "plt.rc('font', **font)\n",
    "# ##PANDAS PLOTTING\n",
    "pd.plotting.register_matplotlib_converters()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b1cab69-cc13-4c24-afb1-6b7d84f165a1",
   "metadata": {},
   "source": [
    "#### Step 1: save environment file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "048045bf-c3cf-4b98-b98b-38e39e78c181",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-12T15:26:29.700058Z",
     "iopub.status.busy": "2025-01-12T15:26:29.699034Z",
     "iopub.status.idle": "2025-01-12T15:26:31.879377Z",
     "shell.execute_reply": "2025-01-12T15:26:31.874580Z",
     "shell.execute_reply.started": "2025-01-12T15:26:29.700007Z"
    }
   },
   "outputs": [],
   "source": [
    "!conda env export > combined_met_environment.yml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18a68556-70f4-43cf-a728-31459c8475b6",
   "metadata": {},
   "source": [
    "#### Step 2: import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4bb09125-64b0-44e1-8662-d89260501677",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-12T15:26:31.906518Z",
     "iopub.status.busy": "2025-01-12T15:26:31.906248Z",
     "iopub.status.idle": "2025-01-12T15:26:31.917848Z",
     "shell.execute_reply": "2025-01-12T15:26:31.916790Z",
     "shell.execute_reply.started": "2025-01-12T15:26:31.906498Z"
    }
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import xml.etree.ElementTree as ET\n",
    "from datetime import datetime, timezone\n",
    "from tqdm import tqdm\n",
    "import codecs\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37825704-269a-452d-b99c-992ac056e6e4",
   "metadata": {},
   "source": [
    "## INFORMATION\n",
    "\n",
    "## ODS - Met Eirean old system (manual) for recording Adverse weather\n",
    "Start date: 2012-04-25 12:00:00\n",
    "End date: 2021-02-17 09:00:00\n",
    "only 1654 events in totat which seems low compared to the new rss xml system even with Advisories removed\n",
    "will use data from 2013 to when the new system starts in 2018 \n",
    "\n",
    "## XLSX \n",
    "some of 2023 was missing from the xml style data so Met Eireann sent on the full 2023 in this excel data format "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8baafdf0-49ea-40b3-94a6-5c56bd4ea920",
   "metadata": {},
   "source": [
    "#### Step 3: import csv files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ce0b8c77-0d0c-4932-b046-5a577a71217a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-12T15:26:32.608530Z",
     "iopub.status.busy": "2025-01-12T15:26:32.608041Z",
     "iopub.status.idle": "2025-01-12T15:26:32.616886Z",
     "shell.execute_reply": "2025-01-12T15:26:32.613590Z",
     "shell.execute_reply.started": "2025-01-12T15:26:32.608493Z"
    }
   },
   "outputs": [],
   "source": [
    "data_directory_xml = \"/mnt/hgfs/shared/weather_warnings/archive_warnings/archive\"\n",
    "data_directory_ods = \"/mnt/hgfs/shared/project_data/met_eireann/Archived_Wx_Warnings_25April2012_17February2021.ods\"\n",
    "data_directory_xl = \"/mnt/hgfs/shared/project_data/met_eireann/National warnings from pdfs_2023.xlsx\"\n",
    "full_path_xml = os.path.abspath(data_directory_xml)\n",
    "full_path_ods = os.path.abspath(data_directory_ods)\n",
    "full_path_xl = os.path.abspath(data_directory_xl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7b75719e-a9a5-4591-b08f-c1666d9926c7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-12T15:26:33.258368Z",
     "iopub.status.busy": "2025-01-12T15:26:33.257790Z",
     "iopub.status.idle": "2025-01-12T15:26:39.090664Z",
     "shell.execute_reply": "2025-01-12T15:26:39.089840Z",
     "shell.execute_reply.started": "2025-01-12T15:26:33.258329Z"
    }
   },
   "outputs": [],
   "source": [
    "# Read the .ods file\n",
    "df_ods = pd.read_excel(full_path_ods, engine='odf', parse_dates=['Issue Time', 'Valid From', 'Valid To'])\n",
    "# Read the 2023 excel file\n",
    "df_xl = pd.read_excel(full_path_xl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "71ce8bde-ef5d-410c-8b7e-6a2c88a2ce4e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-12T15:26:39.109605Z",
     "iopub.status.busy": "2025-01-12T15:26:39.109359Z",
     "iopub.status.idle": "2025-01-12T15:26:39.141404Z",
     "shell.execute_reply": "2025-01-12T15:26:39.139135Z",
     "shell.execute_reply.started": "2025-01-12T15:26:39.109584Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           Issue Time          Valid From            Valid To Warning Colour  \\\n",
      "0 2012-04-25 12:00:00 2012-04-25 12:00:00 2012-04-26 12:00:00         Yellow   \n",
      "1 2012-06-01 21:00:00 2012-06-02 12:00:00 2012-06-03 21:00:00         Yellow   \n",
      "2 2012-06-02 14:00:00 2012-06-02 14:00:00 2012-06-03 12:00:00         Orange   \n",
      "3 2012-06-08 10:00:00 2012-06-08 10:00:00 2012-06-08 23:59:00         Yellow   \n",
      "4 2012-06-14 20:00:00 2012-06-14 20:00:00 2012-06-16 12:00:00         Yellow   \n",
      "\n",
      "  Warning Element                                        WhereToText  \\\n",
      "0            Rain                               Munster and Leinster   \n",
      "1            Rain                     Munster, Connacht and Leinster   \n",
      "2            Rain                               Munster and Leinster   \n",
      "3            Rain                              Connacht and Leinster   \n",
      "4            Rain  Munster, Leinster, Connacht, Donegal, Monaghan...   \n",
      "\n",
      "                                        Warning Text  Munster  Clare   Cork  \\\n",
      "0  Heavy rain moving into Southern coastal counti...     True   True   True   \n",
      "1  Between 25 and 65 mm of rain possible, (heavie...     True   True   True   \n",
      "2  Between 25mm & 65mm of rain expected over Lein...     True   True   True   \n",
      "3  Further persistent and sometimes heavy rain to...    False  False  False   \n",
      "4  Further spells of rain, persistant and heavy a...     True   True   True   \n",
      "\n",
      "   ...  Ulster  Cavan  Donegal  Monaghan  Connacht  Galway  Leitrim   Mayo  \\\n",
      "0  ...   False  False    False     False     False   False    False  False   \n",
      "1  ...   False  False    False     False      True    True     True   True   \n",
      "2  ...   False  False    False     False     False   False    False  False   \n",
      "3  ...   False  False    False     False      True    True     True   True   \n",
      "4  ...    True   True     True      True      True    True     True   True   \n",
      "\n",
      "   Roscommon  Sligo  \n",
      "0      False  False  \n",
      "1       True   True  \n",
      "2      False  False  \n",
      "3       True   True  \n",
      "4       True   True  \n",
      "\n",
      "[5 rows x 38 columns]\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1654 entries, 0 to 1653\n",
      "Data columns (total 38 columns):\n",
      " #   Column           Non-Null Count  Dtype         \n",
      "---  ------           --------------  -----         \n",
      " 0   Issue Time       1654 non-null   datetime64[ns]\n",
      " 1   Valid From       1654 non-null   datetime64[ns]\n",
      " 2   Valid To         1654 non-null   datetime64[ns]\n",
      " 3   Warning Colour   1654 non-null   object        \n",
      " 4   Warning Element  1654 non-null   object        \n",
      " 5   WhereToText      1651 non-null   object        \n",
      " 6   Warning Text     1651 non-null   object        \n",
      " 7   Munster          1654 non-null   bool          \n",
      " 8   Clare            1654 non-null   bool          \n",
      " 9   Cork             1654 non-null   bool          \n",
      " 10  Kerry            1654 non-null   bool          \n",
      " 11  Limerick         1654 non-null   bool          \n",
      " 12  Tipperary        1654 non-null   bool          \n",
      " 13  Tipperary SR     1654 non-null   bool          \n",
      " 14  Waterford        1654 non-null   bool          \n",
      " 15  Leinster         1654 non-null   bool          \n",
      " 16  Carlow           1654 non-null   bool          \n",
      " 17  Dublin           1654 non-null   bool          \n",
      " 18  Kildare          1654 non-null   bool          \n",
      " 19  Kilkenny         1654 non-null   bool          \n",
      " 20  Laois            1654 non-null   bool          \n",
      " 21  Longford         1654 non-null   bool          \n",
      " 22  Louth            1654 non-null   bool          \n",
      " 23  Meath            1654 non-null   bool          \n",
      " 24  Offaly           1654 non-null   bool          \n",
      " 25  Westmeath        1654 non-null   bool          \n",
      " 26  Wexford          1654 non-null   bool          \n",
      " 27  Wicklow          1654 non-null   bool          \n",
      " 28  Ulster           1654 non-null   bool          \n",
      " 29  Cavan            1654 non-null   bool          \n",
      " 30  Donegal          1654 non-null   bool          \n",
      " 31  Monaghan         1654 non-null   bool          \n",
      " 32  Connacht         1654 non-null   bool          \n",
      " 33  Galway           1654 non-null   bool          \n",
      " 34  Leitrim          1654 non-null   bool          \n",
      " 35  Mayo             1654 non-null   bool          \n",
      " 36  Roscommon        1654 non-null   bool          \n",
      " 37  Sligo            1654 non-null   bool          \n",
      "dtypes: bool(31), datetime64[ns](3), object(4)\n",
      "memory usage: 140.7+ KB\n",
      "None\n",
      "Number of rows: 1654\n",
      "Number of columns: 38\n",
      "ODS Filtered Date Range:\n",
      "Start date: 2012-04-25 12:00:00\n",
      "End date: 2021-02-17 09:00:00\n"
     ]
    }
   ],
   "source": [
    "# check the first few rows\n",
    "print(df_ods.head())\n",
    "\n",
    "# look at dataframe info\n",
    "print(df_ods.info())\n",
    "\n",
    "# check the df shape\n",
    "print(f\"Number of rows: {df_ods.shape[0]}\")\n",
    "print(f\"Number of columns: {df_ods.shape[1]}\")\n",
    "\n",
    "print(\"ODS Filtered Date Range:\")\n",
    "print(f\"Start date: {df_ods['Issue Time'].min()}\")\n",
    "print(f\"End date: {df_ods['Issue Time'].max()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "33eebba1-a299-46bf-8741-e7ebd9b95e29",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-12T15:26:39.144752Z",
     "iopub.status.busy": "2025-01-12T15:26:39.144426Z",
     "iopub.status.idle": "2025-01-12T15:26:39.171746Z",
     "shell.execute_reply": "2025-01-12T15:26:39.169291Z",
     "shell.execute_reply.started": "2025-01-12T15:26:39.144727Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            Unnamed: 0 Unnamed: 1  \\\n",
      "0                Total        NaN   \n",
      "1  2023-01-01 00:00:00    Fog/Ice   \n",
      "2                  NaN        NaN   \n",
      "3                  NaN        NaN   \n",
      "4                  NaN        NaN   \n",
      "\n",
      "                                      unprotect cafo Unnamed: 3 Unnamed: 4  \\\n",
      "0                                                NaN     Yellow     Orange   \n",
      "1                                     Level: Yellow         244         50   \n",
      "2                                   Type: Fog / Ice           0          0   \n",
      "3  Message: Icy stretches along with patches of f...          0          0   \n",
      "4                          Affected Regions: ireland          0          0   \n",
      "\n",
      "  Unnamed: 5 Unnamed: 6  Unnamed: 7 Unnamed: 8  Unnamed: 9  ... Unnamed: 33  \\\n",
      "0        Red      Named         NaN    Ireland         NaN  ...          MH   \n",
      "1        6.5          0         NaN          0         NaN  ...           0   \n",
      "2          0          0         NaN          0         NaN  ...           0   \n",
      "3          0          0         NaN          0         NaN  ...           0   \n",
      "4          0          0         NaN          1         NaN  ...           0   \n",
      "\n",
      "  Unnamed: 34 Unnamed: 35 Unnamed: 36 Unnamed: 37 Unnamed: 38 Ulster  \\\n",
      "0          OY          WH          WX          WW         NaN    All   \n",
      "1           0           0           0           0         NaN      0   \n",
      "2           0           0           0           0         NaN      0   \n",
      "3           0           0           0           0         NaN      0   \n",
      "4           0           0           0           0         NaN      0   \n",
      "\n",
      "   Unnamed: 40 Unnamed: 41 Unnamed: 42  \n",
      "0           DL          CN          MN  \n",
      "1            0           0           0  \n",
      "2            0           0           0  \n",
      "3            0           0           0  \n",
      "4            0           0           0  \n",
      "\n",
      "[5 rows x 43 columns]\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 4255 entries, 0 to 4254\n",
      "Data columns (total 43 columns):\n",
      " #   Column          Non-Null Count  Dtype  \n",
      "---  ------          --------------  -----  \n",
      " 0   Unnamed: 0      303 non-null    object \n",
      " 1   Unnamed: 1      302 non-null    object \n",
      " 2   unprotect cafo  2846 non-null   object \n",
      " 3   Unnamed: 3      4245 non-null   object \n",
      " 4   Unnamed: 4      4247 non-null   object \n",
      " 5   Unnamed: 5      4245 non-null   object \n",
      " 6   Unnamed: 6      2756 non-null   object \n",
      " 7   Unnamed: 7      0 non-null      float64\n",
      " 8   Unnamed: 8      3043 non-null   object \n",
      " 9   Unnamed: 9      0 non-null      float64\n",
      " 10  Munster         4247 non-null   object \n",
      " 11  Unnamed: 11     4247 non-null   object \n",
      " 12  Unnamed: 12     4247 non-null   object \n",
      " 13  Unnamed: 13     4246 non-null   object \n",
      " 14  Unnamed: 14     4247 non-null   object \n",
      " 15  Unnamed: 15     4247 non-null   object \n",
      " 16  Unnamed: 16     4247 non-null   object \n",
      " 17  Unnamed: 17     1 non-null      float64\n",
      " 18  Connacht        4247 non-null   object \n",
      " 19  Unnamed: 19     4247 non-null   object \n",
      " 20  Unnamed: 20     4247 non-null   object \n",
      " 21  Unnamed: 21     4247 non-null   object \n",
      " 22  Unnamed: 22     4247 non-null   object \n",
      " 23  Unnamed: 23     4247 non-null   object \n",
      " 24  Unnamed: 24     1 non-null      float64\n",
      " 25  Leinster        4247 non-null   object \n",
      " 26  Unnamed: 26     4247 non-null   object \n",
      " 27  Unnamed: 27     4247 non-null   object \n",
      " 28  Unnamed: 28     4247 non-null   object \n",
      " 29  Unnamed: 29     4247 non-null   object \n",
      " 30  Unnamed: 30     4247 non-null   object \n",
      " 31  Unnamed: 31     4247 non-null   object \n",
      " 32  Unnamed: 32     4247 non-null   object \n",
      " 33  Unnamed: 33     4247 non-null   object \n",
      " 34  Unnamed: 34     4247 non-null   object \n",
      " 35  Unnamed: 35     4247 non-null   object \n",
      " 36  Unnamed: 36     4247 non-null   object \n",
      " 37  Unnamed: 37     4247 non-null   object \n",
      " 38  Unnamed: 38     1 non-null      float64\n",
      " 39  Ulster          4247 non-null   object \n",
      " 40  Unnamed: 40     4247 non-null   object \n",
      " 41  Unnamed: 41     4247 non-null   object \n",
      " 42  Unnamed: 42     4247 non-null   object \n",
      "dtypes: float64(5), object(38)\n",
      "memory usage: 1.4+ MB\n",
      "None\n",
      "Number of rows: 4255\n",
      "Number of columns: 43\n"
     ]
    }
   ],
   "source": [
    "# check the first few rows\n",
    "print(df_xl.head())\n",
    "\n",
    "# look at dataframe info\n",
    "print(df_xl.info())\n",
    "\n",
    "# check the df shape\n",
    "print(f\"Number of rows: {df_xl.shape[0]}\")\n",
    "print(f\"Number of columns: {df_xl.shape[1]}\")\n",
    "\n",
    "#print(\"ODS Filtered Date Range:\")\n",
    "#print(f\"Start date: {df_xl['Issue Time'].min()}\")\n",
    "#print(f\"End date: {df_xl['Issue Time'].max()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4d3285b-5a1b-4ad4-8a8f-5c09edc25a12",
   "metadata": {},
   "source": [
    "#### Step 4: process XML data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a7461037-fd91-4b23-affc-f3c6e1fadb47",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-10T14:03:23.768171Z",
     "iopub.status.busy": "2025-01-10T14:03:23.767423Z",
     "iopub.status.idle": "2025-01-10T14:07:28.661145Z",
     "shell.execute_reply": "2025-01-10T14:07:28.660237Z",
     "shell.execute_reply.started": "2025-01-10T14:03:23.768114Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files: 100%|███████████████████| 10697/10697 [04:10<00:00, 42.68it/s]\n"
     ]
    }
   ],
   "source": [
    "def severity_to_color(severity):\n",
    "    \"\"\"Map severity levels to warning colors\"\"\"\n",
    "    mapping = {\n",
    "        'Extreme': 'Red',\n",
    "        'Severe': 'Orange',\n",
    "        'Moderate': 'Yellow'\n",
    "    }\n",
    "    return mapping.get(severity, 'notmapped')\n",
    "\n",
    "def get_element_text(element, path, namespace):\n",
    "    \"\"\"Safely get text from an XML element\"\"\"\n",
    "    found = element.find(path, namespace)\n",
    "    return found.text if found is not None else None\n",
    "\n",
    "def parse_xml(file_path_xml):\n",
    "    \"\"\"Parse an XML file and extract relevant information.\"\"\"\n",
    "    try:\n",
    "        # Read the file content\n",
    "        with codecs.open(file_path_xml, 'r', encoding='utf-8', errors='ignore') as file:\n",
    "            xml_content = file.read()\n",
    "        \n",
    "        # Parse the XML content\n",
    "        root = ET.fromstring(xml_content)\n",
    "        namespace = {'cap': 'urn:oasis:names:tc:emergency:cap:1.2'}\n",
    "        \n",
    "        # Extract data from the info element first to check if we should process this warning\n",
    "        info = root.find('cap:info', namespace)\n",
    "        if info is None:\n",
    "            return None\n",
    "            \n",
    "        # Check for advisory warnings (type 22) - exclude them\n",
    "        parameters = info.findall('cap:parameter', namespace)\n",
    "        for param in parameters:\n",
    "            if get_element_text(param, 'cap:valueName', namespace) == 'awareness_type':\n",
    "                awareness_type = get_element_text(param, 'cap:value', namespace)\n",
    "                if awareness_type and '22' in awareness_type:\n",
    "                    return None\n",
    "                break\n",
    "        \n",
    "        # Check for county information\n",
    "        area = info.find('cap:area', namespace)\n",
    "        if area is None:\n",
    "            return None\n",
    "            \n",
    "        geocodes = area.findall('cap:geocode', namespace)\n",
    "        if not geocodes:\n",
    "            return None\n",
    "\n",
    "        # Initialize row with the old column format\n",
    "        row = {\n",
    "            'Issue Time': get_element_text(root, 'cap:sent', namespace),\n",
    "            'Valid From': (get_element_text(info, 'cap:effective', namespace) or \n",
    "                         get_element_text(info, 'cap:onset', namespace)),\n",
    "            'Valid To': get_element_text(info, 'cap:expires', namespace),\n",
    "            'Warning Element': get_element_text(info, 'cap:event', namespace),\n",
    "            'Warning Text': get_element_text(info, 'cap:description', namespace),\n",
    "            'WhereToText': get_element_text(area, 'cap:areaDesc', namespace),\n",
    "            'Warning Colour': severity_to_color(get_element_text(info, 'cap:severity', namespace))\n",
    "        }\n",
    "        \n",
    "        # Initialize all county columns to 0\n",
    "        county_info = {\n",
    "            'EI01': 'Carlow', 'EI02': 'Cavan', 'EI03': 'Clare', 'EI04': 'Cork', 'EI32': 'Cork City',\n",
    "            'EI06': 'Donegal', 'EI33': 'Dublin City', 'EI34': 'Dún Laoghaire-Rathdown', 'EI35': 'Fingal',\n",
    "            'EI10': 'Galway', 'EI36': 'Galway City', 'EI11': 'Kerry', 'EI12': 'Kildare', 'EI13': 'Kilkenny',\n",
    "            'EI15': 'Laois', 'EI14': 'Leitrim', 'EI42': 'Limerick', 'EI37': 'Limerick City', 'EI18': 'Longford',\n",
    "            'EI19': 'Louth', 'EI20': 'Mayo', 'EI21': 'Meath', 'EI22': 'Monaghan', 'EI23': 'Offaly',\n",
    "            'EI24': 'Roscommon', 'EI25': 'Sligo', 'EI39': 'South Dublin', 'EI43': 'Tipperary',\n",
    "            'EI44': 'Waterford', 'EI29': 'Westmeath', 'EI30': 'Wexford', 'EI31': 'Wicklow'\n",
    "        }\n",
    "        \n",
    "        # Initialize all counties to 0\n",
    "        for county_name in county_info.values():\n",
    "            row[county_name] = 0\n",
    "            \n",
    "        # Set affected counties to 1\n",
    "        for gc in geocodes:\n",
    "            if get_element_text(gc, 'cap:valueName', namespace) == 'FIPS':\n",
    "                county_code = get_element_text(gc, 'cap:value', namespace)\n",
    "                if county_code in county_info:\n",
    "                    row[county_info[county_code]] = 1\n",
    "        \n",
    "        return row\n",
    "    \n",
    "    except ET.ParseError as e:\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        return None\n",
    "\n",
    "def process_files(data_directory):\n",
    "    \"\"\"Process all XML files in the given directory and extract weather warning data.\"\"\"\n",
    "    file_pattern = os.path.join(data_directory, '*.xml')\n",
    "    file_list = glob.glob(file_pattern)\n",
    "\n",
    "    data = []\n",
    "    error_files = []\n",
    "    excluded_files = []\n",
    "\n",
    "    for file in tqdm(file_list, desc=\"Processing files\"):\n",
    "        row = parse_xml(file)\n",
    "        if row is not None:\n",
    "            data.append(row)\n",
    "        else:\n",
    "            try:\n",
    "                with codecs.open(file, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "                    ET.parse(f)\n",
    "                excluded_files.append(file)\n",
    "            except:\n",
    "                error_files.append(file)\n",
    "\n",
    "    # Create DataFrame\n",
    "    df_xml = pd.DataFrame(data)\n",
    "\n",
    "    # Convert date fields to datetime\n",
    "    date_columns = ['Issue Time', 'Valid From', 'Valid To']\n",
    "    for col in date_columns:\n",
    "        if col in df_xml.columns:\n",
    "            df_xml[col] = pd.to_datetime(df_xml[col], utc=True, errors='coerce')\n",
    "\n",
    "    return df_xml, error_files, excluded_files\n",
    "\n",
    "def save_error_files(file_list, filename):\n",
    "    \"\"\"Save list of error files to CSV\"\"\"\n",
    "    pd.DataFrame({'file': file_list}).to_csv(filename, index=False)\n",
    "\n",
    "\n",
    "df_xml, error_files, excluded_files = process_files(data_directory_xml)\n",
    "\n",
    "# Save results\n",
    "# df_xml.to_csv('weather_warnings.csv', index=False)\n",
    "save_error_files(error_files, 'error_files.csv')\n",
    "save_error_files(excluded_files, 'excluded_files.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4f3bc96f-13c8-43b0-9204-f1b0fbe292b9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-10T14:10:25.371611Z",
     "iopub.status.busy": "2025-01-10T14:10:25.371344Z",
     "iopub.status.idle": "2025-01-10T14:10:25.947067Z",
     "shell.execute_reply": "2025-01-10T14:10:25.944589Z",
     "shell.execute_reply.started": "2025-01-10T14:10:25.371592Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Issue Time                Valid From  \\\n",
      "0 2018-03-29 19:46:16+00:00 2018-03-22 23:00:01+00:00   \n",
      "1 2018-03-29 19:50:05+00:00 2018-03-29 19:00:01+00:00   \n",
      "2 2018-03-29 21:56:16+00:00 2018-03-29 19:00:01+00:00   \n",
      "3 2018-03-29 23:10:21+00:00 2018-03-29 19:00:01+00:00   \n",
      "4 2018-03-30 04:15:33+00:00 2018-03-29 19:00:01+00:00   \n",
      "\n",
      "                   Valid To            Warning Element  \\\n",
      "0 2018-03-23 09:00:01+00:00      Moderate Hail warning   \n",
      "1 2018-03-30 09:00:01+00:00      Moderate Hail warning   \n",
      "2 2018-03-30 09:00:01+00:00      Moderate Hail warning   \n",
      "3 2018-03-30 09:00:01+00:00  Moderate Snow-ice warning   \n",
      "4 2018-03-30 09:00:01+00:00      Moderate Hail warning   \n",
      "\n",
      "                                        Warning Text WhereToText  \\\n",
      "0  Heavy showery rain continuing overnight with s...     Ireland   \n",
      "1  Heavy showery rain overnight with some wintry ...     Ireland   \n",
      "2  Heavy showery rain overnight with some wintry ...     Ireland   \n",
      "3  Heavy showery rain overnight with some wintry ...     Ireland   \n",
      "4  Heavy showery rain overnight with some wintry ...     Ireland   \n",
      "\n",
      "  Warning Colour  Carlow  Cavan  Clare  ...  Monaghan  Offaly  Roscommon  \\\n",
      "0         Yellow       1      1      1  ...         1       1          1   \n",
      "1         Yellow       1      1      1  ...         1       1          1   \n",
      "2         Yellow       1      1      1  ...         1       1          1   \n",
      "3         Yellow       1      1      1  ...         1       1          1   \n",
      "4         Yellow       1      1      1  ...         1       1          1   \n",
      "\n",
      "   Sligo  South Dublin  Tipperary  Waterford  Westmeath  Wexford  Wicklow  \n",
      "0      1             0          0          0          1        1        1  \n",
      "1      1             0          0          0          1        1        1  \n",
      "2      1             0          0          0          1        1        1  \n",
      "3      1             0          0          0          1        1        1  \n",
      "4      1             0          0          0          1        1        1  \n",
      "\n",
      "[5 rows x 39 columns]\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 8040 entries, 0 to 8039\n",
      "Data columns (total 39 columns):\n",
      " #   Column                  Non-Null Count  Dtype              \n",
      "---  ------                  --------------  -----              \n",
      " 0   Issue Time              8040 non-null   datetime64[ns, UTC]\n",
      " 1   Valid From              8040 non-null   datetime64[ns, UTC]\n",
      " 2   Valid To                8039 non-null   datetime64[ns, UTC]\n",
      " 3   Warning Element         8040 non-null   object             \n",
      " 4   Warning Text            8040 non-null   object             \n",
      " 5   WhereToText             8040 non-null   object             \n",
      " 6   Warning Colour          8040 non-null   object             \n",
      " 7   Carlow                  8040 non-null   int64              \n",
      " 8   Cavan                   8040 non-null   int64              \n",
      " 9   Clare                   8040 non-null   int64              \n",
      " 10  Cork                    8040 non-null   int64              \n",
      " 11  Cork City               8040 non-null   int64              \n",
      " 12  Donegal                 8040 non-null   int64              \n",
      " 13  Dublin City             8040 non-null   int64              \n",
      " 14  Dún Laoghaire-Rathdown  8040 non-null   int64              \n",
      " 15  Fingal                  8040 non-null   int64              \n",
      " 16  Galway                  8040 non-null   int64              \n",
      " 17  Galway City             8040 non-null   int64              \n",
      " 18  Kerry                   8040 non-null   int64              \n",
      " 19  Kildare                 8040 non-null   int64              \n",
      " 20  Kilkenny                8040 non-null   int64              \n",
      " 21  Laois                   8040 non-null   int64              \n",
      " 22  Leitrim                 8040 non-null   int64              \n",
      " 23  Limerick                8040 non-null   int64              \n",
      " 24  Limerick City           8040 non-null   int64              \n",
      " 25  Longford                8040 non-null   int64              \n",
      " 26  Louth                   8040 non-null   int64              \n",
      " 27  Mayo                    8040 non-null   int64              \n",
      " 28  Meath                   8040 non-null   int64              \n",
      " 29  Monaghan                8040 non-null   int64              \n",
      " 30  Offaly                  8040 non-null   int64              \n",
      " 31  Roscommon               8040 non-null   int64              \n",
      " 32  Sligo                   8040 non-null   int64              \n",
      " 33  South Dublin            8040 non-null   int64              \n",
      " 34  Tipperary               8040 non-null   int64              \n",
      " 35  Waterford               8040 non-null   int64              \n",
      " 36  Westmeath               8040 non-null   int64              \n",
      " 37  Wexford                 8040 non-null   int64              \n",
      " 38  Wicklow                 8040 non-null   int64              \n",
      "dtypes: datetime64[ns, UTC](3), int64(32), object(4)\n",
      "memory usage: 2.4+ MB\n",
      "None\n",
      "Number of rows: 8040\n",
      "Number of columns: 39\n",
      "XML Filtered Date Range:\n",
      "Start date: 2017-09-25 04:34:48+00:00\n",
      "End date: 2023-08-05 11:33:52+00:00\n"
     ]
    }
   ],
   "source": [
    "# check the first few rows\n",
    "print(df_xml.head())\n",
    "\n",
    "# look at dataframe info\n",
    "print(df_xml.info())\n",
    "\n",
    "# check the df shape\n",
    "print(f\"Number of rows: {df_xml.shape[0]}\")\n",
    "print(f\"Number of columns: {df_xml.shape[1]}\")\n",
    "\n",
    "print(\"XML Filtered Date Range:\")\n",
    "print(f\"Start date: {df_xml['Issue Time'].min()}\")\n",
    "print(f\"End date: {df_xml['Issue Time'].max()}\")\n",
    "df_xml.to_csv(\"output.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f166e127-012c-4804-9b3c-0d4ba0751d77",
   "metadata": {},
   "source": [
    "#### Step 5: removing unneeded data and missing data from ODS "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7ebdc561-5a37-4392-916e-d3a0a3c9118c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-08T16:03:27.243294Z",
     "iopub.status.busy": "2025-01-08T16:03:27.242900Z",
     "iopub.status.idle": "2025-01-08T16:03:27.272698Z",
     "shell.execute_reply": "2025-01-08T16:03:27.271870Z",
     "shell.execute_reply.started": "2025-01-08T16:03:27.243274Z"
    }
   },
   "outputs": [],
   "source": [
    "##we dont need provinces will add hse regions after combined\n",
    "df_ods= df_ods.drop(['Connacht', 'Leinster', 'Munster', 'Ulster'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fee7ed5f-126c-4f42-b60f-36bb2f1b03cd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-08T16:07:00.501400Z",
     "iopub.status.busy": "2025-01-08T16:07:00.500017Z",
     "iopub.status.idle": "2025-01-08T16:07:00.523248Z",
     "shell.execute_reply": "2025-01-08T16:07:00.520730Z",
     "shell.execute_reply.started": "2025-01-08T16:07:00.501363Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create a copy of the filtered data\n",
    "df_ods = df_ods.copy()\n",
    "\n",
    "## Its not clear what Tipperary SR actually is and is not the same in the XML data so i'm going to merge tipperary and tipperary SR \n",
    "df_ods['Tipperary'] = df_ods[['Tipperary', 'Tipperary SR']].max(axis=1)\n",
    "df_ods = df_ods.drop('Tipperary SR', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "18af59cc-eb38-4f0d-9272-c228406e05d0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-10T14:30:04.073166Z",
     "iopub.status.busy": "2025-01-10T14:30:04.072730Z",
     "iopub.status.idle": "2025-01-10T14:30:04.548944Z",
     "shell.execute_reply": "2025-01-10T14:30:04.545736Z",
     "shell.execute_reply.started": "2025-01-10T14:30:04.073139Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from /mnt/hgfs/shared/ul_project_Msc_AI/data/met_eireann/eda/output.csv...\n",
      "Consolidating warnings...\n",
      "Analyzing results...\n",
      "\n",
      "Weather Warnings Analysis Summary:\n",
      "Total warnings issued: 1353\n",
      "Number of unique events: 1353\n",
      "\n",
      "Warning types frequency:\n",
      "- Moderate Wind warning: 239\n",
      "- Moderate Rainfall warning: 225\n",
      "- Yellow Wind: 129\n",
      "- Yellow Rain: 123\n",
      "- Moderate Snow-ice warning: 122\n",
      "- Yellow Thunderstorm: 103\n",
      "- Severe Wind warning: 84\n",
      "- Moderate Thunder warning: 32\n",
      "- Moderate Low-Temperature warning: 29\n",
      "- Yellow Snow/Ice: 27\n",
      "- Severe Snow-ice warning: 26\n",
      "- Orange Wind: 21\n",
      "- Yellow fog: 20\n",
      "- Yellow Low Temperature/Ice: 20\n",
      "- Extreme Wind warning: 16\n",
      "- Orange Rain: 15\n",
      "- Orange Thunderstorm: 13\n",
      "- Extreme Snow-ice warning: 12\n",
      "- Moderate High-Temperature warning: 10\n",
      "- Moderate Fog warning: 10\n",
      "- Yellow High Temperature: 10\n",
      "- Severe Thunder warning: 8\n",
      "- Severe Rainfall warning: 8\n",
      "- Orange Snow/Ice: 6\n",
      "- Red Wind: 5\n",
      "- Orange Low Temperature/Ice: 5\n",
      "- Severe Fog warning: 5\n",
      "- Moderate Advisory warning: 3\n",
      "- Orange High Temperature: 2\n",
      "- Minor Rain Warning: 2\n",
      "- Moderate snow-ice warning: 2\n",
      "- Severe Low-Temperature warning: 2\n",
      "- Moderate Hail warning: 2\n",
      "- Minor Wind Warning: 2\n",
      "- Minor Thunderstorm Warning: 2\n",
      "- Minor Low Temperature/Ice Warning: 2\n",
      "- Minor High Temperature Warning: 2\n",
      "- Minor Hail Warning: 2\n",
      "- Minor Fog Warning: 2\n",
      "- Moderate Small-Craft warning: 1\n",
      "- Minor Snow/Ice Warning: 1\n",
      "- Severe snow-ice warning: 1\n",
      "- Rain: 1\n",
      "- Orange Fog: 1\n",
      "\n",
      "Most reissued warning:\n",
      "- Type: Moderate Wind warning\n",
      "- Location: Ireland\n",
      "- Times issued: 49\n",
      "\n",
      "Average issues per event: 5.94\n",
      "\n",
      "Exporting consolidated data to /mnt/hgfs/shared/ul_project_Msc_AI/data/met_eireann/eda/consolidated_weather_warnings.csv...\n",
      "Export complete!\n"
     ]
    }
   ],
   "source": [
    "###XML data has a lot of duplicates for the same event just issued multiple times, so this will consolidate into single events \n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "def load_weather_warnings(file_path):\n",
    "    \"\"\"\n",
    "    Load weather warnings from a CSV file and perform initial data cleaning.\n",
    "    This function preserves all county columns and ensures proper datetime formatting.\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(file_path)\n",
    "    \n",
    "    # Convert datetime columns to proper datetime objects\n",
    "    datetime_columns = ['Issue Time', 'Valid From', 'Valid To']\n",
    "    for col in datetime_columns:\n",
    "        df[col] = pd.to_datetime(df[col])\n",
    "    \n",
    "    return df\n",
    "\n",
    "def consolidate_warnings(df):\n",
    "    \"\"\"\n",
    "    Consolidate weather warnings by grouping similar events.\n",
    "    Uses a flat aggregation structure to avoid nested dictionary errors.\n",
    "    \"\"\"\n",
    "    # First, let's identify our county columns\n",
    "    base_columns = ['Issue Time', 'Valid From', 'Valid To', 'Warning Element',\n",
    "                   'Warning Text', 'WhereToText', 'Warning Colour']\n",
    "    county_columns = [col for col in df.columns if col not in base_columns]\n",
    "    \n",
    "    # Create the event key for grouping\n",
    "    df['event_key'] = df.apply(\n",
    "        lambda x: f\"{x['Valid To']}_{x['Warning Element']}_{x['Warning Colour']}_{x['Warning Text']}\", \n",
    "        axis=1\n",
    "    )\n",
    "    \n",
    "    # Create a flat aggregation dictionary\n",
    "    agg_dict = {\n",
    "        'Issue Time': 'first',                    # Keep the first issue time\n",
    "        'Valid From': 'first',\n",
    "        'Valid To': 'first',\n",
    "        'Warning Element': 'first',\n",
    "        'Warning Text': 'first',\n",
    "        'WhereToText': 'first',\n",
    "        'Warning Colour': 'first'\n",
    "    }\n",
    "    \n",
    "    # Add county columns to aggregation\n",
    "    for col in county_columns:\n",
    "        agg_dict[col] = 'first'\n",
    "    \n",
    "    # First grouping to get the basic consolidated data\n",
    "    df_consolidated = df.groupby('event_key').agg(agg_dict).reset_index()\n",
    "    \n",
    "    # Now calculate the additional metrics separately\n",
    "    issue_counts = df.groupby('event_key').size().reset_index(name='issue_count')\n",
    "    first_issues = df.groupby('event_key')['Issue Time'].min().reset_index(name='first_issue')\n",
    "    last_issues = df.groupby('event_key')['Issue Time'].max().reset_index(name='last_issue')\n",
    "    \n",
    "    # Merge all the metrics back together\n",
    "    df_consolidated = (df_consolidated\n",
    "                      .merge(issue_counts, on='event_key')\n",
    "                      .merge(first_issues, on='event_key')\n",
    "                      .merge(last_issues, on='event_key'))\n",
    "    \n",
    "    # Rename columns to match desired output\n",
    "    rename_dict = {\n",
    "        'Warning Element': 'warning_type',\n",
    "        'Warning Text': 'warning_text',\n",
    "        'WhereToText': 'location',\n",
    "        'Warning Colour': 'warning_colour'\n",
    "    }\n",
    "    df_consolidated = df_consolidated.rename(columns=rename_dict)\n",
    "    \n",
    "    # Arrange columns in the desired order\n",
    "    column_order = [\n",
    "        'event_key', \n",
    "        'Issue Time', \n",
    "        'issue_count',\n",
    "        'first_issue',\n",
    "        'last_issue',\n",
    "        'Valid From',\n",
    "        'Valid To',\n",
    "        'warning_type',\n",
    "        'warning_text',\n",
    "        'location',\n",
    "        'warning_colour'\n",
    "    ] + county_columns\n",
    "    \n",
    "    # Only select columns that exist\n",
    "    final_columns = [col for col in column_order if col in df_consolidated.columns]\n",
    "    df_consolidated = df_consolidated[final_columns]\n",
    "    \n",
    "    return df_consolidated\n",
    "\n",
    "def analyze_warnings(df):\n",
    "    \"\"\"\n",
    "    Generate summary statistics about the weather warnings.\n",
    "    \"\"\"\n",
    "    analysis = {\n",
    "        'total_warnings': len(df),\n",
    "        'unique_events': df['event_key'].nunique(),\n",
    "        'warning_types': df['warning_type'].value_counts().to_dict(),\n",
    "        'most_reissued': df.nlargest(1, 'issue_count')[['warning_type', 'location', 'issue_count']].to_dict('records')[0],\n",
    "        'avg_issues_per_event': df['issue_count'].mean()\n",
    "    }\n",
    "    return analysis\n",
    "\n",
    "def process_weather_warnings(input_file, output_file):\n",
    "    \"\"\"\n",
    "    Main function to process weather warnings data.\n",
    "    \"\"\"\n",
    "    print(f\"Loading data from {input_file}...\")\n",
    "    df = load_weather_warnings(input_file)\n",
    "    \n",
    "    print(\"Consolidating warnings...\")\n",
    "    df_xml_consolidated = consolidate_warnings(df)\n",
    "    \n",
    "    print(\"Analyzing results...\")\n",
    "    analysis_results = analyze_warnings(df_xml_consolidated)\n",
    "    \n",
    "    print(\"\\nWeather Warnings Analysis Summary:\")\n",
    "    print(f\"Total warnings issued: {analysis_results['total_warnings']}\")\n",
    "    print(f\"Number of unique events: {analysis_results['unique_events']}\")\n",
    "    print(f\"\\nWarning types frequency:\")\n",
    "    for warning_type, count in analysis_results['warning_types'].items():\n",
    "        print(f\"- {warning_type}: {count}\")\n",
    "    print(f\"\\nMost reissued warning:\")\n",
    "    print(f\"- Type: {analysis_results['most_reissued']['warning_type']}\")\n",
    "    print(f\"- Location: {analysis_results['most_reissued']['location']}\")\n",
    "    print(f\"- Times issued: {analysis_results['most_reissued']['issue_count']}\")\n",
    "    print(f\"\\nAverage issues per event: {analysis_results['avg_issues_per_event']:.2f}\")\n",
    "    \n",
    "    print(f\"\\nExporting consolidated data to {output_file}...\")\n",
    "    df_xml_consolidated.to_csv(output_file, index=False)\n",
    "    print(\"Export complete!\")\n",
    "    \n",
    "    return df_xml_consolidated\n",
    "\n",
    "from pathlib import Path\n",
    "input_file = \"/mnt/hgfs/shared/ul_project_Msc_AI/data/met_eireann/eda/output.csv\"\n",
    "output_file = \"/mnt/hgfs/shared/ul_project_Msc_AI/data/met_eireann/eda/consolidated_weather_warnings.csv\"\n",
    "df_xml_consolidated = process_weather_warnings(input_file, output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "87585756-6afd-43c7-b1d6-c9dd2631f246",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of events in 2017: 91\n",
      "\n",
      "Events per year:\n",
      "Issue Time\n",
      "2017     91\n",
      "2018    258\n",
      "2019    176\n",
      "2020    270\n",
      "2021    206\n",
      "2022    212\n",
      "2023    140\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "year_to_count = 2017\n",
    "events_in_year = len(df_xml_consolidated[df_xml_consolidated['Issue Time'].dt.year == year_to_count])\n",
    "\n",
    "print(f\"Number of events in {year_to_count}: {events_in_year}\")\n",
    "\n",
    "# If you want to see the distribution across all years\n",
    "year_counts = df_xml_consolidated['Issue Time'].dt.year.value_counts().sort_index()\n",
    "print(\"\\nEvents per year:\")\n",
    "print(year_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "0a8c8e9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of events in 2017: 193\n",
      "\n",
      "Events per year:\n",
      "Issue Time\n",
      "2012     14\n",
      "2013    135\n",
      "2014    180\n",
      "2015    255\n",
      "2016    122\n",
      "2017    193\n",
      "2018    249\n",
      "2019    187\n",
      "2020    277\n",
      "2021     42\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "year_to_count = 2017\n",
    "events_in_year = len(df_ods[df_ods['Issue Time'].dt.year == year_to_count])\n",
    "\n",
    "print(f\"Number of events in {year_to_count}: {events_in_year}\")\n",
    "\n",
    "# If you want to see the distribution across all years\n",
    "year_counts = df_ods['Issue Time'].dt.year.value_counts().sort_index()\n",
    "print(\"\\nEvents per year:\")\n",
    "print(year_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7e8c05c-43ca-4976-aa94-023256aed92f",
   "metadata": {},
   "source": [
    "#### Step:6 confirm date/time settings, check date ranges and select appropriate rnages to combine "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "aa544bf2-bbbd-427f-8f6f-7c10ebfba050",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-10T14:30:39.089420Z",
     "iopub.status.busy": "2025-01-10T14:30:39.088054Z",
     "iopub.status.idle": "2025-01-10T14:30:39.108657Z",
     "shell.execute_reply": "2025-01-10T14:30:39.107357Z",
     "shell.execute_reply.started": "2025-01-10T14:30:39.089383Z"
    }
   },
   "outputs": [],
   "source": [
    "##make sure datetime is the same in both dataframes\n",
    "# Ensure datetime columns are consistently UTC\n",
    "datetime_cols = ['Issue Time', 'Valid From', 'Valid To']\n",
    "df_ods[datetime_cols] = df_ods[datetime_cols].apply(pd.to_datetime, utc=True)\n",
    "df_xml_consolidated[datetime_cols] = df_xml_consolidated[datetime_cols].apply(pd.to_datetime, utc=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2313229a-f83a-468d-8ef8-a21902e560c5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-10T14:31:16.884540Z",
     "iopub.status.busy": "2025-01-10T14:31:16.884288Z",
     "iopub.status.idle": "2025-01-10T14:31:16.893227Z",
     "shell.execute_reply": "2025-01-10T14:31:16.891925Z",
     "shell.execute_reply.started": "2025-01-10T14:31:16.884521Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ODS Filtered Date Range:\n",
      "Start date: 2012-04-25 12:00:00+00:00\n",
      "End date: 2021-02-17 09:00:00+00:00\n",
      "XML Filtered Date Range:\n",
      "Start date: 2017-09-25 04:34:48+00:00\n",
      "End date: 2023-08-05 11:33:52+00:00\n"
     ]
    }
   ],
   "source": [
    "print(\"ODS Filtered Date Range:\")\n",
    "print(f\"Start date: {df_ods['Issue Time'].min()}\")\n",
    "print(f\"End date: {df_ods['Issue Time'].max()}\")\n",
    "\n",
    "print(\"XML Filtered Date Range:\")\n",
    "print(f\"Start date: {df_xml_consolidated['Issue Time'].min()}\")\n",
    "print(f\"End date: {df_xml_consolidated['Issue Time'].max()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7d04d084-c939-401f-a25a-b47d1df0b921",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-12T14:33:10.993891Z",
     "iopub.status.busy": "2025-01-12T14:33:10.985513Z",
     "iopub.status.idle": "2025-01-12T14:33:11.075554Z",
     "shell.execute_reply": "2025-01-12T14:33:11.074141Z",
     "shell.execute_reply.started": "2025-01-12T14:33:10.993763Z"
    }
   },
   "outputs": [],
   "source": [
    "####on examination the XML data has dupocate entires for the same event e'g one event the warning may be issues mutliple times during the event. \n",
    "####the ods data has just individual events so i will use as much data from ODS and remove duplicates from the xml data  \n",
    "# Filter ODS data from 2013 to end of 2020\n",
    "### check plots on date change check for duplicates \n",
    "df_ods_filtered = df_ods[\n",
    "    (df_ods['Issue Time'] >= '2013-01-01') & \n",
    "    (df_ods['Issue Time'] <= '2020-12-31 23:59:59')\n",
    "]\n",
    "\n",
    "# Filter XML data from start of 2018 to 2023\n",
    "df_xml_filtered = df_xml_consolidated[\n",
    "    (df_xml_consolidated['Issue Time'] >= '2021-01-01') & \n",
    "    (df_xml_consolidated['Issue Time'] <= '2023-08-05 23:59:59')\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "32d0cd2c-a0b7-4d7f-9ce7-02985a63185f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-12T14:37:12.090203Z",
     "iopub.status.busy": "2025-01-12T14:37:12.089899Z",
     "iopub.status.idle": "2025-01-12T14:37:12.255100Z",
     "shell.execute_reply": "2025-01-12T14:37:12.253143Z",
     "shell.execute_reply.started": "2025-01-12T14:37:12.090180Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_xml_filtered' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mdf_xml_filtered\u001b[49m\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mxml_warnings_2020_2023_08.csv\u001b[39m\u001b[38;5;124m'\u001b[39m, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df_xml_filtered' is not defined"
     ]
    }
   ],
   "source": [
    "df_xml_filtered.to_csv('xml_warnings_2020_2023_08.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47c69100-4bd6-48d3-ae00-ca7512c68602",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
