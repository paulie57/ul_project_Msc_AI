{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "10c2277c-d9ab-45ac-8b9c-9a68e3d9724c",
   "metadata": {},
   "source": [
    "#### Set styling for plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "008257ad-fd71-4beb-a4c5-8589307d01de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "import matplotlib.ticker as ticker\n",
    "import seaborn as sns\n",
    "sns.set_palette('colorblind')\n",
    "from matplotlib.pyplot import tight_layout\n",
    "# ##SETTING PARAMS FOR MATPLOTLIB FIGURES\n",
    "plt.rcParams.update({\"figure.figsize\": (6, 6),\n",
    "                 \"axes.facecolor\": \"white\",\n",
    "                 \"axes.edgecolor\": \"black\"})\n",
    "plt.rcParams['axes.prop_cycle'] = plt.cycler(color=sns.color_palette('colorblind'))\n",
    "##set font size\n",
    "font = {'family': 'sans-serif',\n",
    "       'weight': 'normal',\n",
    "       'size': 14}\n",
    "plt.rc('font', **font)\n",
    "# ##PANDAS PLOTTING\n",
    "pd.plotting.register_matplotlib_converters()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b1cab69-cc13-4c24-afb1-6b7d84f165a1",
   "metadata": {},
   "source": [
    "#### Step 1: save environment file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "048045bf-c3cf-4b98-b98b-38e39e78c181",
   "metadata": {},
   "outputs": [],
   "source": [
    "!conda env export > xml_met_environment.yml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18a68556-70f4-43cf-a728-31459c8475b6",
   "metadata": {},
   "source": [
    "#### Step 2: import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bb09125-64b0-44e1-8662-d89260501677",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import shutil\n",
    "import codecs\n",
    "import logging\n",
    "from datetime import datetime\n",
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37825704-269a-452d-b99c-992ac056e6e4",
   "metadata": {},
   "source": [
    "## INFORMATION\n",
    "\n",
    "### XML - Met Eirean new system for collecting/ notifying of adverse weather events \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4d3285b-5a1b-4ad4-8a8f-5c09edc25a12",
   "metadata": {},
   "source": [
    "#### Step 3: process XML data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ec5a942",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WeatherDataProcessor:\n",
    "    \"\"\"Process weather warning XML files, handling advisories, errors, and valid weather events.\"\"\"\n",
    "    \n",
    "    def __init__(self, data_directory):\n",
    "        \"\"\"Initialize the processor with directory setup and configurations.\"\"\"\n",
    "        self.data_directory = data_directory\n",
    "        \n",
    "        # Set up directory paths first\n",
    "        self.advisories_dir = os.path.join(self.data_directory, 'advisories')\n",
    "        self.error_dir = os.path.join(self.data_directory, 'error')\n",
    "        self.log_dir = os.path.join(self.data_directory, 'logs')\n",
    "        \n",
    "        # Create directories\n",
    "        for directory in [self.advisories_dir, self.error_dir, self.log_dir]:\n",
    "            os.makedirs(directory, exist_ok=True)\n",
    "        \n",
    "        # Set up logger\n",
    "        self.setup_logger()\n",
    "        \n",
    "        # Log initialization\n",
    "        self.logger.info(f\"WeatherDataProcessor initialized for directory: {data_directory}\")\n",
    "        self.logger.info(\"Directory structure created successfully\")\n",
    "        \n",
    "        # Initialize processing statistics\n",
    "        self.stats = {\n",
    "            'total_files': 0,\n",
    "            'advisory_files': 0,\n",
    "            'error_files': 0,\n",
    "            'area_missing_files': 0,\n",
    "            'valid_files': 0\n",
    "        }\n",
    "        \n",
    "        # Define county mapping\n",
    "        self.county_info = {\n",
    "            'EI01': 'Carlow', 'EI02': 'Cavan', 'EI03': 'Clare', 'EI04': 'Cork',\n",
    "            'EI32': 'Cork City', 'EI06': 'Donegal', 'EI33': 'Dublin City',\n",
    "            'EI34': 'Dún Laoghaire-Rathdown', 'EI35': 'Fingal', 'EI10': 'Galway',\n",
    "            'EI36': 'Galway City', 'EI11': 'Kerry', 'EI12': 'Kildare',\n",
    "            'EI13': 'Kilkenny', 'EI15': 'Laois', 'EI14': 'Leitrim',\n",
    "            'EI42': 'Limerick', 'EI37': 'Limerick City', 'EI18': 'Longford',\n",
    "            'EI19': 'Louth', 'EI20': 'Mayo', 'EI21': 'Meath', 'EI22': 'Monaghan',\n",
    "            'EI23': 'Offaly', 'EI24': 'Roscommon', 'EI25': 'Sligo',\n",
    "            'EI39': 'South Dublin', 'EI43': 'Tipperary', 'EI44': 'Waterford',\n",
    "            'EI29': 'Westmeath', 'EI30': 'Wexford', 'EI31': 'Wicklow'\n",
    "        }\n",
    "\n",
    "    def setup_logger(self):\n",
    "        \"\"\"Configure logging to both file and console with Jupyter-friendly formatting.\"\"\"\n",
    "        log_file = os.path.join(self.log_dir, f'weather_processing_{datetime.now().strftime(\"%Y%m%d_%H%M%S\")}.log')\n",
    "        \n",
    "        # Create a logger\n",
    "        self.logger = logging.getLogger('WeatherProcessor')\n",
    "        self.logger.setLevel(logging.INFO)\n",
    "        \n",
    "        # Remove any existing handlers\n",
    "        if self.logger.handlers:\n",
    "            self.logger.handlers.clear()\n",
    "        \n",
    "        # Create handlers\n",
    "        file_handler = logging.FileHandler(log_file)\n",
    "        console_handler = logging.StreamHandler()\n",
    "        \n",
    "        # Create a formatting style that works well in Jupyter\n",
    "        formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n",
    "        file_handler.setFormatter(formatter)\n",
    "        console_handler.setFormatter(formatter)\n",
    "        \n",
    "        # Add handlers to logger\n",
    "        self.logger.addHandler(file_handler)\n",
    "        self.logger.addHandler(console_handler)\n",
    "\n",
    "    def get_element_text(self, element, path, namespace):\n",
    "        \"\"\"Safely extract text from an XML element.\"\"\"\n",
    "        found = element.find(path, namespace)\n",
    "        return found.text if found is not None else None\n",
    "\n",
    "    def check_advisory(self, info, namespace):\n",
    "        \"\"\"Check if the weather event is an advisory (type 22).\"\"\"\n",
    "        parameters = info.findall('cap:parameter', namespace)\n",
    "        for param in parameters:\n",
    "            if self.get_element_text(param, 'cap:valueName', namespace) == 'awareness_type':\n",
    "                awareness_type = self.get_element_text(param, 'cap:value', namespace)\n",
    "                if awareness_type and '22' in awareness_type:\n",
    "                    return True, awareness_type\n",
    "        return False, None\n",
    "\n",
    "    def move_file_to_directory(self, file_path, target_dir):\n",
    "        \"\"\"Move a file to the specified directory and return the new path.\"\"\"\n",
    "        filename = os.path.basename(file_path)\n",
    "        target_path = os.path.join(target_dir, filename)\n",
    "        shutil.move(file_path, target_path)\n",
    "        return target_path\n",
    "\n",
    "    def process_single_file(self, file_path):\n",
    "        \"\"\"Process a single XML file and categorize it appropriately.\"\"\"\n",
    "        try:\n",
    "            with codecs.open(file_path, 'r', encoding='utf-8', errors='ignore') as file:\n",
    "                xml_content = file.read()\n",
    "            \n",
    "            root = ET.fromstring(xml_content)\n",
    "            namespace = {'cap': 'urn:oasis:names:tc:emergency:cap:1.2'}\n",
    "            \n",
    "            info = root.find('cap:info', namespace)\n",
    "            if info is None:\n",
    "                raise ValueError(\"Missing info element\")\n",
    "\n",
    "            # Check if it's an advisory\n",
    "            is_advisory, awareness_type = self.check_advisory(info, namespace)\n",
    "            if is_advisory:\n",
    "                self.stats['advisory_files'] += 1\n",
    "                new_path = self.move_file_to_directory(file_path, self.advisories_dir)\n",
    "                return 'advisory', new_path, awareness_type\n",
    "\n",
    "            # Check area description\n",
    "            area = info.find('cap:area', namespace)\n",
    "            if area is None or not area.findall('cap:geocode', namespace):\n",
    "                self.stats['area_missing_files'] += 1\n",
    "                new_path = self.move_file_to_directory(file_path, self.error_dir)\n",
    "                return 'error', new_path, \"Missing area description\"\n",
    "\n",
    "            # File is valid\n",
    "            self.stats['valid_files'] += 1\n",
    "            return 'valid', file_path, None\n",
    "\n",
    "        except Exception as e:\n",
    "            self.stats['error_files'] += 1\n",
    "            new_path = self.move_file_to_directory(file_path, self.error_dir)\n",
    "            return 'error', new_path, str(e)\n",
    "\n",
    "    def severity_to_color(self, severity):\n",
    "        \"\"\"Map severity levels to warning colors.\"\"\"\n",
    "        mapping = {\n",
    "            'Extreme': 'Red',\n",
    "            'Severe': 'Orange',\n",
    "            'Moderate': 'Yellow'\n",
    "        }\n",
    "        return mapping.get(severity, 'notmapped')\n",
    "\n",
    "    def parse_valid_file(self, file_path):\n",
    "        \"\"\"Parse a valid weather warning XML file.\"\"\"\n",
    "        try:\n",
    "            with codecs.open(file_path, 'r', encoding='utf-8', errors='ignore') as file:\n",
    "                xml_content = file.read()\n",
    "            \n",
    "            root = ET.fromstring(xml_content)\n",
    "            namespace = {'cap': 'urn:oasis:names:tc:emergency:cap:1.2'}\n",
    "            info = root.find('cap:info', namespace)\n",
    "            area = info.find('cap:area', namespace)\n",
    "            \n",
    "            row = {\n",
    "                'Issue Time': self.get_element_text(root, 'cap:sent', namespace),\n",
    "                'Valid From': (self.get_element_text(info, 'cap:onset', namespace) or \n",
    "                             self.get_element_text(info, 'cap:effective', namespace)),\n",
    "                'Valid To': self.get_element_text(info, 'cap:expires', namespace),\n",
    "                'Warning Element': self.get_element_text(info, 'cap:event', namespace),\n",
    "                'Warning Text': self.get_element_text(info, 'cap:description', namespace),\n",
    "                'WhereToText': self.get_element_text(area, 'cap:areaDesc', namespace),\n",
    "                'Warning Colour': self.severity_to_color(\n",
    "                    self.get_element_text(info, 'cap:severity', namespace)\n",
    "                )\n",
    "            }\n",
    "            \n",
    "            # Initialize all counties to 0\n",
    "            for county_name in self.county_info.values():\n",
    "                row[county_name] = 0\n",
    "                \n",
    "            # Set affected counties to 1\n",
    "            geocodes = area.findall('cap:geocode', namespace)\n",
    "            for gc in geocodes:\n",
    "                if self.get_element_text(gc, 'cap:valueName', namespace) == 'FIPS':\n",
    "                    county_code = self.get_element_text(gc, 'cap:value', namespace)\n",
    "                    if county_code in self.county_info:\n",
    "                        row[self.county_info[county_code]] = 1\n",
    "            \n",
    "            return row\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error parsing valid file {file_path}: {str(e)}\")\n",
    "            return None\n",
    "\n",
    "    def process_files(self):\n",
    "        \"\"\"Process all XML files in the data directory.\"\"\"\n",
    "        file_pattern = os.path.join(self.data_directory, '*.xml')\n",
    "        file_list = glob.glob(file_pattern)\n",
    "        self.stats['total_files'] = len(file_list)\n",
    "        \n",
    "        advisory_data = []\n",
    "        error_data = []\n",
    "        valid_data = []\n",
    "\n",
    "        self.logger.info(f\"Starting to process {len(file_list)} files\")\n",
    "        \n",
    "        # Using tqdm.notebook for Jupyter-friendly progress bar\n",
    "        for file_path in tqdm(file_list, desc=\"Processing files\"):\n",
    "            category, new_path, additional_info = self.process_single_file(file_path)\n",
    "            \n",
    "            if category == 'advisory':\n",
    "                advisory_data.append({\n",
    "                    'filename': os.path.basename(new_path),\n",
    "                    'awareness_type': additional_info\n",
    "                })\n",
    "            elif category == 'error':\n",
    "                error_data.append({\n",
    "                    'filename': os.path.basename(new_path),\n",
    "                    'error_message': additional_info\n",
    "                })\n",
    "            elif category == 'valid':\n",
    "                row = self.parse_valid_file(new_path)\n",
    "                if row:\n",
    "                    valid_data.append(row)\n",
    "\n",
    "        # Save results\n",
    "        self.save_results(advisory_data, error_data, valid_data)\n",
    "        self.save_statistics()\n",
    "        \n",
    "        return pd.DataFrame(valid_data)\n",
    "\n",
    "    def save_results(self, advisory_data, error_data, valid_data):\n",
    "        \"\"\"Save processing results to files.\"\"\"\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        \n",
    "        # Save advisory files list\n",
    "        if advisory_data:\n",
    "            advisory_df = pd.DataFrame(advisory_data)\n",
    "            advisory_path = os.path.join(self.advisories_dir, f'advisory_files_{timestamp}.xlsx')\n",
    "            advisory_df.to_excel(advisory_path, index=False)\n",
    "            self.logger.info(f\"Saved {len(advisory_data)} advisory files to {advisory_path}\")\n",
    "        \n",
    "        # Save error files list\n",
    "        if error_data:\n",
    "            error_df = pd.DataFrame(error_data)\n",
    "            error_path = os.path.join(self.error_dir, f'error_files_{timestamp}.xlsx')\n",
    "            error_df.to_excel(error_path, index=False)\n",
    "            self.logger.info(f\"Saved {len(error_data)} error files to {error_path}\")\n",
    "        \n",
    "        # Save valid data\n",
    "        if valid_data:\n",
    "            df_valid = pd.DataFrame(valid_data)\n",
    "            # Convert date fields to datetime\n",
    "            date_columns = ['Issue Time', 'Valid From', 'Valid To']\n",
    "            for col in date_columns:\n",
    "                if col in df_valid.columns:\n",
    "                    df_valid[col] = pd.to_datetime(df_valid[col], utc=True, errors='coerce')\n",
    "            \n",
    "            valid_path = os.path.join(self.data_directory, f'weather_warnings_{timestamp}.csv')\n",
    "            df_valid.to_csv(valid_path, index=False)\n",
    "            self.logger.info(f\"Saved {len(valid_data)} valid weather warnings to {valid_path}\")\n",
    "\n",
    "    def save_statistics(self):\n",
    "        \"\"\"Save processing statistics to a file and display them.\"\"\"\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        stats_file = os.path.join(self.log_dir, f'processing_stats_{timestamp}.txt')\n",
    "        \n",
    "        stats_text = [\n",
    "            \"Weather Data Processing Statistics\",\n",
    "            \"=================================\",\n",
    "            f\"Processing completed at: {datetime.now()}\",\n",
    "            \"\",\n",
    "            f\"Total files processed: {self.stats['total_files']}\",\n",
    "            f\"Advisory files found: {self.stats['advisory_files']}\",\n",
    "            f\"Files with missing area description: {self.stats['area_missing_files']}\",\n",
    "            f\"Files with XML parsing errors: {self.stats['error_files']}\",\n",
    "            f\"Valid files processed: {self.stats['valid_files']}\"\n",
    "        ]\n",
    "        \n",
    "        # Save to file\n",
    "        with open(stats_file, 'w') as f:\n",
    "            f.write('\\n'.join(stats_text))\n",
    "        \n",
    "        # Log statistics\n",
    "        self.logger.info(\"Processing Statistics:\")\n",
    "        for stat in stats_text[4:]:  # Skip the header lines\n",
    "            self.logger.info(stat)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f3f7843",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the processor:\n",
    "data_directory = \"/mnt/hgfs/shared/weather_warnings/archive_warnings/archive\"\n",
    "processor = WeatherDataProcessor(data_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e774d950",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_xml = processor.process_files()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "193bd284",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nProcessed Data Summary:\")\n",
    "print(\"-----------------------\")\n",
    "print(f\"Total rows: {len(df_xml)}\")\n",
    "print(\"\\nWarning Elements distribution:\")\n",
    "print(df_xml['Warning Element'].value_counts())\n",
    "print(\"\\nWarning Colors distribution:\")\n",
    "print(df_xml['Warning Colour'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f3bc96f-13c8-43b0-9204-f1b0fbe292b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the first few rows\n",
    "print(df_xml.head())\n",
    "\n",
    "# look at dataframe info\n",
    "print(df_xml.info())\n",
    "\n",
    "# check the df shape\n",
    "print(f\"Number of rows: {df_xml.shape[0]}\")\n",
    "print(f\"Number of columns: {df_xml.shape[1]}\")\n",
    "\n",
    "print(\"XML Filtered Date Range:\")\n",
    "print(f\"Start date: {df_xml['Issue Time'].min()}\")\n",
    "print(f\"End date: {df_xml['Issue Time'].max()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f166e127-012c-4804-9b3c-0d4ba0751d77",
   "metadata": {},
   "source": [
    "### Step 4: Cleaning and processing data into a common format for combination"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f574de01-a484-41cc-a9f0-cdd98062e432",
   "metadata": {},
   "source": [
    "#### Step 4.1 check for provinces "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ebdc561-5a37-4392-916e-d3a0a3c9118c",
   "metadata": {},
   "outputs": [],
   "source": [
    "##list all columns (check does xml have provinces)\n",
    "df_xml.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8991d2a6",
   "metadata": {},
   "source": [
    "##### data has no provinces or specifc ireland columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15e761c5",
   "metadata": {},
   "source": [
    "#### Step 4.2 check for missing values and duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fee7ed5f-126c-4f42-b60f-36bb2f1b03cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_missing = df_xml.isnull().sum().sum()\n",
    "percent_missing = (total_missing / df_xml.size) * 100\n",
    "print(f\"Total missing values: {total_missing}\")\n",
    "print(f\"Percentage of missing values: {percent_missing:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c5c2667",
   "metadata": {},
   "outputs": [],
   "source": [
    "# All rows with any missing values\n",
    "rows_with_missing = df_xml[df_xml.isnull().any(axis=1)]\n",
    "rows_with_missing.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cec431ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "##look for duplicates\n",
    "duplicate_rows = df_xml.duplicated()\n",
    "duplicate_rows.sum()\n",
    "df_xml[duplicate_rows]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6096307a-282e-4a12-8c2e-9ff0d9ebed7b",
   "metadata": {},
   "source": [
    "#### Step 4.3  Aggregate duplicate events into individual per county events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c661da4",
   "metadata": {},
   "outputs": [],
   "source": [
    "####duplictes could be due to the expanded list of location compared to the ods data and additionally multiple notificatiosn of the same event \n",
    "####consolidate the data into single events \n",
    "\n",
    "def identify_and_aggregate_unique_events(df):\n",
    "    df_processed = df.copy()\n",
    "    \n",
    "    try:\n",
    "        # CHANGED: Now preserving full timestamp instead of just date\n",
    "        for column in ['Valid From', 'Valid To']:\n",
    "            # CHANGED: Using timestamp conversion instead of date\n",
    "            df_processed[f'{column}_timestamp'] = pd.to_datetime(\n",
    "                df_processed[column], \n",
    "                utc=True,\n",
    "                errors='coerce'\n",
    "            )\n",
    "            \n",
    "            # CHANGED: Format with time information included\n",
    "            df_processed[f'{column}_formatted'] = df_processed[f'{column}_timestamp'].apply(\n",
    "                lambda x: x.strftime('%Y-%m-%d %H:%M:%S') if pd.notnull(x) else 'unknown'\n",
    "            )\n",
    "            \n",
    "            # Clean up intermediate column\n",
    "            df_processed = df_processed.drop(columns=[f'{column}_timestamp'])\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"\\nError processing timestamps for column: {column}\")\n",
    "        print(f\"Error details: {str(e)}\")\n",
    "        raise\n",
    "    \n",
    "    # CHANGED: Using formatted timestamps in event identifier\n",
    "    df_processed['event_id'] = (\n",
    "        df_processed['Valid From_formatted'] + '_' +  # includes time\n",
    "        df_processed['Valid To_formatted'] + '_' +    # includes time\n",
    "        df_processed['Warning Colour'].fillna('unknown').astype(str) + '_' +\n",
    "        df_processed['Warning Element'].fillna('unknown').astype(str) + '_' +\n",
    "        #df_processed['WhereToText'].fillna('unknown').astype(str) + '_' +\n",
    "        df_processed['Warning Text'].fillna('unknown').astype(str) \n",
    "        \n",
    "    )\n",
    "    \n",
    "    def aggregate_regions_row(row):\n",
    "        row['Dublin'] = int(\n",
    "            row['Dublin City'] | \n",
    "            row['Dún Laoghaire-Rathdown'] | \n",
    "            row['South Dublin'] | \n",
    "            row['Fingal']\n",
    "        )\n",
    "        row['Limerick'] = int(row['Limerick'] | row['Limerick City'])\n",
    "        row['Cork'] = int(row['Cork'] | row['Cork City'])\n",
    "        row['Galway'] = int(row['Galway'] | row['Galway City'])\n",
    "        return row\n",
    "    \n",
    "    df_processed = df_processed.apply(aggregate_regions_row, axis=1)\n",
    "    \n",
    "    columns_to_drop = [\n",
    "        'Dublin City', 'Dún Laoghaire-Rathdown', 'South Dublin', 'Fingal',\n",
    "        'Limerick City', 'Cork City', 'Galway City',\n",
    "        'Valid From_formatted', 'Valid To_formatted'  # Changed from _date to _formatted\n",
    "    ]\n",
    "    \n",
    "    df_processed = df_processed.drop(columns=columns_to_drop)\n",
    "    df_unique = df_processed.drop_duplicates(subset=['event_id'])\n",
    "    df_unique = df_unique.drop(columns=['event_id'])\n",
    "    \n",
    "    return df_unique\n",
    "\n",
    "def verify_aggregation(original_df, aggregated_df):\n",
    "    print(f\"Original number of rows: {len(original_df)}\")\n",
    "    print(f\"Aggregated number of rows: {len(aggregated_df)}\")\n",
    "    \n",
    "    temp_df = original_df.copy()\n",
    "    \n",
    "    try:\n",
    "        # use full timestamps\n",
    "        for column in ['Valid From', 'Valid To']:\n",
    "            temp_datetime = pd.to_datetime(temp_df[column], utc=True, errors='coerce')\n",
    "            #Using timestamp instead of just date\n",
    "            temp_df[f'{column}_formatted'] = temp_datetime.apply(\n",
    "                lambda x: x.strftime('%Y-%m-%d %H:%M:%S') if pd.notnull(x) else 'unknown'\n",
    "            )\n",
    "        \n",
    "        # Use formatted timestamps in groupby\n",
    "        original_events = temp_df.groupby([\n",
    "            'Valid From_formatted',\n",
    "            'Valid To_formatted',\n",
    "            'Warning Colour',\n",
    "            'Warning Element',\n",
    "            #'WhereToText',\n",
    "            'Warning Text'\n",
    "        ]).size().reset_index(name='count')\n",
    "        \n",
    "        print(f\"Number of unique events (including time): {len(original_events)}\")\n",
    "        print(\"\\nSample of unique events with timestamps:\")\n",
    "        print(original_events.head())\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error during verification: {str(e)}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83020acb-b7dd-405b-9c49-56fd09868023",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final = identify_and_aggregate_unique_events(df_xml)\n",
    "verify_aggregation(df_xml, df_final)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f158b3a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "####check for duplicates again after processing \n",
    "duplicate_rows = df_final.duplicated()\n",
    "duplicate_rows.sum()\n",
    "df_final[duplicate_rows]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b9a0541",
   "metadata": {},
   "source": [
    "#### Step 4.4 convert true/false to 0/1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eb5d661",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify all boolean columns in the DataFrame\n",
    "bool_cols = df_final.select_dtypes(include=['bool']).columns\n",
    "\n",
    "# Convert boolean columns to integers (True -> 1, False -> 0)\n",
    "df_final[bool_cols] = df_final[bool_cols].astype(int)\n",
    "\n",
    "# Verify the changes by displaying data types\n",
    "print(\"Data types after conversion:\")\n",
    "print(df_final.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af3779ff",
   "metadata": {},
   "source": [
    "#### Step 4.5 confirm date/time settings, check date ranges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8cad641",
   "metadata": {},
   "outputs": [],
   "source": [
    "##make sure datetime is the same in both dataframes\n",
    "# Ensure datetime columns are consistently UTC\n",
    "datetime_cols = ['Issue Time', 'Valid From', 'Valid To']\n",
    "df_final[datetime_cols] = df_final[datetime_cols].apply(pd.to_datetime, utc=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33f51d0c",
   "metadata": {},
   "source": [
    "#### Step 4.6 filter data for dates of interest 2013 to 2020 (full years of data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89d10746",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"XML Filtered Date Range:\")\n",
    "print(f\"Start date: {df_final['Issue Time'].min()}\")\n",
    "print(f\"End date: {df_final['Issue Time'].max()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "497aaf01",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Filter XML data from 2018 to end of 2022\n",
    "### Although we have data in the ODS format from 2017 to 2020 using the years from both datasets 2018,2019,2020 can help validate my processing steps  \n",
    "df_xml_filtered = df_final[\n",
    "    (df_final['Issue Time'] >= '2018-01-01') & \n",
    "    (df_final['Issue Time'] <= '2023-08-04 23:59:59')\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55d8d35a",
   "metadata": {},
   "source": [
    "##### Step 5.5 drop unneeded columns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bd199f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "##drop columns\n",
    "#columns_to_drop = ['WhereToText', 'Warning Text']\n",
    "#df_xml_filtered = df_xml_filtered.drop(columns=columns_to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b68cda8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "####quick final check \n",
    "df_xml_filtered.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ba2b26f",
   "metadata": {},
   "source": [
    "#### Step 5 save filtered data to file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32d0cd2c-a0b7-4d7f-9ce7-02985a63185f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_xml_filtered.to_csv('/mnt/hgfs/shared/project_data/met_eireann/xml_warnings_2018_2023_08.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47c69100-4bd6-48d3-ae00-ca7512c68602",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08b93442-72b3-484f-a8bd-115b5f9c6cd8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba2a33eb-c47a-42c5-b6e9-242985a8212e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
