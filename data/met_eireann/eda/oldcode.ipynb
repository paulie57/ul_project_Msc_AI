{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "def load_weather_warnings(file_path):\n",
    "    \"\"\"\n",
    "    Load weather warnings from a CSV file and perform initial data cleaning.\n",
    "    This function preserves all county columns and ensures proper datetime formatting.\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(file_path)\n",
    "    \n",
    "    # Convert datetime columns to proper datetime objects\n",
    "    datetime_columns = ['Issue Time', 'Valid From', 'Valid To']\n",
    "    for col in datetime_columns:\n",
    "        df[col] = pd.to_datetime(df[col])\n",
    "    \n",
    "    return df\n",
    "\n",
    "def consolidate_warnings(df):\n",
    "    \"\"\"\n",
    "    Consolidate weather warnings by grouping similar events.\n",
    "    Uses a flat aggregation structure to avoid nested dictionary errors.\n",
    "    \"\"\"\n",
    "    # First, let's identify our county columns\n",
    "    base_columns = ['Issue Time', 'Valid From', 'Valid To', 'Warning Element',\n",
    "                   'Warning Text', 'WhereToText', 'Warning Colour']\n",
    "    county_columns = [col for col in df.columns if col not in base_columns]\n",
    "    \n",
    "    # Create the event key for grouping\n",
    "    df['event_key'] = df.apply(\n",
    "        lambda x: f\"{x['Valid To']}_{x['Warning Element']}_{x['Warning Colour']}_{x['Warning Text']}\", \n",
    "        axis=1\n",
    "    )\n",
    "    \n",
    "    # Create a flat aggregation dictionary\n",
    "    agg_dict = {\n",
    "        'Issue Time': 'first',                    # Keep the first issue time\n",
    "        'Valid From': 'first',\n",
    "        'Valid To': 'first',\n",
    "        'Warning Element': 'first',\n",
    "        'Warning Text': 'first',\n",
    "        'WhereToText': 'first',\n",
    "        'Warning Colour': 'first'\n",
    "    }\n",
    "    \n",
    "    # Add county columns to aggregation\n",
    "    for col in county_columns:\n",
    "        agg_dict[col] = 'first'\n",
    "    \n",
    "    # First grouping to get the basic consolidated data\n",
    "    df_consolidated = df.groupby('event_key').agg(agg_dict).reset_index()\n",
    "    \n",
    "    # Now calculate the additional metrics separately\n",
    "    issue_counts = df.groupby('event_key').size().reset_index(name='issue_count')\n",
    "    first_issues = df.groupby('event_key')['Issue Time'].min().reset_index(name='first_issue')\n",
    "    last_issues = df.groupby('event_key')['Issue Time'].max().reset_index(name='last_issue')\n",
    "    \n",
    "    # Merge all the metrics back together\n",
    "    df_consolidated = (df_consolidated\n",
    "                      .merge(issue_counts, on='event_key')\n",
    "                      .merge(first_issues, on='event_key')\n",
    "                      .merge(last_issues, on='event_key'))\n",
    "    \n",
    "    # Rename columns to match desired output\n",
    "    rename_dict = {\n",
    "        'Warning Element': 'warning_type',\n",
    "        'Warning Text': 'warning_text',\n",
    "        'WhereToText': 'location',\n",
    "        'Warning Colour': 'warning_colour'\n",
    "    }\n",
    "    df_consolidated = df_consolidated.rename(columns=rename_dict)\n",
    "    \n",
    "    # Arrange columns in the desired order\n",
    "    column_order = [\n",
    "        'event_key', \n",
    "        'Issue Time', \n",
    "        'issue_count',\n",
    "        'first_issue',\n",
    "        'last_issue',\n",
    "        'Valid From',\n",
    "        'Valid To',\n",
    "        'warning_type',\n",
    "        'warning_text',\n",
    "        'location',\n",
    "        'warning_colour'\n",
    "    ] + county_columns\n",
    "    \n",
    "    # Only select columns that exist\n",
    "    final_columns = [col for col in column_order if col in df_consolidated.columns]\n",
    "    df_consolidated = df_consolidated[final_columns]\n",
    "    \n",
    "    return df_consolidated\n",
    "\n",
    "def analyze_warnings(df):\n",
    "    \"\"\"\n",
    "    Generate summary statistics about the weather warnings.\n",
    "    \"\"\"\n",
    "    analysis = {\n",
    "        'total_warnings': len(df),\n",
    "        'unique_events': df['event_key'].nunique(),\n",
    "        'warning_types': df['warning_type'].value_counts().to_dict(),\n",
    "        'most_reissued': df.nlargest(1, 'issue_count')[['warning_type', 'location', 'issue_count']].to_dict('records')[0],\n",
    "        'avg_issues_per_event': df['issue_count'].mean()\n",
    "    }\n",
    "    return analysis\n",
    "\n",
    "def process_weather_warnings(input_file, output_file):\n",
    "    \"\"\"\n",
    "    Main function to process weather warnings data.\n",
    "    \"\"\"\n",
    "    print(f\"Loading data from {input_file}...\")\n",
    "    df = load_weather_warnings(input_file)\n",
    "    \n",
    "    print(\"Consolidating warnings...\")\n",
    "    df_xml_consolidated = consolidate_warnings(df)\n",
    "    \n",
    "    print(\"Analyzing results...\")\n",
    "    analysis_results = analyze_warnings(df_xml_consolidated)\n",
    "    \n",
    "    print(\"\\nWeather Warnings Analysis Summary:\")\n",
    "    print(f\"Total warnings issued: {analysis_results['total_warnings']}\")\n",
    "    print(f\"Number of unique events: {analysis_results['unique_events']}\")\n",
    "    print(f\"\\nWarning types frequency:\")\n",
    "    for warning_type, count in analysis_results['warning_types'].items():\n",
    "        print(f\"- {warning_type}: {count}\")\n",
    "    print(f\"\\nMost reissued warning:\")\n",
    "    print(f\"- Type: {analysis_results['most_reissued']['warning_type']}\")\n",
    "    print(f\"- Location: {analysis_results['most_reissued']['location']}\")\n",
    "    print(f\"- Times issued: {analysis_results['most_reissued']['issue_count']}\")\n",
    "    print(f\"\\nAverage issues per event: {analysis_results['avg_issues_per_event']:.2f}\")\n",
    "    \n",
    "    print(f\"\\nExporting consolidated data to {output_file}...\")\n",
    "    df_xml_consolidated.to_csv(output_file, index=False)\n",
    "    print(\"Export complete!\")\n",
    "    \n",
    "    return df_xml_consolidated\n",
    "\n",
    "from pathlib import Path\n",
    "input_file = \"/mnt/hgfs/shared/ul_project_Msc_AI/data/met_eireann/eda/output.csv\"\n",
    "output_file = \"/mnt/hgfs/shared/ul_project_Msc_AI/data/met_eireann/eda/consolidated_weather_warnings.csv\"\n",
    "df_xml_consolidated = process_weather_warnings(input_file, output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def severity_to_color(severity):\n",
    "    \"\"\"Map severity levels to warning colors\"\"\"\n",
    "    mapping = {\n",
    "        'Extreme': 'Red',\n",
    "        'Severe': 'Orange',\n",
    "        'Moderate': 'Yellow'\n",
    "    }\n",
    "    return mapping.get(severity, 'notmapped')\n",
    "\n",
    "def get_element_text(element, path, namespace):\n",
    "    \"\"\"Safely get text from an XML element\"\"\"\n",
    "    found = element.find(path, namespace)\n",
    "    return found.text if found is not None else None\n",
    "\n",
    "def parse_xml(file_path_xml):\n",
    "    \"\"\"Parse an XML file and extract relevant information.\"\"\"\n",
    "    try:\n",
    "        # Read the file content\n",
    "        with codecs.open(file_path_xml, 'r', encoding='utf-8', errors='ignore') as file:\n",
    "            xml_content = file.read()\n",
    "        \n",
    "        # Parse the XML content\n",
    "        root = ET.fromstring(xml_content)\n",
    "        namespace = {'cap': 'urn:oasis:names:tc:emergency:cap:1.2'}\n",
    "        \n",
    "        # Extract data from the info element first to check if we should process this warning\n",
    "        info = root.find('cap:info', namespace)\n",
    "        if info is None:\n",
    "            return None\n",
    "            \n",
    "        # Check for advisory warnings (type 22) - exclude them\n",
    "        parameters = info.findall('cap:parameter', namespace)\n",
    "        for param in parameters:\n",
    "            if get_element_text(param, 'cap:valueName', namespace) == 'awareness_type':\n",
    "                awareness_type = get_element_text(param, 'cap:value', namespace)\n",
    "                if awareness_type and '22' in awareness_type:\n",
    "                    return None\n",
    "                break\n",
    "        \n",
    "        # Check for county information\n",
    "        area = info.find('cap:area', namespace)\n",
    "        if area is None:\n",
    "            return None\n",
    "            \n",
    "        geocodes = area.findall('cap:geocode', namespace)\n",
    "        if not geocodes:\n",
    "            return None\n",
    "\n",
    "        # Initialize row with the old column format\n",
    "        row = {\n",
    "            'Issue Time': get_element_text(root, 'cap:sent', namespace),\n",
    "            'Valid From': (get_element_text(info, 'cap:effective', namespace) or \n",
    "                         get_element_text(info, 'cap:onset', namespace)),\n",
    "            'Valid To': get_element_text(info, 'cap:expires', namespace),\n",
    "            'Warning Element': get_element_text(info, 'cap:event', namespace),\n",
    "            'Warning Text': get_element_text(info, 'cap:description', namespace),\n",
    "            'WhereToText': get_element_text(area, 'cap:areaDesc', namespace),\n",
    "            'Warning Colour': severity_to_color(get_element_text(info, 'cap:severity', namespace))\n",
    "        }\n",
    "        \n",
    "        # Initialize all county columns to 0\n",
    "        county_info = {\n",
    "            'EI01': 'Carlow', 'EI02': 'Cavan', 'EI03': 'Clare', 'EI04': 'Cork', 'EI32': 'Cork City',\n",
    "            'EI06': 'Donegal', 'EI33': 'Dublin City', 'EI34': 'Dún Laoghaire-Rathdown', 'EI35': 'Fingal',\n",
    "            'EI10': 'Galway', 'EI36': 'Galway City', 'EI11': 'Kerry', 'EI12': 'Kildare', 'EI13': 'Kilkenny',\n",
    "            'EI15': 'Laois', 'EI14': 'Leitrim', 'EI42': 'Limerick', 'EI37': 'Limerick City', 'EI18': 'Longford',\n",
    "            'EI19': 'Louth', 'EI20': 'Mayo', 'EI21': 'Meath', 'EI22': 'Monaghan', 'EI23': 'Offaly',\n",
    "            'EI24': 'Roscommon', 'EI25': 'Sligo', 'EI39': 'South Dublin', 'EI43': 'Tipperary',\n",
    "            'EI44': 'Waterford', 'EI29': 'Westmeath', 'EI30': 'Wexford', 'EI31': 'Wicklow'\n",
    "        }\n",
    "        \n",
    "        # Initialize all counties to 0\n",
    "        for county_name in county_info.values():\n",
    "            row[county_name] = 0\n",
    "            \n",
    "        # Set affected counties to 1\n",
    "        for gc in geocodes:\n",
    "            if get_element_text(gc, 'cap:valueName', namespace) == 'FIPS':\n",
    "                county_code = get_element_text(gc, 'cap:value', namespace)\n",
    "                if county_code in county_info:\n",
    "                    row[county_info[county_code]] = 1\n",
    "        \n",
    "        return row\n",
    "    \n",
    "    except ET.ParseError as e:\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        return None\n",
    "\n",
    "def process_files(data_directory):\n",
    "    \"\"\"Process all XML files in the given directory and extract weather warning data.\"\"\"\n",
    "    file_pattern = os.path.join(data_directory, '*.xml')\n",
    "    file_list = glob.glob(file_pattern)\n",
    "\n",
    "    data = []\n",
    "    error_files = []\n",
    "    excluded_files = []\n",
    "\n",
    "    for file in tqdm(file_list, desc=\"Processing files\"):\n",
    "        row = parse_xml(file)\n",
    "        if row is not None:\n",
    "            data.append(row)\n",
    "        else:\n",
    "            try:\n",
    "                with codecs.open(file, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "                    ET.parse(f)\n",
    "                excluded_files.append(file)\n",
    "            except:\n",
    "                error_files.append(file)\n",
    "\n",
    "    # Create DataFrame\n",
    "    df_xml = pd.DataFrame(data)\n",
    "\n",
    "    # Convert date fields to datetime\n",
    "    date_columns = ['Issue Time', 'Valid From', 'Valid To']\n",
    "    for col in date_columns:\n",
    "        if col in df_xml.columns:\n",
    "            df_xml[col] = pd.to_datetime(df_xml[col], utc=True, errors='coerce')\n",
    "\n",
    "    return df_xml, error_files, excluded_files\n",
    "\n",
    "def save_error_files(file_list, filename):\n",
    "    \"\"\"Save list of error files to CSV\"\"\"\n",
    "    pd.DataFrame({'file': file_list}).to_csv(filename, index=False)\n",
    "\n",
    "\n",
    "df_xml, error_files, excluded_files = process_files(data_directory_xml)\n",
    "\n",
    "# Save results\n",
    "# df_xml.to_csv('weather_warnings.csv', index=False)\n",
    "save_error_files(error_files, 'error_files.csv')\n",
    "save_error_files(excluded_files, 'excluded_files.csv')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
