{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "def load_weather_warnings(file_path):\n",
    "    \"\"\"\n",
    "    Load weather warnings from a CSV file and perform initial data cleaning.\n",
    "    This function preserves all county columns and ensures proper datetime formatting.\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(file_path)\n",
    "    \n",
    "    # Convert datetime columns to proper datetime objects\n",
    "    datetime_columns = ['Issue Time', 'Valid From', 'Valid To']\n",
    "    for col in datetime_columns:\n",
    "        df[col] = pd.to_datetime(df[col])\n",
    "    \n",
    "    return df\n",
    "\n",
    "def consolidate_warnings(df):\n",
    "    \"\"\"\n",
    "    Consolidate weather warnings by grouping similar events.\n",
    "    Uses a flat aggregation structure to avoid nested dictionary errors.\n",
    "    \"\"\"\n",
    "    # First, let's identify our county columns\n",
    "    base_columns = ['Issue Time', 'Valid From', 'Valid To', 'Warning Element',\n",
    "                   'Warning Text', 'WhereToText', 'Warning Colour']\n",
    "    county_columns = [col for col in df.columns if col not in base_columns]\n",
    "    \n",
    "    # Create the event key for grouping\n",
    "    df['event_key'] = df.apply(\n",
    "        lambda x: f\"{x['Valid To']}_{x['Warning Element']}_{x['Warning Colour']}_{x['Warning Text']}\", \n",
    "        axis=1\n",
    "    )\n",
    "    \n",
    "    # Create a flat aggregation dictionary\n",
    "    agg_dict = {\n",
    "        'Issue Time': 'first',                    # Keep the first issue time\n",
    "        'Valid From': 'first',\n",
    "        'Valid To': 'first',\n",
    "        'Warning Element': 'first',\n",
    "        'Warning Text': 'first',\n",
    "        'WhereToText': 'first',\n",
    "        'Warning Colour': 'first'\n",
    "    }\n",
    "    \n",
    "    # Add county columns to aggregation\n",
    "    for col in county_columns:\n",
    "        agg_dict[col] = 'first'\n",
    "    \n",
    "    # First grouping to get the basic consolidated data\n",
    "    df_consolidated = df.groupby('event_key').agg(agg_dict).reset_index()\n",
    "    \n",
    "    # Now calculate the additional metrics separately\n",
    "    issue_counts = df.groupby('event_key').size().reset_index(name='issue_count')\n",
    "    first_issues = df.groupby('event_key')['Issue Time'].min().reset_index(name='first_issue')\n",
    "    last_issues = df.groupby('event_key')['Issue Time'].max().reset_index(name='last_issue')\n",
    "    \n",
    "    # Merge all the metrics back together\n",
    "    df_consolidated = (df_consolidated\n",
    "                      .merge(issue_counts, on='event_key')\n",
    "                      .merge(first_issues, on='event_key')\n",
    "                      .merge(last_issues, on='event_key'))\n",
    "    \n",
    "    # Rename columns to match desired output\n",
    "    rename_dict = {\n",
    "        'Warning Element': 'warning_type',\n",
    "        'Warning Text': 'warning_text',\n",
    "        'WhereToText': 'location',\n",
    "        'Warning Colour': 'warning_colour'\n",
    "    }\n",
    "    df_consolidated = df_consolidated.rename(columns=rename_dict)\n",
    "    \n",
    "    # Arrange columns in the desired order\n",
    "    column_order = [\n",
    "        'event_key', \n",
    "        'Issue Time', \n",
    "        'issue_count',\n",
    "        'first_issue',\n",
    "        'last_issue',\n",
    "        'Valid From',\n",
    "        'Valid To',\n",
    "        'warning_type',\n",
    "        'warning_text',\n",
    "        'location',\n",
    "        'warning_colour'\n",
    "    ] + county_columns\n",
    "    \n",
    "    # Only select columns that exist\n",
    "    final_columns = [col for col in column_order if col in df_consolidated.columns]\n",
    "    df_consolidated = df_consolidated[final_columns]\n",
    "    \n",
    "    return df_consolidated\n",
    "\n",
    "def analyze_warnings(df):\n",
    "    \"\"\"\n",
    "    Generate summary statistics about the weather warnings.\n",
    "    \"\"\"\n",
    "    analysis = {\n",
    "        'total_warnings': len(df),\n",
    "        'unique_events': df['event_key'].nunique(),\n",
    "        'warning_types': df['warning_type'].value_counts().to_dict(),\n",
    "        'most_reissued': df.nlargest(1, 'issue_count')[['warning_type', 'location', 'issue_count']].to_dict('records')[0],\n",
    "        'avg_issues_per_event': df['issue_count'].mean()\n",
    "    }\n",
    "    return analysis\n",
    "\n",
    "def process_weather_warnings(input_file, output_file):\n",
    "    \"\"\"\n",
    "    Main function to process weather warnings data.\n",
    "    \"\"\"\n",
    "    print(f\"Loading data from {input_file}...\")\n",
    "    df = load_weather_warnings(input_file)\n",
    "    \n",
    "    print(\"Consolidating warnings...\")\n",
    "    df_xml_consolidated = consolidate_warnings(df)\n",
    "    \n",
    "    print(\"Analyzing results...\")\n",
    "    analysis_results = analyze_warnings(df_xml_consolidated)\n",
    "    \n",
    "    print(\"\\nWeather Warnings Analysis Summary:\")\n",
    "    print(f\"Total warnings issued: {analysis_results['total_warnings']}\")\n",
    "    print(f\"Number of unique events: {analysis_results['unique_events']}\")\n",
    "    print(f\"\\nWarning types frequency:\")\n",
    "    for warning_type, count in analysis_results['warning_types'].items():\n",
    "        print(f\"- {warning_type}: {count}\")\n",
    "    print(f\"\\nMost reissued warning:\")\n",
    "    print(f\"- Type: {analysis_results['most_reissued']['warning_type']}\")\n",
    "    print(f\"- Location: {analysis_results['most_reissued']['location']}\")\n",
    "    print(f\"- Times issued: {analysis_results['most_reissued']['issue_count']}\")\n",
    "    print(f\"\\nAverage issues per event: {analysis_results['avg_issues_per_event']:.2f}\")\n",
    "    \n",
    "    print(f\"\\nExporting consolidated data to {output_file}...\")\n",
    "    df_xml_consolidated.to_csv(output_file, index=False)\n",
    "    print(\"Export complete!\")\n",
    "    \n",
    "    return df_xml_consolidated\n",
    "\n",
    "from pathlib import Path\n",
    "input_file = \"/mnt/hgfs/shared/ul_project_Msc_AI/data/met_eireann/eda/output.csv\"\n",
    "output_file = \"/mnt/hgfs/shared/ul_project_Msc_AI/data/met_eireann/eda/consolidated_weather_warnings.csv\"\n",
    "df_xml_consolidated = process_weather_warnings(input_file, output_file)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
